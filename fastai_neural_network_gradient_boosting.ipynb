{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"},{"sourceId":7349720,"sourceType":"datasetVersion","datasetId":4268036}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rubanzasilva/fastai-neural-network-gradient-boosting?scriptVersionId=190267753\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Binary Classification of Insurance Cross \n\nThe objective of this competition is to predict which customers respond positively to an automobile insurance offer.\n\nA company offering health insurance to a segment of customers wants to find out whether their health insurance customers are willing to also get/sign up for their motor vehicle insurance plan.\n\nSubmissions are evaluated on area under the ROC curve metric using the predicted probabilities and ground truth targets. This means we shall always have to return the predicted probabilities for each class. Probability predictions can be more informative than outright class predictions.\n\nIn my submissions below I use the probabilities of the second class by using [:, 1] on the resulting probability predictions. This selects the probability of the positive class.\n\n","metadata":{}},{"cell_type":"markdown","source":"The above metric is suited to binary classification because it measures the model's ability to distinguish between classes.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"markdown","source":"## Library Imports","metadata":{}},{"cell_type":"code","source":"%pip install catboost\n%pip install optuna\n%pip install optuna_distributed\n%pip install openfe\n%pip install seaborn\n%pip install xgboost\n%pip install lightgbm\n%pip install fastkaggle\n#%pip install h2o\n%pip install -Uqq fastbook\n#%pip install -q -U autogluon.tabular\n#%pip install autogluon\n#ip install --upgrade pip","metadata":{"_kg_hide-output":true,"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-29T09:11:38.897117Z","iopub.execute_input":"2024-07-29T09:11:38.898123Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.5)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.5)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (2.2.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.11.4)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.1.1)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.2.3)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: optuna_distributed in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: optuna>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (3.6.1)\nRequirement already satisfied: dask[distributed] in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (2024.5.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from optuna_distributed) (13.7.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (4.66.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna>=3.1.0->optuna_distributed) (6.0.1)\nRequirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (2024.3.1)\nRequirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (1.4.2)\nRequirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (6.11.0)\nRequirement already satisfied: distributed==2024.5.2 in /opt/conda/lib/python3.10/site-packages (from dask[distributed]->optuna_distributed) (2024.5.2)\nRequirement already satisfied: jinja2>=2.10.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (3.1.2)\nRequirement already satisfied: locket>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (1.0.0)\nRequirement already satisfied: msgpack>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (1.0.7)\nRequirement already satisfied: psutil>=5.7.2 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (5.9.3)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (3.0.0)\nRequirement already satisfied: tornado>=6.0.4 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (6.3.3)\nRequirement already satisfied: urllib3>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (1.26.18)\nRequirement already satisfied: zict>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from distributed==2024.5.2->dask[distributed]->optuna_distributed) (3.0.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->optuna_distributed) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->optuna_distributed) (2.17.2)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_distributed) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna_distributed) (4.9.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[distributed]->optuna_distributed) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->optuna_distributed) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna>=3.1.0->optuna_distributed) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna>=3.1.0->optuna_distributed) (3.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2024.5.2->dask[distributed]->optuna_distributed) (2.1.3)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: openfe in /opt/conda/lib/python3.10/site-packages (0.0.12)\nRequirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.26.4)\nRequirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from openfe) (2.2.1)\nRequirement already satisfied: scikit-learn>=0.24.2 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.2.2)\nRequirement already satisfied: lightgbm>=3.3.2 in /opt/conda/lib/python3.10/site-packages (from openfe) (4.2.0)\nRequirement already satisfied: scipy>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from openfe) (1.11.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openfe) (4.66.4)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from openfe) (14.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->openfe) (2023.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->openfe) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.24.2->openfe) (3.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->openfe) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nRequirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.26.4)\nRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.2.1)\nRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%pip freeze > requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import fastbook\nfastbook.setup_book()\nfrom fastbook import *\nfrom fastai.tabular.all import *\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom numpy import random\nfrom tqdm import tqdm\nfrom ipywidgets import interact\n\nfrom fastai.imports import *\nnp.set_printoptions(linewidth=130)\n\n\nfrom pathlib import Path\nimport os\n\n\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score,accuracy_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score\n\n\n\n#transformers and pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import set_config\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom xgboost import XGBClassifier\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nfrom catboost import CatBoostClassifier,CatBoostRegressor,Pool, metrics, cv\n\n\n\n\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice\nfrom optuna.samplers import TPESampler\nimport warnings\n\n\nmatplotlib.rc('image', cmap='Greys')\n\nfrom fastkaggle import setup_comp\n\n\n\nfrom openfe import OpenFE, transform\n#from autogluon.tabular import TabularDataset, TabularPredictor\n\nimport gc\n\nfrom xgboost import plot_importance\n\n#from IPython.display import FileLink","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/health-insurance-cross-sell-prediction-data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('/kaggle/input/playground-series-s4e7/')\npath","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(path/'train.csv',index_col='id')\ntest_df = pd.read_csv(path/'test.csv',index_col='id')\nsub_df = pd.read_csv(path/'sample_submission.csv')\noriginal_train_df = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction-data/train.csv',index_col='id')\noriginal_test_df = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction-data/test.csv',index_col='id')\n                                \n#original_df = pd.read_csv('/kaggle/input/academic-success-dataset/data.csv',delimiter=';')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape,test_df.shape,original_train_df.shape,original_test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Subset","metadata":{}},{"cell_type":"code","source":"train_subset = train_df.sample(n=1000000,replace=False)\ntest_subset = test_df.sample(n=500000,replace=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subset.shape,test_subset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subset.shape,test_subset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_names,cat_names = cont_cat_split(train_subset, dep_var='Response')\nsplits = RandomSplitter(valid_pct=0.2)(range_of(train_subset))\nto = TabularPandas(train_subset, procs=[Categorify, FillMissing,Normalize],\n                  cat_names = cat_names,\n                  cont_names = cont_names,\n                  y_names='Response',\n                  y_block=CategoryBlock(),\n                  splits=splits)\ndls = to.dataloaders(bs=64)\ntest_dl = dls.test_dl(test_subset)\nX_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Full Dataset\n\nBelow i use the fastai cont_cat_split which returns the values of a particular dataframe as either categorical or continous values depending on the cardinality of the column values which is how many levels a particular column has.\n\nThis function takes an argument of max_card whose default is 20. If the number of unique values is above the max_card, then that variables is considered to be continuos and vice versa.","metadata":{}},{"cell_type":"code","source":"cont_names,cat_names = cont_cat_split(train_df, dep_var='Response')\nlen(cat_names),len(cont_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, i use fastai's RandomSplitter to divide my dataset into a train and validation set, in this example it creates a validation set with 20% of the training dataset.\n","metadata":{}},{"cell_type":"code","source":"splits = RandomSplitter(valid_pct=0.2)(range_of(train_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below i build a TabularPandas Object which is a subclass of the Tabular class that has methods we can use to handle tabular data preparation and preprocessing.\n\nThis object takes in arguments where i define my training dataset and preprocessing steps i.e. Categorify, FillMissing,Normalize.\n\nWe also define our categorical and continuous variables resulting from the cont_cat_split.\nFurthermore, i define my Target variable, the y which in this case is Response, y_block represents the type of problem.\n\nLastly we have the splits argument which was defined above and represents the dataset split.","metadata":{}},{"cell_type":"code","source":"to = TabularPandas(train_df, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = cat_names,\n                   cont_names = cont_names,\n                   y_names='Response',\n                   y_block=CategoryBlock(),\n                   splits=splits)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to.xs.iloc[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = to.dataloaders(bs=64)\ntest_dl = dls.test_dl(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nBelow are the feature descriptions as described in this [notebook](https://www.kaggle.com/code/satyaprakashshukl/insurance-boost-analysis)\n\nGender: Categorical variable indicating the gender of the customer.\n\nAge: Numeric variable indicating the age of the customer.\n\nDriving_License: Binary variable indicating if the customer has a driving license (1 if yes, 0 if no).\n\nRegion_Code: Numeric variable indicating the region code of the customer.\n\nPreviously_Insured: Binary variable indicating if the customer was previously insured (1 if yes, 0 if no).\n\nVehicle_Age: Categorical variable indicating the age of the vehicle.\n\nVehicle_Damage: Categorical variable indicating if the vehicle was damaged in the past.\n\nAnnual_Premium: Numeric variable indicating the annual premium amount.\n\nPolicy_Sales_Channel: Numeric variable indicating the sales channel of the policy.\n\nVintage: Numeric variable indicating the number of days the customer has been associated with the company.\n\nResponse: Binary target variable indicating if the customer responded positively to the automobile insurance offer (1 if yes, 0 if no).","metadata":{}},{"cell_type":"markdown","source":"### Summary Statistics","metadata":{}},{"cell_type":"code","source":"train_df.shape,test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cont_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cat_names),len(cont_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Null Value Check","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.hist(figsize=(20,15),edgecolor='black');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe(include=[object])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target","metadata":{}},{"cell_type":"code","source":"train_df['Response'].hist(figsize=(6,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Response'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Response'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#barplot\nsns.countplot(x='Response', data=train_df, palette='winter')\nplt.figure(figsize=(6, 4)) ;\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n\n# Count the occurrences of each response\nresponse_counts = train_df['Response'].value_counts()\n\n# Create a pie chart\nplt.figure(figsize=(3, 3))\ncolors = sns.color_palette('winter', n_colors=len(response_counts))\nplt.pie(response_counts.values, labels=response_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)\nplt.title('Distribution of Response')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target variable is imbalanced heavily skewing towards 0 which represents a no.","metadata":{}},{"cell_type":"markdown","source":"### Categorical Variables","metadata":{}},{"cell_type":"code","source":"cat_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Vehicle Damage","metadata":{}},{"cell_type":"code","source":"train_df['Vehicle_Damage'].hist(figsize=(6,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n\n# Count the occurrences of each response\nvd_counts = train_df['Vehicle_Damage'].value_counts()\n\n# Create a pie chart\nplt.figure(figsize=(3, 3))\ncolors = sns.color_palette('winter', n_colors=len(vd_counts))\nplt.pie(vd_counts.values, labels=vd_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)\nplt.title('Distribution of Vehicle_Damage')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"contingency_table = pd.crosstab(train_df['Vehicle_Damage'], train_df['Response'])\nprint(contingency_table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Vehicle_Damage', hue='Response', data=train_df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gender","metadata":{}},{"cell_type":"code","source":"train_df['Gender'].hist(figsize=(6,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n\n# Count the occurrences of each response\ngender_counts = train_df['Gender'].value_counts()\n\n# Create a pie chart\nplt.figure(figsize=(3, 3))\ncolors = sns.color_palette('winter', n_colors=len(gender_counts))\nplt.pie(gender_counts.values, labels=gender_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)\nplt.title('Distribution of Gender')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='Gender', hue='Response', data=train_df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Continous Variables","metadata":{}},{"cell_type":"code","source":"cont_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Age","metadata":{}},{"cell_type":"code","source":"train_df['Age'].hist(figsize=(6,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x='Age', data=train_df)\nplt.show()\nplt.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(x = 'Age', data = train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Change the display options\npd.set_option('display.float_format', '{:.4f}'.format)\n\n# Now run your describe() function\nprint(train_df['Age'].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.reset_option('display.float_format')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.Age.mode()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outlier check","metadata":{}},{"cell_type":"code","source":"train_df.Age.var()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age and response","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation","metadata":{}},{"cell_type":"code","source":"# Extract data from fastai DataLoaders\ndf = pd.DataFrame(dls.train.dataset.items)\n\n# Select numeric columns\nnumeric_df = df.select_dtypes(include=[np.number])\n\n# Replace inf values with NaN if any\n#numeric_df = numeric_df.replace([np.inf, -np.inf], np.nan)\n\n# Calculate correlation matrix\ncorrelation_matrix = numeric_df.corr()\n\n# Create the plot\nplt.figure(figsize=(20, 10))\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".1f\", linewidths=0.1)\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation_matrix = .corr()\nplt.figure(figsize=(20, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", linewidths=0.1)\n#plt.gcf().set_facecolor('skyblue')  \nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Theories\n\nFrom the correlation matrix, i infer that vehicle damage and Age have a heavy influence on the Response.\nWe notice that people with vehicle damage are way more likely to buy car insuarance.","metadata":{}},{"cell_type":"code","source":"dls","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.977426Z","iopub.status.idle":"2024-07-28T05:35:08.97775Z","shell.execute_reply.started":"2024-07-28T05:35:08.977591Z","shell.execute_reply":"2024-07-28T05:35:08.977605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.979088Z","iopub.status.idle":"2024-07-28T05:35:08.979459Z","shell.execute_reply.started":"2024-07-28T05:35:08.979295Z","shell.execute_reply":"2024-07-28T05:35:08.97931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.980762Z","iopub.status.idle":"2024-07-28T05:35:08.981096Z","shell.execute_reply.started":"2024-07-28T05:35:08.980931Z","shell.execute_reply":"2024-07-28T05:35:08.980945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"code","source":"learn = tabular_learner(dls, metrics=RocAucBinary())","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.983062Z","iopub.status.idle":"2024-07-28T05:35:08.983424Z","shell.execute_reply.started":"2024-07-28T05:35:08.983261Z","shell.execute_reply":"2024-07-28T05:35:08.983275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find(suggest_funcs=(slide,valley))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.984969Z","iopub.status.idle":"2024-07-28T05:35:08.985324Z","shell.execute_reply.started":"2024-07-28T05:35:08.985153Z","shell.execute_reply":"2024-07-28T05:35:08.985167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nlearn.fit_one_cycle(8,0.005)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.98737Z","iopub.status.idle":"2024-07-28T05:35:08.987704Z","shell.execute_reply.started":"2024-07-28T05:35:08.98754Z","shell.execute_reply":"2024-07-28T05:35:08.987554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl = test_dl\nnn_preds = learn.get_preds(dl=dl)\nnn_preds_x = learn.get_preds()[0]\na_preds, _ = learn.get_preds(dl=dl)\nnn_preds_y = a_preds.squeeze(1)\nnn_preds_proba = (a_preds[:, 1])","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:35:08.988659Z","iopub.status.idle":"2024-07-28T05:35:08.988982Z","shell.execute_reply.started":"2024-07-28T05:35:08.988821Z","shell.execute_reply":"2024-07-28T05:35:08.988835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(path/'sample_submission.csv')\nsubmit['Response'] = nn_preds_proba\nsubmit.to_csv('submission.csv', index=False)\nsub = pd.read_csv('submission.csv')\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network Ensemble","metadata":{}},{"cell_type":"code","source":"def ensemble():\n    learn = tabular_learner(dls, metrics=RocAucMulti())\n    with learn.no_bar(),learn.no_logging(): learn.fit(5, 0.005)\n    return learn.get_preds(test_dl=test_dl)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learns = [ensemble() for _ in range(5)]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ens_preds = torch.stack(learns).mean(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"%%time\nrf = RandomForestClassifier(100, min_samples_leaf=3)\nrf_model = rf.fit(X_train, y_train);\n\nrf_preds = tensor(rf_model.predict(test_dl.xs))\nrf_preds_probs = tensor(rf_model.predict_proba(test_dl.xs))[:, 1]\n\nrf_preds_x = tensor(rf_model.predict(X_test))\nrf_preds_proba = tensor(rf_model.predict_proba(X_test))[:, 1]\n\n#mse = mean_absolute_error(y_test, rf_preds_x)\n#rmse = np.sqrt(mse)\n\n#accuracy_score(y_test,rf_preds_x)\nrf_score = roc_auc_score(y_test,rf_preds_proba)\nrf_score","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:10:08.752329Z","iopub.execute_input":"2024-07-26T04:10:08.75325Z","iopub.status.idle":"2024-07-26T04:13:38.726359Z","shell.execute_reply.started":"2024-07-26T04:10:08.753207Z","shell.execute_reply":"2024-07-26T04:13:38.725358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#After dropping Driving_License column\nrf = RandomForestClassifier(100, min_samples_leaf=3)\nrf_model = rf.fit(X_train, y_train);\n\nrf_preds = tensor(rf_model.predict(test_dl.xs))\nrf_preds_probs = tensor(rf_model.predict_proba(test_dl.xs))[:, 1]\n\nrf_preds_x = tensor(rf_model.predict(X_test))\nrf_preds_proba = tensor(rf_model.predict_proba(X_test))[:, 1]\n\n#mse = mean_absolute_error(y_test, rf_preds_x)\n#rmse = np.sqrt(mse)\n\n#accuracy_score(y_test,rf_preds_x)\nrf_score = roc_auc_score(y_test,rf_preds_proba)\nrf_score","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:24:57.433625Z","iopub.execute_input":"2024-07-26T04:24:57.433997Z","iopub.status.idle":"2024-07-26T04:28:30.171253Z","shell.execute_reply.started":"2024-07-26T04:24:57.433965Z","shell.execute_reply":"2024-07-26T04:28:30.170125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#After dropping Driving_License,Gender column\nrf = RandomForestClassifier(100, min_samples_leaf=3)\nrf_model = rf.fit(X_train, y_train);\n\nrf_preds = tensor(rf_model.predict(test_dl.xs))\nrf_preds_probs = tensor(rf_model.predict_proba(test_dl.xs))[:, 1]\n\nrf_preds_x = tensor(rf_model.predict(X_test))\nrf_preds_proba = tensor(rf_model.predict_proba(X_test))[:, 1]\n\n#mse = mean_absolute_error(y_test, rf_preds_x)\n#rmse = np.sqrt(mse)\n\n#accuracy_score(y_test,rf_preds_x)\nrf_score = roc_auc_score(y_test,rf_preds_proba)\nrf_score","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:29:55.62328Z","iopub.execute_input":"2024-07-26T04:29:55.623707Z","iopub.status.idle":"2024-07-26T04:33:01.037879Z","shell.execute_reply.started":"2024-07-26T04:29:55.623673Z","shell.execute_reply":"2024-07-26T04:33:01.036977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#After dropping Driving_License,Gender,Vehicle_Age column\nrf = RandomForestClassifier(100, min_samples_leaf=3)\nrf_model = rf.fit(X_train, y_train);\n\nrf_preds = tensor(rf_model.predict(test_dl.xs))\nrf_preds_probs = tensor(rf_model.predict_proba(test_dl.xs))[:, 1]\n\nrf_preds_x = tensor(rf_model.predict(X_test))\nrf_preds_proba = tensor(rf_model.predict_proba(X_test))[:, 1]\n\n#mse = mean_absolute_error(y_test, rf_preds_x)\n#rmse = np.sqrt(mse)\n\n#accuracy_score(y_test,rf_preds_x)\nrf_score = roc_auc_score(y_test,rf_preds_proba)\nrf_score","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:37:43.292505Z","iopub.execute_input":"2024-07-26T04:37:43.292863Z","iopub.status.idle":"2024-07-26T04:40:57.503899Z","shell.execute_reply.started":"2024-07-26T04:37:43.292836Z","shell.execute_reply":"2024-07-26T04:40:57.502929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RandomForestClassifier??","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:13:38.728167Z","iopub.execute_input":"2024-07-26T04:13:38.728547Z","iopub.status.idle":"2024-07-26T04:13:38.733019Z","shell.execute_reply.started":"2024-07-26T04:13:38.728518Z","shell.execute_reply":"2024-07-26T04:13:38.732039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Importance","metadata":{}},{"cell_type":"code","source":"def rf_feat_importance(m, train_subset):\n    return pd.DataFrame({'cols':train_subset.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:14:12.274011Z","iopub.execute_input":"2024-07-26T04:14:12.274607Z","iopub.status.idle":"2024-07-26T04:14:12.279946Z","shell.execute_reply.started":"2024-07-26T04:14:12.274576Z","shell.execute_reply":"2024-07-26T04:14:12.278899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fi = rf_feat_importance(rf_model, X_train)\n#fi[:10]\n\nfi","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:14:12.787119Z","iopub.execute_input":"2024-07-26T04:14:12.78804Z","iopub.status.idle":"2024-07-26T04:14:12.907115Z","shell.execute_reply.started":"2024-07-26T04:14:12.788003Z","shell.execute_reply":"2024-07-26T04:14:12.906189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\n#plot_fi(fi[:30]);\nplot_fi(fi);\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:15:12.617821Z","iopub.execute_input":"2024-07-26T04:15:12.618195Z","iopub.status.idle":"2024-07-26T04:15:12.950801Z","shell.execute_reply.started":"2024-07-26T04:15:12.618164Z","shell.execute_reply":"2024-07-26T04:15:12.949838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Investigate effect of dropping columns","metadata":{}},{"cell_type":"code","source":"# Dropping a single column\ntrain_subset = train_subset.drop('Driving_License', axis=1)\n\n# Dropping multiple columns\n#train_subset_new = train_subset.drop(['Driving_License', 'Gender', 'Vehicle_Age'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:22:33.430735Z","iopub.execute_input":"2024-07-26T04:22:33.43149Z","iopub.status.idle":"2024-07-26T04:22:33.486907Z","shell.execute_reply.started":"2024-07-26T04:22:33.431442Z","shell.execute_reply":"2024-07-26T04:22:33.485881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping a single column\ntrain_subset = train_subset.drop('Gender', axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:29:11.241454Z","iopub.execute_input":"2024-07-26T04:29:11.242205Z","iopub.status.idle":"2024-07-26T04:29:11.273891Z","shell.execute_reply.started":"2024-07-26T04:29:11.242171Z","shell.execute_reply":"2024-07-26T04:29:11.272874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping a single column\ntrain_subset = train_subset.drop('Vehicle_Age', axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:37:02.833592Z","iopub.execute_input":"2024-07-26T04:37:02.833996Z","iopub.status.idle":"2024-07-26T04:37:02.8628Z","shell.execute_reply.started":"2024-07-26T04:37:02.833962Z","shell.execute_reply":"2024-07-26T04:37:02.861973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subset.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:22:45.992623Z","iopub.execute_input":"2024-07-26T04:22:45.993014Z","iopub.status.idle":"2024-07-26T04:22:46.001045Z","shell.execute_reply.started":"2024-07-26T04:22:45.992982Z","shell.execute_reply":"2024-07-26T04:22:46.000028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subset.columns","metadata":{"execution":{"iopub.status.busy":"2024-07-26T04:23:09.556337Z","iopub.execute_input":"2024-07-26T04:23:09.557297Z","iopub.status.idle":"2024-07-26T04:23:09.56607Z","shell.execute_reply.started":"2024-07-26T04:23:09.55725Z","shell.execute_reply":"2024-07-26T04:23:09.564841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting","metadata":{}},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"print(xgb.get_config())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n              \n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'booster': 'gbtree',\n    'n_estimators': 1500,       \n    'learning_rate': 0.55,                   \n    'min_child_weight': 17,      \n    'reg_lambda': 0.2,           \n    'reg_alpha': 7,              \n    'max_bin': 52000,            \n    'colsample_bytree': 0.65,    \n    'max_delta_step': 2,         \n    #'random_state': 0,\n    #'device' : 'gpu',\n    'device' : 'cuda',\n    'tree_method': 'hist'\n}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb_model = xgb.XGBClassifier(**xgb_params)\nxgb_model = xgb_model.fit(X_train, y_train)\n\nxgb_preds = tensor(xgb_model.predict(test_dl.xs))\nxgb_preds_proba = tensor(xgb_model.predict_proba(test_dl.xs))[:, 1]\n\nxgb_preds_x = tensor(xgb_model.predict(X_test))\nxgb_preds_x_proba = tensor(xgb_model.predict_proba(X_test))[:, 1]\n\nxgb_score = roc_auc_score(y_test,xgb_preds_x_proba)\nxgb_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(path/'sample_submission.csv')\nsubmit['Response'] = xgb_preds_proba\nsubmit.to_csv('submission.csv', index=False)\nsub = pd.read_csv('submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2024-07-25T10:50:53.860306Z","iopub.execute_input":"2024-07-25T10:50:53.860699Z","iopub.status.idle":"2024-07-25T10:51:14.743464Z","shell.execute_reply.started":"2024-07-25T10:50:53.860667Z","shell.execute_reply":"2024-07-25T10:51:14.742478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XgBoost feature importance","metadata":{}},{"cell_type":"code","source":"#plot_importance(xgb_model.fit(X_train, y_train))\nplot_importance(xgb_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna","metadata":{}},{"cell_type":"code","source":"%%time\ndef objective(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 100, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 300, 2000),\n        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 20000, 300000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 500),\n        'min_child_weight': trial.suggest_int('min_child_weight', 5, 30),  \n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'max_bin': trial.suggest_int('max_bin', 40000, 60000),\n        'max_delta_step': trial.suggest_int('max_delta_step', 1, 10),\n        'device_type': 'gpu'\n        \n    }\n\n    model =  xgb.XGBClassifier(**params, random_state=0)\n    \n    # Cross-validation with 5 folds using KFold\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kf, scoring='roc_auc')\n    \n    # We maximize accuracy, so we return the mean accuracy of the cross-validation\n    return np.mean(cv_results)\n\nstudy = optuna.create_study(sampler=TPESampler(n_startup_trials=30, multivariate=True, seed=0), direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\nprint('Best value:', study.best_value)\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:56:16.431686Z","iopub.execute_input":"2024-07-22T21:56:16.432093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best value:', study.best_value)\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T21:56:11.397543Z","iopub.execute_input":"2024-07-22T21:56:11.397946Z","iopub.status.idle":"2024-07-22T21:56:11.562534Z","shell.execute_reply.started":"2024-07-22T21:56:11.397913Z","shell.execute_reply":"2024-07-22T21:56:11.561019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cross validation implementation","metadata":{}},{"cell_type":"code","source":"%%time\n\nK_FOLDS = 5  # Number of folds for cross-validation\nkfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n\n# This part is just to demonstrate how the indices work\nfor train_index, test_index in kfold.split(train_df):\n    print(f'train: {len(train_index)} samples, test: {len(test_index)} samples')\n\nfold_scores = []\n\nfor train_index, val_index in kfold.split(X_train):\n    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n    \n    # Train the model on the current fold\n    Xgb_model_fold = xgb.XGBClassifier(**xgb_params)\n    Xgb_model_fold.fit(X_fold_train, y_fold_train)\n    \n    # Predict probabilities on the validation set for the current fold\n    y_pred_fold = Xgb_model_fold.predict_proba(X_fold_val)[:, 1]\n    y_pred_fold_tt = Xgb_model_fold.predict_proba(test_dl.xs)[:, 1]\n    \n    # Calculate and store the AUC-ROC score for the current fold\n    score = roc_auc_score(y_fold_val, y_pred_fold)\n    fold_scores.append(score)\n\n# Calculate the average score across all folds\navg_score = np.mean(fold_scores)\nprint(f\"Average AUC-ROC Score Across All Folds: {avg_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T05:21:40.819266Z","iopub.execute_input":"2024-07-25T05:21:40.819673Z","iopub.status.idle":"2024-07-25T05:43:36.380473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_fold_tt.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(path/'sample_submission.csv')\nsubmit['Response'] = y_pred_fold_tt\nsubmit.to_csv('submission.csv', index=False)\nsub = pd.read_csv('submission.csv')\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"lgbm_params = {'n_estimators': 1190, 'learning_rate': 0.22952000374471332, 'max_depth': 13, 'reg_alpha': 8.200152384535924,\n          'reg_lambda': 4.285393733702208, \n          'num_leaves': 100, 'subsample': 0.6497981764924947, 'colsample_bytree': 0.37368304607248115,\\\n               'device_type':'gpu'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#ds subset\nlgbm_model = lgb.LGBMClassifier(**lgbm_params)\nlgbm_model = lgbm_model.fit(X_train, y_train)\n\n#test set preds\nlgbm_preds = tensor(lgbm_model.predict(test_dl.xs))\nlgbm_preds_prob = tensor(lgbm_model.predict_proba(test_dl.xs))\nlgbm_positive_preds = (lgbm_preds_prob[:, 1])\n\n#validation set preds\nlgbm_preds_x = tensor(lgbm_model.predict(X_test))\nlgbm_preds_x_prob = tensor(lgbm_model.predict_proba(X_test))\nlgbm_positive_preds_x = (lgbm_preds_x_prob[:, 1])\n\nlgbm_score = roc_auc_score(y_test,lgbm_positive_preds_x)\nlgbm_score\n\n#lgb_preds_x_prob = tensor(lgb_model.predict_proba(X_test))\n\nlgbm_score_prob = roc_auc_score(y_test,lgbm_positive_preds_x)\nlgbm_score_prob\n","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_score_prob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nK_FOLDS = 5  # Number of folds for cross-validation\nkfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n\n# This part is just to demonstrate how the indices work\nfor train_index, test_index in kfold.split(train_df):\n    print(f'train: {len(train_index)} samples, test: {len(test_index)} samples')\n\nfold_scores = []\n\nfor train_index, val_index in kfold.split(X_train):\n    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n    \n    # Train the model on the current fold\n    lgb_model_fold = lgb.LGBMClassifier(**params)\n    lgb_model_fold.fit(X_fold_train, y_fold_train)\n    \n    # Predict probabilities on the validation set for the current fold\n    y_pred_fold = lgb_model_fold.predict_proba(X_fold_val)[:, 1]\n    y_pred_fold_tt = lgb_model_fold.predict_proba(test_dl.xs)[:, 1]\n    \n    # Calculate and store the AUC-ROC score for the current fold\n    score = roc_auc_score(y_fold_val, y_pred_fold)\n    fold_scores.append(score)\n\n# Calculate the average score across all folds\navg_score = np.mean(fold_scores)\nprint(f\"Average AUC-ROC Score Across All Folds: {avg_score}\")","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparamter optimization with Optuna","metadata":{}},{"cell_type":"code","source":"# finetuned optuna code\n%%time\n\ndef objective(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 100, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0, log=True),\n        'n_estimators': trial.suggest_int('n_estimators', 300, 1200),\n        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 20000, 300000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 500),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n        'max_depth': trial.suggest_int('max_depth', 1, 15),\n        'device_type': 'gpu'\n    }\n    \n    K_FOLDS = 5  # Number of folds for cross-validation\n    kfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n\n    fold_scores = []\n    \n    #for train_index, test_index in kfold.split(train_df):\n    #print(f'train: {len(train_index)} samples, test: {len(test_index)} samples')\n\n    for train_index, val_index in kfold.split(X_train):\n        X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n        y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n        \n        # Train the model on the current fold\n        lgb_model_fold = lgb.LGBMClassifier(**params)  # Use params, not lgbm_params\n        lgb_model_fold.fit(X_fold_train, y_fold_train)\n        \n        # Predict probabilities on the validation set for the current fold\n        y_pred_fold = lgb_model_fold.predict_proba(X_fold_val)[:, 1]\n        \n        # Calculate and store the AUC-ROC score for the current fold\n        score = roc_auc_score(y_fold_val, y_pred_fold)\n        fold_scores.append(score)\n    \n    # Calculate the average score across all folds\n    return np.mean(fold_scores)\n\n# Create and run the study\nstudy = optuna.create_study(sampler=TPESampler(n_startup_trials=30, multivariate=True, seed=0), direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\nprint('Best value:', study.best_value)\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best value:', study.best_value)\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_optuna_run2_params  =  {\n    'num_leaves': 320, \n    'learning_rate': 0.13981961408994045, \n    'n_estimators': 843,\n    'subsample_for_bin': 172567, \n    'min_child_samples': 223,\n    'reg_alpha': 0.0028770084050677926,\n    'reg_lambda': 2.3761404778025532e-05,\n    'colsample_bytree': 0.9350638004692479,\n    'subsample': 0.9727470703757719,\n    'max_depth': 6,\n    'device_type':'gpu'\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:10:45.769815Z","iopub.execute_input":"2024-07-20T05:10:45.770666Z","iopub.status.idle":"2024-07-20T05:10:45.776695Z","shell.execute_reply.started":"2024-07-20T05:10:45.77063Z","shell.execute_reply":"2024-07-20T05:10:45.77551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With Optuna tuned parameters","metadata":{}},{"cell_type":"code","source":"%%time\n#ds subset\nlgbm_model = lgb.LGBMClassifier(**lgbm_optuna_run2_params)\nlgbm_model = lgbm_model.fit(X_train, y_train)\n\n#test set preds\nlgbm_preds = tensor(lgbm_model.predict(test_dl.xs))\nlgbm_preds_prob = tensor(lgbm_model.predict_proba(test_dl.xs))\nlgbm_positive_preds = (lgbm_preds_prob[:, 1])\n\n#validation set preds\nlgbm_preds_x = tensor(lgbm_model.predict(X_test))\nlgbm_preds_x_prob = tensor(lgbm_model.predict_proba(X_test))\nlgbm_positive_preds_x = (lgbm_preds_x_prob[:, 1])\n\nlgbm_score = roc_auc_score(y_test,lgbm_positive_preds_x)\nlgbm_score\n\n#lgb_preds_x_prob = tensor(lgb_model.predict_proba(X_test))\n\nlgbm_score_prob = roc_auc_score(y_test,lgbm_positive_preds_x)\nlgbm_score_prob\n","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-20T05:10:45.778107Z","iopub.execute_input":"2024-07-20T05:10:45.778441Z","iopub.status.idle":"2024-07-20T05:30:17.626697Z","shell.execute_reply.started":"2024-07-20T05:10:45.77841Z","shell.execute_reply":"2024-07-20T05:30:17.625586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_score_prob\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:30:17.628148Z","iopub.execute_input":"2024-07-20T05:30:17.628531Z","iopub.status.idle":"2024-07-20T05:30:17.634678Z","shell.execute_reply.started":"2024-07-20T05:30:17.628494Z","shell.execute_reply":"2024-07-20T05:30:17.633702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nK_FOLDS = 5  # Number of folds for cross-validation\nkfold = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n\n# This part is just to demonstrate how the indices work\nfor train_index, test_index in kfold.split(train_df):\n    print(f'train: {len(train_index)} samples, test: {len(test_index)} samples')\n\nfold_scores = []\n\nfor train_index, val_index in kfold.split(X_train):\n    X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n    y_fold_train, y_fold_val = y_train[train_index], y_train[val_index]\n    \n    # Train the model on the current fold\n    lgb_model_fold = lgb.LGBMClassifier(**lgbm_optuna_run2_params)\n    lgb_model_fold.fit(X_fold_train, y_fold_train)\n    \n    # Predict probabilities on the validation set for the current fold\n    y_pred_fold = lgb_model_fold.predict_proba(X_fold_val)[:, 1]\n    y_pred_fold_tt = lgb_model_fold.predict_proba(test_dl.xs)[:, 1]\n    \n    # Calculate and store the AUC-ROC score for the current fold\n    score = roc_auc_score(y_fold_val, y_pred_fold)\n    fold_scores.append(score)\n\n# Calculate the average score across all folds\navg_score = np.mean(fold_scores)\nprint(f\"Average AUC-ROC Score Across All Folds: {avg_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:58:46.296257Z","iopub.execute_input":"2024-07-19T19:58:46.297273Z","iopub.status.idle":"2024-07-19T20:53:06.955701Z","shell.execute_reply.started":"2024-07-19T19:58:46.297237Z","shell.execute_reply":"2024-07-19T20:53:06.954541Z"},"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_score","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:35:04.79044Z","iopub.execute_input":"2024-07-20T05:35:04.791323Z","iopub.status.idle":"2024-07-20T05:35:05.598675Z","shell.execute_reply.started":"2024-07-20T05:35:04.791275Z","shell.execute_reply":"2024-07-20T05:35:05.597109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"code","source":"cat_optuna_params = {\n    \n    'colsample_bylevel': 0.6383474716497279,\n    'learning_rate': 0.075,\n    'max_bin': 490,\n    'depth': 9,\n    'l2_leaf_reg': 0.5,\n    'subsample': 0.8429457747642737,\n    'eval_metric':'AUC',\n    'class_names':[0, 1],\n    'iterations':5000,\n    'random_strength':0,\n    'max_leaves':512,\n    'fold_permutation_block':64,\n    'task_type': 'GPU',\n    'devices': '0:1'\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-07-28T06:18:31.679982Z","iopub.execute_input":"2024-07-28T06:18:31.680687Z","iopub.status.idle":"2024-07-28T06:18:31.68658Z","shell.execute_reply.started":"2024-07-28T06:18:31.680654Z","shell.execute_reply":"2024-07-28T06:18:31.685494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_params = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'class_names': [0, 1],\n    'learning_rate': 0.075,\n    'iterations': 3000,\n    'depth': 9,\n    'random_strength': 0,\n    'l2_leaf_reg': 0.5,\n    'max_leaves': 512,\n    'fold_permutation_block': 64,\n    'task_type': 'GPU',\n    'devices': '0:1',\n    'random_seed': 42,\n    'verbose': False,\n    'allow_writing_files': False\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-28T06:20:53.907753Z","iopub.execute_input":"2024-07-28T06:20:53.908659Z","iopub.status.idle":"2024-07-28T06:20:53.913947Z","shell.execute_reply.started":"2024-07-28T06:20:53.908625Z","shell.execute_reply":"2024-07-28T06:20:53.913041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#using full ds\ncat_model = CatBoostClassifier(**cat_params)\ncat_model = cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n\n#test set preds\ncat_preds = tensor(cat_model.predict(test_dl.xs))\ncat_preds_probs = tensor(cat_model.predict_proba(test_dl.xs))[:, 1]\n#cat_preds_final = cat_preds.squeeze(1)\n\n#validation set preds\ncat_preds_x = tensor(cat_model.predict(X_test))\ncat_preds_x_proba = tensor(cat_model.predict_proba(X_test))[:, 1]\n\n#cat_preds_x_final = cat_preds_x.squeeze(1)\n\n#accuracy_score(y_test,cat_preds_x)\n\ncat_score = roc_auc_score(y_test,cat_preds_x_proba)\ncat_score","metadata":{"execution":{"iopub.status.busy":"2024-07-28T05:59:47.636738Z","iopub.execute_input":"2024-07-28T05:59:47.637156Z","iopub.status.idle":"2024-07-28T06:08:14.457181Z","shell.execute_reply.started":"2024-07-28T05:59:47.637111Z","shell.execute_reply":"2024-07-28T06:08:14.456068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_score","metadata":{"execution":{"iopub.status.busy":"2024-07-28T06:21:03.547244Z","iopub.execute_input":"2024-07-28T06:21:03.548048Z","iopub.status.idle":"2024-07-28T06:21:03.557692Z","shell.execute_reply.started":"2024-07-28T06:21:03.548007Z","shell.execute_reply":"2024-07-28T06:21:03.556829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#using full ds\ncat_model = CatBoostClassifier(**cat_params)\ncat_model = cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n\n#test set preds\ncat_preds = tensor(cat_model.predict(test_dl.xs))\ncat_preds_probs = tensor(cat_model.predict_proba(test_dl.xs))[:, 1]\n#cat_preds_final = cat_preds.squeeze(1)\n\n#validation set preds\ncat_preds_x = tensor(cat_model.predict(X_test))\ncat_preds_x_proba = tensor(cat_model.predict_proba(X_test))[:, 1]\n\n#cat_preds_x_final = cat_preds_x.squeeze(1)\n\n#accuracy_score(y_test,cat_preds_x)\n\ncat_score = roc_auc_score(y_test,cat_preds_x_proba)\ncat_score","metadata":{"execution":{"iopub.status.busy":"2024-07-28T06:21:11.88477Z","iopub.execute_input":"2024-07-28T06:21:11.885178Z","iopub.status.idle":"2024-07-28T06:26:35.408104Z","shell.execute_reply.started":"2024-07-28T06:21:11.885144Z","shell.execute_reply":"2024-07-28T06:26:35.407162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optuna optimization","metadata":{}},{"cell_type":"code","source":"%%time\n#will you graduate notebook\ndef objective(trial):\n    param = {\n        'iterations': trial.suggest_int('iterations', 100, 6000),\n        'depth': trial.suggest_int('depth', 4, 15),\n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10, log=True),\n        'border_count': trial.suggest_int('border_count', 32, 255),\n        'random_strength': trial.suggest_float('random_strength', 1e-2, 10, log=True),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.01, 100.00, log=True),\n        'od_type': 'Iter',\n        'od_wait': trial.suggest_int('od_wait', 10, 50),\n        'task_type': 'GPU',  # Set task type to GPU\n        'devices': '0:1',    # Use both GPUs\n        'eval_metric': 'AUC',\n        'loss_function': 'Logloss',\n        'class_names': [0, 1],\n        #'max_leaves': trial.suggest_int('max_leaves', 300, 1000), \n        \n    }\n\n    cat_model = CatBoostClassifier(**param, verbose=0)\n    cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=0)\n\n    cat_preds_proba = cat_model.predict_proba(X_test)\n    cat_preds_proba = (cat_preds_proba[:, 1])\n    cat_proba_score = roc_auc_score(y_test,cat_preds_proba)\n    return cat_proba_score\n\n# UNCOMMENT THE FOLLOWING LINES TO RUN OPTUNA, I COMMENTED THEM OUT TO SAVE SOME TIME WHILE PUBLISHING\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100, show_progress_bar=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-28T09:10:56.003251Z","iopub.execute_input":"2024-07-28T09:10:56.003626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best value:', study.best_value)\nprint('Best trial:', study.best_trial.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[I 2024-07-28 13:10:00,811] Trial 35 finished with value: 0.8806571694660618 and parameters: {'iterations': 3922, 'depth': 7, 'learning_rate': 0.06785132188911386, 'l2_leaf_reg': 0.010246847658225403, 'border_count': 255, 'random_strength': 8.699168928480285, 'bagging_temperature': 0.07986828861909359, 'od_wait': 36}. Best is trial 35 with value: 0.8806571694660618.\n\n[I 2024-07-28 14:53:15,848] Trial 59 finished with value: 0.880659558073007 and parameters: {'iterations': 4770, 'depth': 9, 'learning_rate': 0.0751956964415004, 'l2_leaf_reg': 0.18411119632584752, 'border_count': 248, 'random_strength': 0.1621690161205084, 'bagging_temperature': 0.3374322444670465, 'od_wait': 38}. Best is trial 59 with value: 0.880659558073007.\n\n[I 2024-07-28 15:05:48,337] Trial 61 finished with value: 0.880751074531679 and parameters: {'iterations': 4801, 'depth': 9, 'learning_rate': 0.07092162741545756, 'l2_leaf_reg': 0.14524960916052582, 'border_count': 249, 'random_strength': 0.16969220470731253, 'bagging_temperature': 0.05031377942990587, 'od_wait': 43}. Best is trial 61 with value: 0.880751074531679.","metadata":{}},{"cell_type":"code","source":"silva_optuna_cat_params = {\n    'iterations': 906, 'depth': 10,\n    'learning_rate': 0.09919645690545881,\n    'l2_leaf_reg': 0.4254495849561265,\n    'border_count': 248,\n    'random_strength': 2.655971961645386,\n    'bagging_temperature': 0.0913538359007028,\n    'od_wait': 46,\n    'task_type': 'GPU',  # Set task type to GPU\n    'devices': '0:1',    # Use both GPUs\n    'eval_metric': 'AUC',\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"silva_optuna_cat_params","metadata":{"execution":{"iopub.status.busy":"2024-07-28T08:54:16.943633Z","iopub.execute_input":"2024-07-28T08:54:16.944373Z","iopub.status.idle":"2024-07-28T08:54:16.950405Z","shell.execute_reply.started":"2024-07-28T08:54:16.944339Z","shell.execute_reply":"2024-07-28T08:54:16.949445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#using full ds\ncat_model = CatBoostClassifier(**silva_optuna_cat_params)\ncat_model = cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n\n#test set preds\ncat_preds = tensor(cat_model.predict(test_dl.xs))\ncat_preds_probs = tensor(cat_model.predict_proba(test_dl.xs))[:, 1]\n#cat_preds_final = cat_preds.squeeze(1)\n\n#validation set preds\ncat_preds_x = tensor(cat_model.predict(X_test))\ncat_preds_x_proba = tensor(cat_model.predict_proba(X_test))[:, 1]\n\n#cat_preds_x_final = cat_preds_x.squeeze(1)\n\n#accuracy_score(y_test,cat_preds_x)\n\ncat_score = roc_auc_score(y_test,cat_preds_x_proba)\ncat_score","metadata":{"execution":{"iopub.status.busy":"2024-07-28T08:54:19.07167Z","iopub.execute_input":"2024-07-28T08:54:19.072574Z","iopub.status.idle":"2024-07-28T08:56:25.193429Z","shell.execute_reply.started":"2024-07-28T08:54:19.072533Z","shell.execute_reply":"2024-07-28T08:56:25.192605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_score","metadata":{"execution":{"iopub.status.busy":"2024-07-28T08:53:47.770764Z","iopub.execute_input":"2024-07-28T08:53:47.771143Z","iopub.status.idle":"2024-07-28T08:53:47.777018Z","shell.execute_reply.started":"2024-07-28T08:53:47.771089Z","shell.execute_reply":"2024-07-28T08:53:47.776174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Predictions","metadata":{}},{"cell_type":"code","source":"model_preds = {\n    \"random forests\":roc_auc_score(y_test,rf_preds_x),\n    \"cat boost\":cat_score,\n    \"lgbm\":lgb_score,\n    \"xgboost\":roc_auc_score(y_test,xgb_preds_x),   \n}\n\n#model_preds_a = model_preds.sort()\nprint(model_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_preds_x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Ensemble","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_preds_final = (lgbm_preds_prob[:, 1])","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:37:54.854046Z","iopub.execute_input":"2024-07-20T05:37:54.855065Z","iopub.status.idle":"2024-07-20T05:37:54.86004Z","shell.execute_reply.started":"2024-07-20T05:37:54.855018Z","shell.execute_reply":"2024-07-20T05:37:54.858875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv(path/'sample_submission.csv')\nsubmit['Response'] = lgbm_preds_final \nsubmit.to_csv('submission.csv', index=False)\nsub = pd.read_csv('submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:38:27.760183Z","iopub.execute_input":"2024-07-20T05:38:27.760561Z","iopub.status.idle":"2024-07-20T05:38:49.302705Z","shell.execute_reply.started":"2024-07-20T05:38:27.760525Z","shell.execute_reply":"2024-07-20T05:38:49.30165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-07-20T05:38:49.304509Z","iopub.execute_input":"2024-07-20T05:38:49.304895Z","iopub.status.idle":"2024-07-20T05:38:50.376234Z","shell.execute_reply.started":"2024-07-20T05:38:49.304859Z","shell.execute_reply":"2024-07-20T05:38:50.374995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OpenFE","metadata":{}},{"cell_type":"code","source":"ofe = OpenFE()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:20:33.437144Z","iopub.execute_input":"2024-07-29T08:20:33.437563Z","iopub.status.idle":"2024-07-29T08:20:33.442458Z","shell.execute_reply.started":"2024-07-29T08:20:33.437523Z","shell.execute_reply":"2024-07-29T08:20:33.441215Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"#set n_jobs to be the actual cpu core count\nCPU_COUNT = os.cpu_count()\nn_jobs = CPU_COUNT\nn_jobs\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:20:33.443829Z","iopub.execute_input":"2024-07-29T08:20:33.444163Z","iopub.status.idle":"2024-07-29T08:20:33.45424Z","shell.execute_reply.started":"2024-07-29T08:20:33.444133Z","shell.execute_reply":"2024-07-29T08:20:33.453354Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"train_subset.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:20:33.455317Z","iopub.execute_input":"2024-07-29T08:20:33.455644Z","iopub.status.idle":"2024-07-29T08:20:33.467141Z","shell.execute_reply.started":"2024-07-29T08:20:33.455615Z","shell.execute_reply":"2024-07-29T08:20:33.466342Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"(1000000, 11)"},"metadata":{}}]},{"cell_type":"code","source":"train_subset.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:20:33.468187Z","iopub.execute_input":"2024-07-29T08:20:33.468512Z","iopub.status.idle":"2024-07-29T08:20:33.485401Z","shell.execute_reply.started":"2024-07-29T08:20:33.468462Z","shell.execute_reply":"2024-07-29T08:20:33.484424Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"         Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\nid                                                                       \n2904941    Male   25                1          9.0                   0   \n7879547  Female   21                1         28.0                   1   \n1554558  Female   31                1         15.0                   0   \n5792501    Male   25                1         10.0                   1   \n7511622    Male   66                1         28.0                   1   \n\n        Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\nid                                                                         \n2904941    < 1 Year            Yes         33990.0                 152.0   \n7879547    < 1 Year             No         49514.0                 152.0   \n1554558    < 1 Year             No         36913.0                 152.0   \n5792501    < 1 Year             No         36274.0                 152.0   \n7511622    1-2 Year            Yes         44214.0                  26.0   \n\n         Vintage  Response  \nid                          \n2904941      166         0  \n7879547       84         0  \n1554558      149         0  \n5792501      256         0  \n7511622      223         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Driving_License</th>\n      <th>Region_Code</th>\n      <th>Previously_Insured</th>\n      <th>Vehicle_Age</th>\n      <th>Vehicle_Damage</th>\n      <th>Annual_Premium</th>\n      <th>Policy_Sales_Channel</th>\n      <th>Vintage</th>\n      <th>Response</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2904941</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>Yes</td>\n      <td>33990.0</td>\n      <td>152.0</td>\n      <td>166</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7879547</th>\n      <td>Female</td>\n      <td>21</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>49514.0</td>\n      <td>152.0</td>\n      <td>84</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1554558</th>\n      <td>Female</td>\n      <td>31</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36913.0</td>\n      <td>152.0</td>\n      <td>149</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5792501</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36274.0</td>\n      <td>152.0</td>\n      <td>256</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7511622</th>\n      <td>Male</td>\n      <td>66</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>1-2 Year</td>\n      <td>Yes</td>\n      <td>44214.0</td>\n      <td>26.0</td>\n      <td>223</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nfeatures = ofe.fit(data=train_subset.drop('Response', axis=1), label=train_subset['Response'], feature_boosting=True, n_jobs=n_jobs)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-29T08:20:33.486616Z","iopub.execute_input":"2024-07-29T08:20:33.486873Z","iopub.status.idle":"2024-07-29T08:51:57.773413Z","shell.execute_reply.started":"2024-07-29T08:20:33.486851Z","shell.execute_reply":"2024-07-29T08:51:57.772206Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187546 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 735\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122876 -> initscore=-1.965470\n[LightGBM] [Info] Start training from score -1.965470\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[63]\tvalid_0's binary_logloss: 0.256062\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062696 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 734\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122876 -> initscore=-1.965470\n[LightGBM] [Info] Start training from score -1.965470\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[62]\tvalid_0's binary_logloss: 0.256561\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171736 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 735\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122876 -> initscore=-1.965470\n[LightGBM] [Info] Start training from score -1.965470\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[65]\tvalid_0's binary_logloss: 0.256618\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176713 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 737\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122876 -> initscore=-1.965470\n[LightGBM] [Info] Start training from score -1.965470\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[70]\tvalid_0's binary_logloss: 0.256531\n[LightGBM] [Info] Number of positive: 98300, number of negative: 701700\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038459 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 739\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.122875 -> initscore=-1.965482\n[LightGBM] [Info] Start training from score -1.965482\nTraining until validation scores don't improve for 200 rounds\nEarly stopping, best iteration is:\n[66]\tvalid_0's binary_logloss: 0.25696\nThe number of candidate features is 543\nStart stage I selection.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/16 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002740 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002819 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 2\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004478 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 71[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002730 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 101\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002922 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 224\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002769 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002813 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 29\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004414 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004627 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 87\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 240\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002575 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002726 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002725 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002629 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004804 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 104[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 62[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002698 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002770 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002741 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002802 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002836 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 34[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004652 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Total Bins 53\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002665 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002873 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 33\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002630 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002727 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 107\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002793 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002799 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 103\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001704 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002539 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 114\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002767 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 4\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002756 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003174 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004614 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002707 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 240\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 109[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002653 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002618 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002677 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 251\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 5\n\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n","output_type":"stream"},{"name":"stderr","text":"  6%|         | 1/16 [00:53<13:26, 53.77s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 43\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002840 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 90[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002738 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 237\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002808 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 103\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002763 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002716 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004444 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 154\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004513 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002747 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 7[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002652 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002994 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002772 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 120\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 73\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002639 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 243\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 243[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002914 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 203\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002632 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002639 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002708 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf","output_type":"stream"},{"name":"stderr","text":" 12%|        | 2/16 [01:13<07:49, 33.55s/it]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 19%|        | 3/16 [01:13<03:58, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005728 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003099 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002583 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002725 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002659 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002739 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 54[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002610 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002592 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 25%|       | 4/16 [01:29<03:28, 17.37s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002825 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002766 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000891 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002661 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002612 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004756 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n\n[LightGBM] [Info] Total Bins 155[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003127 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002749 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 160\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Total Bins 54\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002757 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003231 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002651 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 240\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002699 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002661 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004464 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004521 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002809 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002804 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 31%|      | 5/16 [01:54<03:43, 20.31s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004629 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 193\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 116\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002623 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002772 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002606 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002625 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002643 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004871 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002640 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 59\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002766 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 248\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002640 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002722 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 19\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002691 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 15\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002704 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 15\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002813 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002621 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 73\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002721 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002612 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002632 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002677 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004726 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 243\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002924 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002669 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002707 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Info] Total Bins 62\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 2\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004482 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 63\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002707 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 7[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002728 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 5[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004442 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002614 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004618 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002712 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 159\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004519 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 195\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002925 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002658 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002659 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002637 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 23\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 38%|      | 6/16 [02:25<03:59, 23.98s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002616 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002621 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002733 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002613 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 54[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n","output_type":"stream"},{"name":"stderr","text":" 44%|     | 7/16 [02:31<02:41, 17.98s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002725 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002642 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002828 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n\n[LightGBM] [Info] Total Bins 4[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 50%|     | 8/16 [02:38<01:56, 14.52s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002628 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002628 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 121\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002688 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002634 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002627 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002650 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 251\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 88\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002922 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 5[LightGBM] [Info] Total Bins 5\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002661 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002706 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002673 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000746 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Total Bins 5[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 130\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002697 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Total Bins 217\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements","output_type":"stream"},{"name":"stderr","text":" 56%|    | 9/16 [02:58<01:52, 16.04s/it]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004281 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 42\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 115\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002602 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 155\n[LightGBM] [Info] Total Bins 63[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002664 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004665 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002865 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002708 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004331 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 137[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002688 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004523 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002480 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2342\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002692 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 29\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002795 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 255\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002609 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002726 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002595 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002636 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 112\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002604 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 93\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 198\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002814 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 12\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 111\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 53\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002884 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002834 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002626 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002619 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 72\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 39\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002868 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002728 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n","output_type":"stream"},{"name":"stderr","text":" 62%|   | 10/16 [03:42<02:29, 24.96s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002681 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005277 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","output_type":"stream"},{"name":"stderr","text":" 69%|   | 11/16 [03:43<01:28, 17.63s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002719 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002752 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002870 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 255[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002616 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 104\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 75%|  | 12/16 [03:47<00:53, 13.37s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002129 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002684 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002641 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Total Bins 53\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002612 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002611 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 50\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002638 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 225\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 62\n\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Total Bins 5\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 123\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002712 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005757 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002771 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 9\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002651 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002682 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000824 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n\n[LightGBM] [Info] Total Bins 8\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002852 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002872 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002776 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002954 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003058 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002754 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004478 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 73\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002810 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 0\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002736 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 120\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002938 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 78\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 81%| | 13/16 [04:20<00:58, 19.41s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002804 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 235\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002667 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004329 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002660 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002895 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004733 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002740 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002556 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002647 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n\n[LightGBM] [Info] Total Bins 3[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002781 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 178\n\n\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002724 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002644 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 62\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 78\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002723 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003254 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002892 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002494 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 95\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002632 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255","output_type":"stream"},{"name":"stderr","text":" 88%| | 14/16 [04:48<00:43, 21.75s/it]","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 94%|| 15/16 [04:49<00:15, 15.53s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 12090, number of negative: 87910\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|| 16/16 [04:50<00:00, 18.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"292 same features have been deleted.\nMeet early-stopping in successive feature-wise halving.\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/16 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019482 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022120 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 22\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020009 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 93\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020748 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020950 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018224 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 160\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021363 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021405 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 92\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030794 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 205\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021835 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041064 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040973 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 153\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028356 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011874 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018539 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017727 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048753 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 126\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015716 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025138 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018083 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039526 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Total Bins 251\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032066 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 158\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018873 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041353 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 243\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 29\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034801 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 82\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021024 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020855 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 84\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022810 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021821 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 52\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 50\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 119\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036748 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":"  6%|         | 1/16 [02:40<40:04, 160.30s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023445 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 9\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022909 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021025 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005601 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038443 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020890 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 203\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022081 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005414 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 19%|        | 3/16 [03:13<10:09, 46.91s/it] ","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020093 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 112\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020669 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021125 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020645 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023020 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023710 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 247\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 25%|       | 4/16 [03:44<08:03, 40.26s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 161\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020631 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 239\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032392 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020671 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036542 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022632 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 101\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 122\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 79\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 247\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036020 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023014 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 246\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035893 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023148 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 132\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 105\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021125 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035771 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005487 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021066 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022380 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 110\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022005 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 201\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023633 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020624 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 31%|      | 5/16 [05:04<10:02, 54.76s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020817 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005835 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020086 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010257 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 126\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036827 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 83\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 186\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021388 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020927 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021065 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 19\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021107 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020666 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023274 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021722 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 38%|      | 6/16 [05:45<08:20, 50.02s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005591 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020793 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021213 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021318 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 93\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 39\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020002 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025842 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021563 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 105\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021704 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021872 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 250\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 44%|     | 7/16 [06:54<08:25, 56.19s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021629 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005589 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022732 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019895 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 79\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023462 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 50%|     | 8/16 [07:18<06:07, 45.91s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021663 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 166\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021060 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 243\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021500 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021274 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005753 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 29\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020922 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021842 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022531 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023884 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 99\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022736 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022803 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021329 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 106\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 125\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 63\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 56%|    | 9/16 [08:32<06:23, 54.74s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007347 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023311 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 10\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 62%|   | 10/16 [08:47<04:14, 42.50s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041612 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022887 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 193\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022839 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 168\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021265 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021938 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022129 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 218\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 158\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022029 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035517 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021807 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019658 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 19\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018233 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022772 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 102\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 238\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005482 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006179 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Info] Total Bins 5\n\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021023 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 69%|   | 11/16 [09:41<03:50, 46.19s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020682 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020926 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 112\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021856 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 234\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022088 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021751 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 198\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020965 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 107\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019507 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2391\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 242\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020963 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 75%|  | 12/16 [10:28<03:05, 46.45s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026086 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 112\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 241\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027197 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 186\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022030 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.[LightGBM] [Info] Total Bins 3\n\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036790 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" 81%| | 13/16 [10:48<01:55, 38.43s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021976 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021302 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 126\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021285 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004587 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 4\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","output_type":"stream"},{"name":"stderr","text":" 88%| | 14/16 [11:09<01:06, 33.01s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004797 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007468 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 53\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015319 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 21\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015708 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Total Bins 0\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 0\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015107 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022577 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 54\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 132\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 64\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015237 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 99\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":" 94%|| 15/16 [12:01<00:38, 38.67s/it]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 124\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015035 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 204\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016619 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 47\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 255\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|| 16/16 [12:38<00:00, 47.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"The number of remaining candidate features is 241\nStart stage II selection.\n","output_type":"stream"},{"name":"stderr","text":"100%|| 16/16 [07:28<00:00, 28.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Finish data processing.\n[LightGBM] [Info] Number of positive: 98301, number of negative: 701699\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.175070 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 30052\n[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 248\nCPU times: user 19min 26s, sys: 22.3 s, total: 19min 49s\nWall time: 31min 24s\n","output_type":"stream"}]},{"cell_type":"code","source":"train_x, test_x = transform(train_subset.drop('Response', axis=1), test_subset, features, n_jobs=n_jobs)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:51:57.775029Z","iopub.execute_input":"2024-07-29T08:51:57.775677Z","iopub.status.idle":"2024-07-29T09:04:21.047064Z","shell.execute_reply.started":"2024-07-29T08:51:57.775641Z","shell.execute_reply":"2024-07-29T09:04:21.045898Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"#train_x, test_x = transform(train_df.drop('Response', axis=1), test_df, features, n_jobs=4)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T08:04:49.314854Z","iopub.status.idle":"2024-07-29T08:04:49.31518Z","shell.execute_reply.started":"2024-07-29T08:04:49.315016Z","shell.execute_reply":"2024-07-29T08:04:49.31503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:04:21.050686Z","iopub.execute_input":"2024-07-29T09:04:21.050982Z","iopub.status.idle":"2024-07-29T09:04:21.282358Z","shell.execute_reply.started":"2024-07-29T09:04:21.050955Z","shell.execute_reply":"2024-07-29T09:04:21.281461Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"              Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\nopenfe_index                                                                  \n2904941         Male   25                1          9.0                   0   \n7879547       Female   21                1         28.0                   1   \n1554558       Female   31                1         15.0                   0   \n5792501         Male   25                1         10.0                   1   \n7511622         Male   66                1         28.0                   1   \n\n             Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\nopenfe_index                                                                    \n2904941         < 1 Year            Yes         33990.0                 152.0   \n7879547         < 1 Year             No         49514.0                 152.0   \n1554558         < 1 Year             No         36913.0                 152.0   \n5792501         < 1 Year             No         36274.0                 152.0   \n7511622         1-2 Year            Yes         44214.0                  26.0   \n\n              Vintage  autoFE_f_0  autoFE_f_1  autoFE_f_2  autoFE_f_3  \\\nopenfe_index                                                            \n2904941           166   5642340.0    0.480421       186.0      5109.0   \n7879547            84   4159176.0    0.190024        82.0      8441.0   \n1554558           149   5500037.0    0.451416        25.0      4808.0   \n5792501           256   9286144.0    0.829111       106.0     11913.0   \n7511622           223   9859722.0    0.708394        93.0      4884.0   \n\n              autoFE_f_4  autoFE_f_5  autoFE_f_6  autoFE_f_7 autoFE_f_8  \\\nopenfe_index                                                              \n2904941         542950.0    0.178632       166.0    305910.0        0.0   \n7879547         542950.0    0.477880        84.0   1386392.0        1.0   \n1554558         542950.0    0.248792       149.0    553695.0        2.0   \n5792501         542950.0    0.195258       256.0    362740.0        1.0   \n7511622         317263.0    0.485734       223.0   1237992.0        3.0   \n\n              autoFE_f_9  autoFE_f_10  autoFE_f_11  autoFE_f_12  autoFE_f_13  \\\nopenfe_index                                                                   \n2904941            166.0        166.0     0.653033          NaN     0.380000   \n7879547             83.0         85.0     0.785224         84.0     0.007696   \n1554558            149.0        149.0     0.741447          NaN     0.645197   \n5792501            255.0        257.0     0.921735        256.0     0.466462   \n7511622            222.0        224.0     0.609766        223.0     0.906301   \n\n              autoFE_f_14  autoFE_f_15  autoFE_f_16  autoFE_f_17  autoFE_f_18  \\\nopenfe_index                                                                    \n2904941         14.485116     0.529460     0.510935     0.280615      33990.0   \n7879547         13.832753     0.923739     0.194659     0.842734      49515.0   \n1554558         13.084419     0.715309     0.432102     0.260368      36913.0   \n5792501         11.027021     0.700654     0.835286     0.672440      36275.0   \n7511622         13.832753     0.790905     0.725049     0.842734      44215.0   \n\n              autoFE_f_19  autoFE_f_20  autoFE_f_21  autoFE_f_22  autoFE_f_23  \\\nopenfe_index                                                                    \n2904941             191.0       6910.0     0.573447      34156.0     0.537776   \n7879547             105.0      79439.0     0.908039      49598.0     0.922583   \n1554558             180.0      34800.0     0.670870      37062.0     0.634359   \n5792501             281.0      12342.0     0.655816      36530.0     0.697934   \n7511622             289.0     332339.0     0.821479      44437.0     0.849935   \n\n              autoFE_f_24  autoFE_f_25  autoFE_f_26  autoFE_f_27  autoFE_f_28  \\\nopenfe_index                                                                    \n2904941           33990.0      34015.0  1359.600000     0.509686     0.500451   \n7879547           49514.0      49535.0  2357.809524     0.194317     0.205071   \n1554558           36913.0      36944.0  1190.741935     0.446585     0.417740   \n5792501           36274.0      36299.0  1450.960000     0.835485     0.824923   \n7511622           44214.0      44280.0   669.909091     0.701396     0.717548   \n\n              autoFE_f_29 autoFE_f_30  autoFE_f_31  autoFE_f_32  autoFE_f_33  \\\nopenfe_index                                                                   \n2904941          0.487574         0.0    34.986475     0.589180   223.618421   \n7879547          0.190794         1.0    45.709578     0.867675   325.750000   \n1554558          0.430972         2.0    32.053954     0.553907   242.848684   \n5792501          0.836364         3.0    30.023605     0.539638   238.644737   \n7511622          0.718992         1.0    45.709578     0.223135  1700.538462   \n\n              autoFE_f_34  autoFE_f_35  autoFE_f_36  autoFE_f_37  autoFE_f_38  \\\nopenfe_index                                                                    \n2904941          0.499175      17590.0       6078.0         28.0    78.856796   \n7879547          0.266412      37064.0     191241.0         45.0    79.938583   \n1554558          0.546335      11290.0      26554.0         26.0    80.427887   \n5792501          0.499175      64870.0       7566.0         25.0    82.260215   \n7511622          0.303853       6696.0     258450.0         45.0    79.938583   \n\n              autoFE_f_39  autoFE_f_40   autoFE_f_41  autoFE_f_42  \\\nopenfe_index                                                        \n2904941             175.0        895.0  14079.233812     0.643177   \n7879547             112.0       4709.0  15600.277416     0.937936   \n1554558             164.0        855.0  13766.828032     0.746904   \n5792501             266.0       1847.0  11346.682460     0.731384   \n7511622             251.0       4950.0  15600.277416     0.787907   \n\n              autoFE_f_43  autoFE_f_44  autoFE_f_45  autoFE_f_46 autoFE_f_47  \\\nopenfe_index                                                                   \n2904941            6846.0    16.888889     0.625551     0.162050         0.0   \n7879547          141443.0     5.428571     0.080719     0.662050         1.0   \n1554558           27245.0    10.133333     0.946592     0.162050         2.0   \n5792501           10686.0    15.200000     0.625551     0.662050         3.0   \n7511622          141443.0     0.928571     0.903795     0.839407         4.0   \n\n               autoFE_f_48  autoFE_f_49 autoFE_f_50  autoFE_f_51  autoFE_f_52  \\\nopenfe_index                                                                    \n2904941       26902.168033      30648.0         0.0     0.554006   125.991393   \n7879547       39558.794219      39802.0         1.0     0.913898    86.787510   \n1554558       28959.286744      31574.5         1.0     0.690318   132.888517   \n5792501       24892.287308      28015.0         0.0     0.638378   139.645371   \n7511622       39558.794219      39802.0         2.0     0.810001    86.787510   \n\n              autoFE_f_53 autoFE_f_54  autoFE_f_55  autoFE_f_56  autoFE_f_57  \\\nopenfe_index                                                                   \n2904941           33965.0         0.0     0.225569     0.499973        166.0   \n7879547           49493.0         1.0     0.043722     0.201356         84.0   \n1554558           36882.0         2.0     0.522780     0.439885        149.0   \n5792501           36249.0         3.0     0.225569     0.842750        256.0   \n7511622           44148.0         1.0     0.938345     0.713352        223.0   \n\n              autoFE_f_58  autoFE_f_59 autoFE_f_60  autoFE_f_61  autoFE_f_62  \\\nopenfe_index                                                                   \n2904941         5166480.0          NaN         0.0     0.505359    79.646454   \n7879547         7526128.0      49514.0         1.0     0.195420    79.342478   \n1554558         5610776.0          NaN         2.0     0.431017    79.379644   \n5792501         5513648.0      36274.0         3.0     0.840829    79.646454   \n7511622         1149564.0      44214.0         4.0     0.714188    79.709674   \n\n              autoFE_f_63 autoFE_f_64  autoFE_f_65  autoFE_f_66  autoFE_f_67  \\\nopenfe_index                                                                   \n2904941             127.0         0.0     6.640000        165.0     849750.0   \n7879547             131.0         1.0     4.000000         83.0    1039794.0   \n1554558             121.0         2.0     4.806452        148.0    1144303.0   \n5792501             127.0         0.0    10.240000        255.0     906850.0   \n7511622             -40.0         3.0     3.378788        222.0    2918124.0   \n\n              autoFE_f_68  autoFE_f_69  autoFE_f_70  autoFE_f_71  autoFE_f_72  \\\nopenfe_index                                                                    \n2904941          0.438852        167.0      33990.0     0.466655        177.0   \n7879547          0.314534         85.0      49514.0     0.466655        173.0   \n1554558          0.479282        150.0      36913.0     0.466655        183.0   \n5792501          0.655181        257.0      36274.0     0.466655        177.0   \n7511622          0.314534        224.0      44214.0     0.235340         92.0   \n\n              autoFE_f_73  autoFE_f_74  autoFE_f_75  autoFE_f_76  autoFE_f_77  \\\nopenfe_index                                                                    \n2904941           34142.0      81240.0      35526.0        318.0     290594.0   \n7879547           49666.0      63813.0      36329.0        236.0     488138.0   \n1554558           37065.0      15033.0       9979.0        301.0     346200.0   \n5792501           36426.0      81240.0      35526.0        408.0     290594.0   \n7511622           44240.0       9586.0       6662.0        249.0     308615.0   \n\n              autoFE_f_78  autoFE_f_79  autoFE_f_80  autoFE_f_81  autoFE_f_82  \\\nopenfe_index                                                                    \n2904941            1494.0         25.0     0.496267      33999.0        152.0   \n7879547            2352.0         28.0     0.464330      49542.0         84.0   \n1554558            2235.0         31.0     0.499575      36928.0        149.0   \n5792501            2560.0         25.0     0.475324      36284.0        152.0   \n7511622            6244.0         66.0     0.464330      44242.0         26.0   \n\n              autoFE_f_83  autoFE_f_84  autoFE_f_85  autoFE_f_86  autoFE_f_87  \\\nopenfe_index                                                                    \n2904941            3800.0      33990.0        143.0        166.0        157.0   \n7879547            3192.0      49513.0        124.0         84.0         56.0   \n1554558            4712.0      36913.0        137.0        149.0        134.0   \n5792501            3800.0      36273.0        142.0        256.0        246.0   \n7511622            1716.0      44213.0         -2.0        223.0        195.0   \n\n              autoFE_f_88  autoFE_f_89  autoFE_f_90  autoFE_f_91  autoFE_f_92  \\\nopenfe_index                                                                    \n2904941           82460.0     0.998033      33838.0         80.0        225.0   \n7879547           65868.0     0.997138      49362.0         85.0        588.0   \n1554558           21118.0     0.998624      36761.0         81.0        465.0   \n5792501           82460.0     0.999387      36122.0         80.0        250.0   \n7511622           10791.0     0.997138      44188.0         85.0       1848.0   \n\n              autoFE_f_93  autoFE_f_94 autoFE_f_95  autoFE_f_96  autoFE_f_97  \\\nopenfe_index                                                                   \n2904941          0.147187   204.759036         0.0     751966.0         25.0   \n7879547          0.492940   589.452381         1.0     745026.0         21.0   \n1554558          0.286846   247.738255         2.0     745026.0         31.0   \n5792501          0.193205   141.695312         3.0     745026.0         25.0   \n7511622          0.496518   198.269058         4.0     751966.0         66.0   \n\n              autoFE_f_98  autoFE_f_99  autoFE_f_100  autoFE_f_101  \\\nopenfe_index                                                         \n2904941            6063.0      33981.0         -14.0  12986.968804   \n7879547          152580.0      49486.0          68.0  13812.283547   \n1554558           29458.0      36898.0           3.0  17397.458965   \n5792501           11198.0      36264.0        -104.0  12986.968804   \n7511622          297111.0      44186.0        -197.0  19314.044119   \n\n              autoFE_f_102  autoFE_f_103  autoFE_f_104  autoFE_f_105  \\\nopenfe_index                                                           \n2904941        3776.666667      2.777778      0.129475       12176.0   \n7879547        1768.357143      0.750000      0.730673      448404.0   \n1554558        2460.866667      2.066667      0.254759       52250.0   \n5792501        3627.400000      2.500000      0.629475       16300.0   \n7511622        1579.071429      2.357143      0.813899      448404.0   \n\n              autoFE_f_106 autoFE_f_107  autoFE_f_108 autoFE_f_109  \\\nopenfe_index                                                         \n2904941              152.0          0.0          16.0          0.0   \n7879547              152.0          1.0          -7.0          1.0   \n1554558              152.0          2.0          16.0          2.0   \n5792501              152.0          3.0          15.0          0.0   \n7511622               26.0          4.0          38.0          3.0   \n\n              autoFE_f_110  autoFE_f_111  autoFE_f_112  autoFE_f_113  \\\nopenfe_index                                                           \n2904941                NaN        4150.0      0.752077          25.0   \n7879547              152.0        1764.0      0.627876          21.0   \n1554558                NaN        4619.0      0.627876          31.0   \n5792501              152.0        6400.0      0.627876          25.0   \n7511622               26.0       14718.0      0.187160          26.0   \n\n              autoFE_f_114  autoFE_f_115  autoFE_f_116  autoFE_f_117  \\\nopenfe_index                                                           \n2904941         162.882705          34.0       25232.0     25.408137   \n7879547         163.305323          49.0       12768.0     26.768385   \n1554558         166.741256          46.0       22648.0     26.660858   \n5792501         164.072226          35.0       38912.0     25.408137   \n7511622         163.305323          94.0        5798.0     27.238165   \n\n              autoFE_f_118 autoFE_f_119  autoFE_f_120  autoFE_f_121  \\\nopenfe_index                                                          \n2904941               1.00          0.0        1368.0         152.0   \n7879547               1.00          1.0        4256.0         152.0   \n1554558               1.00          2.0        2280.0         152.0   \n5792501               1.00          0.0        1520.0         152.0   \n7511622               0.99          3.0         728.0          66.0   \n\n              autoFE_f_122 autoFE_f_123  autoFE_f_124  autoFE_f_125  \\\nopenfe_index                                                          \n2904941                1.0          0.0      0.149294      212903.0   \n7879547                1.0          1.0      0.490040      444005.0   \n1554558                1.0          2.0      0.218216      212903.0   \n5792501                1.0          3.0      0.193213      444005.0   \n7511622                1.0          4.0      0.490040      250795.0   \n\n              autoFE_f_126  autoFE_f_127  autoFE_f_128  autoFE_f_129  \\\nopenfe_index                                                           \n2904941           0.000000      186375.0           9.0         152.0   \n7879547           0.000000      470533.0          29.0         152.0   \n1554558           0.000000      470533.0          15.0         152.0   \n5792501           0.000000      470533.0          11.0         152.0   \n7511622           0.099503      505403.0          29.0          28.0   \n\n              autoFE_f_130  autoFE_f_131  autoFE_f_132  autoFE_f_133  \\\nopenfe_index                                                           \n2904941            21352.0      5.023881      0.111111           9.0   \n7879547            35481.0      5.023881      0.035714          28.0   \n1554558            10759.0      5.023881      0.066667          15.0   \n5792501            61108.0      5.023881      0.100000          10.0   \n7511622             4058.0      3.258097      0.035714          28.0   \n\n              autoFE_f_134  autoFE_f_135  autoFE_f_136  autoFE_f_137  \\\nopenfe_index                                                           \n2904941                9.0       33990.0      3.000000      0.632113   \n7879547               28.0       49514.0      5.291503      0.942969   \n1554558               15.0       36913.0      3.872983      0.766029   \n5792501               10.0       36274.0      3.162278      0.719155   \n7511622               28.0       44214.0      5.291503      0.709128   \n\n              autoFE_f_138  autoFE_f_139  autoFE_f_140 autoFE_f_141  \\\nopenfe_index                                                          \n2904941                1.0    167.142991      444306.0          0.0   \n7879547                1.0    166.127907      378441.0          1.0   \n1554558                0.0    163.824747      378441.0          2.0   \n5792501                1.0    167.142991      367578.0          3.0   \n7511622                0.0    165.347064      444306.0          4.0   \n\n              autoFE_f_142  autoFE_f_143  autoFE_f_144  autoFE_f_145  \\\nopenfe_index                                                           \n2904941                0.0           9.0      809627.0      0.268198   \n7879547            49514.0          27.0      687365.0      0.768198   \n1554558                0.0          15.0      687365.0      0.268198   \n5792501            36274.0           9.0      809627.0      0.768198   \n7511622            44214.0          27.0      809627.0      0.768198   \n\n              autoFE_f_146  autoFE_f_147  autoFE_f_148  autoFE_f_149  \\\nopenfe_index                                                           \n2904941          47.998774           0.0      0.186864           0.0   \n7879547          54.847088           1.0      0.497347         152.0   \n1554558          43.929969           0.0      0.313013           0.0   \n5792501          35.676699           0.0      0.201518         152.0   \n7511622          54.847088           0.0      0.492609          26.0   \n\n              autoFE_f_150  autoFE_f_151  autoFE_f_152  autoFE_f_153  \\\nopenfe_index                                                           \n2904941            12200.0         299.0      0.181256      0.729068   \n7879547           449691.0         299.0      0.038379      0.644572   \n1554558            52322.0         299.0      0.267417      0.644572   \n5792501            16310.0         299.0      0.393759      0.729068   \n7511622           449691.0         299.0      0.952428      0.162168   \n\n              autoFE_f_154  autoFE_f_155 autoFE_f_156  autoFE_f_157  \\\nopenfe_index                                                          \n2904941          13.707443     18.444444          0.0           NaN   \n7879547          15.008618      3.000000          1.0           1.0   \n1554558          13.183065      9.933333          1.0           NaN   \n5792501          13.707443     25.600000          1.0           1.0   \n7511622          11.906707      7.964286          0.0           1.0   \n\n              autoFE_f_158  autoFE_f_159  autoFE_f_160  autoFE_f_161  \\\nopenfe_index                                                           \n2904941               25.0         161.0          24.0      0.280270   \n7879547               21.0         180.0          20.0      0.036905   \n1554558               31.0         167.0          30.0      0.433391   \n5792501               25.0         162.0          24.0      0.280270   \n7511622               66.0          54.0          65.0      0.944127   \n\n              autoFE_f_162  autoFE_f_163  autoFE_f_164  autoFE_f_165  \\\nopenfe_index                                                           \n2904941                0.0         166.0         153.0      0.689718   \n7879547                0.0         152.0         153.0      0.689718   \n1554558                0.0         152.0         153.0      0.689718   \n5792501                0.0         256.0         153.0      0.689718   \n7511622                0.0         223.0          27.0      0.143149   \n\n              autoFE_f_166  autoFE_f_167  autoFE_f_168  autoFE_f_169  \\\nopenfe_index                                                           \n2904941               25.0           1.0      0.748409          10.0   \n7879547               21.0           1.0      0.622999          29.0   \n1554558               31.0           1.0      0.748409          16.0   \n5792501               25.0           1.0      0.622999          11.0   \n7511622               66.0           1.0      0.095901          29.0   \n\n              autoFE_f_170 autoFE_f_171  autoFE_f_172  autoFE_f_173  \\\nopenfe_index                                                          \n2904941           0.173696          0.0      0.915663         166.0   \n7879547           0.037390          1.0      1.809524         166.0   \n1554558           0.613586          0.0      1.020134         171.0   \n5792501           0.386855          1.0      0.593750         169.0   \n7511622           0.932316          2.0      0.116592         166.0   \n\n              autoFE_f_174  autoFE_f_175  autoFE_f_176  autoFE_f_177  \\\nopenfe_index                                                           \n2904941              166.0           1.0           1.0          61.0   \n7879547               84.0           1.0           2.0          66.0   \n1554558              149.0           1.0           1.0          62.0   \n5792501              256.0           1.0           2.0          61.0   \n7511622              223.0           1.0           2.0          66.0   \n\n              autoFE_f_178  autoFE_f_179  autoFE_f_180  autoFE_f_181  \\\nopenfe_index                                                           \n2904941           2.197225      0.741062      0.999877      0.501025   \n7879547           3.332205      0.538668      1.000000      0.501432   \n1554558           2.708050      0.490529      1.000000      0.500698   \n5792501           2.302585      0.741062      0.999955      0.500337   \n7511622           3.332205      0.372294      1.000000      0.501432   \n\n             autoFE_f_182  autoFE_f_183  autoFE_f_184  autoFE_f_185  \\\nopenfe_index                                                          \n2904941               0.0      0.164303       30984.0      0.501280   \n7879547               1.0      0.494718       30502.0      0.500683   \n1554558               1.0      0.250191       29007.5      0.501280   \n5792501               1.0      0.173814       30984.0      0.500683   \n7511622               2.0      0.494718       35500.0      0.500683   \n\n              autoFE_f_186  autoFE_f_187  autoFE_f_188  autoFE_f_189  \\\nopenfe_index                                                           \n2904941              152.0       32993.0          -8.0      6.080000   \n7879547              122.0       30962.0         -27.0      7.238095   \n1554558              152.0       30962.0         -14.0      4.903226   \n5792501              152.0       30962.0          -9.0      6.080000   \n7511622              122.0       32993.0         -27.0      0.393939   \n\n              autoFE_f_190  autoFE_f_191  autoFE_f_192  autoFE_f_193  \\\nopenfe_index                                                           \n2904941              163.0           3.0           0.0           1.0   \n7879547              163.0           3.0          21.0           1.0   \n1554558              163.0           3.0           0.0           1.0   \n5792501              163.0           3.0          25.0           1.0   \n7511622              163.0           3.0          66.0           1.0   \n\n              autoFE_f_194  autoFE_f_195  autoFE_f_196  autoFE_f_197  \\\nopenfe_index                                                           \n2904941                9.0           0.0           NaN         152.0   \n7879547               21.0           1.0          28.0         151.0   \n1554558               15.0           0.0           NaN         152.0   \n5792501               10.0           1.0          10.0         151.0   \n7511622               28.0           1.0          28.0          25.0   \n\n              autoFE_f_198  autoFE_f_199  autoFE_f_200  autoFE_f_201  \\\nopenfe_index                                                           \n2904941               25.0      0.166778          25.0  31400.949636   \n7879547               20.0      0.493777          22.0  30066.032353   \n1554558               31.0      0.254576          31.0  25857.063169   \n5792501               24.0      0.175181          26.0  31400.949636   \n7511622               65.0      0.495553          67.0  32524.870734   \n\n              autoFE_f_202  autoFE_f_203  autoFE_f_204  autoFE_f_205  \\\nopenfe_index                                                           \n2904941                1.0           1.0         164.0         152.0   \n7879547                0.0           1.0         170.0         153.0   \n1554558                1.0           1.0         164.0         152.0   \n5792501                0.0           1.0         170.0         153.0   \n7511622                0.0           1.0         170.0          27.0   \n\n              autoFE_f_206  autoFE_f_207  autoFE_f_208  autoFE_f_209  \\\nopenfe_index                                                           \n2904941           291520.0           1.0       82460.0          20.0   \n7879547           365388.0           1.0       65868.0          20.0   \n1554558           365388.0           1.0       21118.0          20.0   \n5792501           291520.0           1.0       82460.0          20.0   \n7511622           479626.0           1.0       10900.0          20.0   \n\n              autoFE_f_210  autoFE_f_211  autoFE_f_212  autoFE_f_213  \\\nopenfe_index                                                           \n2904941                1.0           9.0           0.0           2.0   \n7879547                1.0          28.0           0.0           2.0   \n1554558                1.0          15.0           0.0           2.0   \n5792501                1.0          10.0           1.0           2.0   \n7511622                1.0          28.0           0.0           2.0   \n\n              autoFE_f_214  autoFE_f_215  autoFE_f_216  autoFE_f_217  \\\nopenfe_index                                                           \n2904941                0.0    163.172999      0.438054          26.0   \n7879547                1.0    164.729211      0.498506          22.0   \n1554558                0.0    164.729211      0.499922          32.0   \n5792501                1.0    163.172999      0.438054          26.0   \n7511622                1.0    163.172999      0.483438          67.0   \n\n              autoFE_f_218  autoFE_f_219  autoFE_f_220  autoFE_f_221  \\\nopenfe_index                                                           \n2904941                NaN         152.0         152.0           9.0   \n7879547               21.0         152.0         152.0          28.0   \n1554558                NaN         152.0         152.0          15.0   \n5792501               25.0         152.0         152.0          10.0   \n7511622               66.0          26.0          26.0          26.0   \n\n              autoFE_f_222  autoFE_f_223  autoFE_f_224 autoFE_f_225  \\\nopenfe_index                                                          \n2904941                1.0         151.0          53.0          0.0   \n7879547                1.0         151.0          52.0          1.0   \n1554558                1.0         151.0          53.0          2.0   \n5792501                1.0         151.0          53.0          3.0   \n7511622                1.0          25.0          53.0          3.0   \n\n              autoFE_f_226  autoFE_f_227  autoFE_f_228  autoFE_f_229  \\\nopenfe_index                                                           \n2904941          20.478651          10.0       33824.0           9.0   \n7879547          12.379879          10.0       49430.0          28.0   \n1554558          37.619003          10.0       36764.0          15.0   \n5792501          20.478651          10.0       36018.0          10.0   \n7511622          53.125551          10.0       43991.0          28.0   \n\n              autoFE_f_230  autoFE_f_231  autoFE_f_232  autoFE_f_233  \\\nopenfe_index                                                           \n2904941           0.500006      0.288312          10.0           1.0   \n7879547           0.500008      0.744792          10.0           1.0   \n1554558           0.500024      0.244792          10.0           1.0   \n5792501           0.500006      0.788312          10.0           1.0   \n7511622           0.505046      0.788312          10.0           1.0   \n\n              autoFE_f_234  autoFE_f_235 autoFE_f_236  autoFE_f_237  \\\nopenfe_index                                                          \n2904941              166.0         299.0          0.0      313424.0   \n7879547               84.0         299.0          1.0      540165.0   \n1554558              149.0         299.0          1.0      309870.0   \n5792501              256.0         299.0          2.0       85517.0   \n7511622              223.0         299.0          0.0      540165.0   \n\n              autoFE_f_238  autoFE_f_239  autoFE_f_240  \nopenfe_index                                            \n2904941                0.0      717515.0    148.647138  \n7879547               28.0      658494.0    154.448108  \n1554558                0.0       87525.0    135.694052  \n5792501               10.0      658494.0    148.647138  \n7511622               28.0       36466.0     71.982385  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Driving_License</th>\n      <th>Region_Code</th>\n      <th>Previously_Insured</th>\n      <th>Vehicle_Age</th>\n      <th>Vehicle_Damage</th>\n      <th>Annual_Premium</th>\n      <th>Policy_Sales_Channel</th>\n      <th>Vintage</th>\n      <th>autoFE_f_0</th>\n      <th>autoFE_f_1</th>\n      <th>autoFE_f_2</th>\n      <th>autoFE_f_3</th>\n      <th>autoFE_f_4</th>\n      <th>autoFE_f_5</th>\n      <th>autoFE_f_6</th>\n      <th>autoFE_f_7</th>\n      <th>autoFE_f_8</th>\n      <th>autoFE_f_9</th>\n      <th>autoFE_f_10</th>\n      <th>autoFE_f_11</th>\n      <th>autoFE_f_12</th>\n      <th>autoFE_f_13</th>\n      <th>autoFE_f_14</th>\n      <th>autoFE_f_15</th>\n      <th>autoFE_f_16</th>\n      <th>autoFE_f_17</th>\n      <th>autoFE_f_18</th>\n      <th>autoFE_f_19</th>\n      <th>autoFE_f_20</th>\n      <th>autoFE_f_21</th>\n      <th>autoFE_f_22</th>\n      <th>autoFE_f_23</th>\n      <th>autoFE_f_24</th>\n      <th>autoFE_f_25</th>\n      <th>autoFE_f_26</th>\n      <th>autoFE_f_27</th>\n      <th>autoFE_f_28</th>\n      <th>autoFE_f_29</th>\n      <th>autoFE_f_30</th>\n      <th>autoFE_f_31</th>\n      <th>autoFE_f_32</th>\n      <th>autoFE_f_33</th>\n      <th>autoFE_f_34</th>\n      <th>autoFE_f_35</th>\n      <th>autoFE_f_36</th>\n      <th>autoFE_f_37</th>\n      <th>autoFE_f_38</th>\n      <th>autoFE_f_39</th>\n      <th>autoFE_f_40</th>\n      <th>autoFE_f_41</th>\n      <th>autoFE_f_42</th>\n      <th>autoFE_f_43</th>\n      <th>autoFE_f_44</th>\n      <th>autoFE_f_45</th>\n      <th>autoFE_f_46</th>\n      <th>autoFE_f_47</th>\n      <th>autoFE_f_48</th>\n      <th>autoFE_f_49</th>\n      <th>autoFE_f_50</th>\n      <th>autoFE_f_51</th>\n      <th>autoFE_f_52</th>\n      <th>autoFE_f_53</th>\n      <th>autoFE_f_54</th>\n      <th>autoFE_f_55</th>\n      <th>autoFE_f_56</th>\n      <th>autoFE_f_57</th>\n      <th>autoFE_f_58</th>\n      <th>autoFE_f_59</th>\n      <th>autoFE_f_60</th>\n      <th>autoFE_f_61</th>\n      <th>autoFE_f_62</th>\n      <th>autoFE_f_63</th>\n      <th>autoFE_f_64</th>\n      <th>autoFE_f_65</th>\n      <th>autoFE_f_66</th>\n      <th>autoFE_f_67</th>\n      <th>autoFE_f_68</th>\n      <th>autoFE_f_69</th>\n      <th>autoFE_f_70</th>\n      <th>autoFE_f_71</th>\n      <th>autoFE_f_72</th>\n      <th>autoFE_f_73</th>\n      <th>autoFE_f_74</th>\n      <th>autoFE_f_75</th>\n      <th>autoFE_f_76</th>\n      <th>autoFE_f_77</th>\n      <th>autoFE_f_78</th>\n      <th>autoFE_f_79</th>\n      <th>autoFE_f_80</th>\n      <th>autoFE_f_81</th>\n      <th>autoFE_f_82</th>\n      <th>autoFE_f_83</th>\n      <th>autoFE_f_84</th>\n      <th>autoFE_f_85</th>\n      <th>autoFE_f_86</th>\n      <th>autoFE_f_87</th>\n      <th>autoFE_f_88</th>\n      <th>autoFE_f_89</th>\n      <th>autoFE_f_90</th>\n      <th>autoFE_f_91</th>\n      <th>autoFE_f_92</th>\n      <th>autoFE_f_93</th>\n      <th>autoFE_f_94</th>\n      <th>autoFE_f_95</th>\n      <th>autoFE_f_96</th>\n      <th>autoFE_f_97</th>\n      <th>autoFE_f_98</th>\n      <th>autoFE_f_99</th>\n      <th>autoFE_f_100</th>\n      <th>autoFE_f_101</th>\n      <th>autoFE_f_102</th>\n      <th>autoFE_f_103</th>\n      <th>autoFE_f_104</th>\n      <th>autoFE_f_105</th>\n      <th>autoFE_f_106</th>\n      <th>autoFE_f_107</th>\n      <th>autoFE_f_108</th>\n      <th>autoFE_f_109</th>\n      <th>autoFE_f_110</th>\n      <th>autoFE_f_111</th>\n      <th>autoFE_f_112</th>\n      <th>autoFE_f_113</th>\n      <th>autoFE_f_114</th>\n      <th>autoFE_f_115</th>\n      <th>autoFE_f_116</th>\n      <th>autoFE_f_117</th>\n      <th>autoFE_f_118</th>\n      <th>autoFE_f_119</th>\n      <th>autoFE_f_120</th>\n      <th>autoFE_f_121</th>\n      <th>autoFE_f_122</th>\n      <th>autoFE_f_123</th>\n      <th>autoFE_f_124</th>\n      <th>autoFE_f_125</th>\n      <th>autoFE_f_126</th>\n      <th>autoFE_f_127</th>\n      <th>autoFE_f_128</th>\n      <th>autoFE_f_129</th>\n      <th>autoFE_f_130</th>\n      <th>autoFE_f_131</th>\n      <th>autoFE_f_132</th>\n      <th>autoFE_f_133</th>\n      <th>autoFE_f_134</th>\n      <th>autoFE_f_135</th>\n      <th>autoFE_f_136</th>\n      <th>autoFE_f_137</th>\n      <th>autoFE_f_138</th>\n      <th>autoFE_f_139</th>\n      <th>autoFE_f_140</th>\n      <th>autoFE_f_141</th>\n      <th>autoFE_f_142</th>\n      <th>autoFE_f_143</th>\n      <th>autoFE_f_144</th>\n      <th>autoFE_f_145</th>\n      <th>autoFE_f_146</th>\n      <th>autoFE_f_147</th>\n      <th>autoFE_f_148</th>\n      <th>autoFE_f_149</th>\n      <th>autoFE_f_150</th>\n      <th>autoFE_f_151</th>\n      <th>autoFE_f_152</th>\n      <th>autoFE_f_153</th>\n      <th>autoFE_f_154</th>\n      <th>autoFE_f_155</th>\n      <th>autoFE_f_156</th>\n      <th>autoFE_f_157</th>\n      <th>autoFE_f_158</th>\n      <th>autoFE_f_159</th>\n      <th>autoFE_f_160</th>\n      <th>autoFE_f_161</th>\n      <th>autoFE_f_162</th>\n      <th>autoFE_f_163</th>\n      <th>autoFE_f_164</th>\n      <th>autoFE_f_165</th>\n      <th>autoFE_f_166</th>\n      <th>autoFE_f_167</th>\n      <th>autoFE_f_168</th>\n      <th>autoFE_f_169</th>\n      <th>autoFE_f_170</th>\n      <th>autoFE_f_171</th>\n      <th>autoFE_f_172</th>\n      <th>autoFE_f_173</th>\n      <th>autoFE_f_174</th>\n      <th>autoFE_f_175</th>\n      <th>autoFE_f_176</th>\n      <th>autoFE_f_177</th>\n      <th>autoFE_f_178</th>\n      <th>autoFE_f_179</th>\n      <th>autoFE_f_180</th>\n      <th>autoFE_f_181</th>\n      <th>autoFE_f_182</th>\n      <th>autoFE_f_183</th>\n      <th>autoFE_f_184</th>\n      <th>autoFE_f_185</th>\n      <th>autoFE_f_186</th>\n      <th>autoFE_f_187</th>\n      <th>autoFE_f_188</th>\n      <th>autoFE_f_189</th>\n      <th>autoFE_f_190</th>\n      <th>autoFE_f_191</th>\n      <th>autoFE_f_192</th>\n      <th>autoFE_f_193</th>\n      <th>autoFE_f_194</th>\n      <th>autoFE_f_195</th>\n      <th>autoFE_f_196</th>\n      <th>autoFE_f_197</th>\n      <th>autoFE_f_198</th>\n      <th>autoFE_f_199</th>\n      <th>autoFE_f_200</th>\n      <th>autoFE_f_201</th>\n      <th>autoFE_f_202</th>\n      <th>autoFE_f_203</th>\n      <th>autoFE_f_204</th>\n      <th>autoFE_f_205</th>\n      <th>autoFE_f_206</th>\n      <th>autoFE_f_207</th>\n      <th>autoFE_f_208</th>\n      <th>autoFE_f_209</th>\n      <th>autoFE_f_210</th>\n      <th>autoFE_f_211</th>\n      <th>autoFE_f_212</th>\n      <th>autoFE_f_213</th>\n      <th>autoFE_f_214</th>\n      <th>autoFE_f_215</th>\n      <th>autoFE_f_216</th>\n      <th>autoFE_f_217</th>\n      <th>autoFE_f_218</th>\n      <th>autoFE_f_219</th>\n      <th>autoFE_f_220</th>\n      <th>autoFE_f_221</th>\n      <th>autoFE_f_222</th>\n      <th>autoFE_f_223</th>\n      <th>autoFE_f_224</th>\n      <th>autoFE_f_225</th>\n      <th>autoFE_f_226</th>\n      <th>autoFE_f_227</th>\n      <th>autoFE_f_228</th>\n      <th>autoFE_f_229</th>\n      <th>autoFE_f_230</th>\n      <th>autoFE_f_231</th>\n      <th>autoFE_f_232</th>\n      <th>autoFE_f_233</th>\n      <th>autoFE_f_234</th>\n      <th>autoFE_f_235</th>\n      <th>autoFE_f_236</th>\n      <th>autoFE_f_237</th>\n      <th>autoFE_f_238</th>\n      <th>autoFE_f_239</th>\n      <th>autoFE_f_240</th>\n    </tr>\n    <tr>\n      <th>openfe_index</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2904941</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>Yes</td>\n      <td>33990.0</td>\n      <td>152.0</td>\n      <td>166</td>\n      <td>5642340.0</td>\n      <td>0.480421</td>\n      <td>186.0</td>\n      <td>5109.0</td>\n      <td>542950.0</td>\n      <td>0.178632</td>\n      <td>166.0</td>\n      <td>305910.0</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>0.653033</td>\n      <td>NaN</td>\n      <td>0.380000</td>\n      <td>14.485116</td>\n      <td>0.529460</td>\n      <td>0.510935</td>\n      <td>0.280615</td>\n      <td>33990.0</td>\n      <td>191.0</td>\n      <td>6910.0</td>\n      <td>0.573447</td>\n      <td>34156.0</td>\n      <td>0.537776</td>\n      <td>33990.0</td>\n      <td>34015.0</td>\n      <td>1359.600000</td>\n      <td>0.509686</td>\n      <td>0.500451</td>\n      <td>0.487574</td>\n      <td>0.0</td>\n      <td>34.986475</td>\n      <td>0.589180</td>\n      <td>223.618421</td>\n      <td>0.499175</td>\n      <td>17590.0</td>\n      <td>6078.0</td>\n      <td>28.0</td>\n      <td>78.856796</td>\n      <td>175.0</td>\n      <td>895.0</td>\n      <td>14079.233812</td>\n      <td>0.643177</td>\n      <td>6846.0</td>\n      <td>16.888889</td>\n      <td>0.625551</td>\n      <td>0.162050</td>\n      <td>0.0</td>\n      <td>26902.168033</td>\n      <td>30648.0</td>\n      <td>0.0</td>\n      <td>0.554006</td>\n      <td>125.991393</td>\n      <td>33965.0</td>\n      <td>0.0</td>\n      <td>0.225569</td>\n      <td>0.499973</td>\n      <td>166.0</td>\n      <td>5166480.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.505359</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>6.640000</td>\n      <td>165.0</td>\n      <td>849750.0</td>\n      <td>0.438852</td>\n      <td>167.0</td>\n      <td>33990.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>34142.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>318.0</td>\n      <td>290594.0</td>\n      <td>1494.0</td>\n      <td>25.0</td>\n      <td>0.496267</td>\n      <td>33999.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>33990.0</td>\n      <td>143.0</td>\n      <td>166.0</td>\n      <td>157.0</td>\n      <td>82460.0</td>\n      <td>0.998033</td>\n      <td>33838.0</td>\n      <td>80.0</td>\n      <td>225.0</td>\n      <td>0.147187</td>\n      <td>204.759036</td>\n      <td>0.0</td>\n      <td>751966.0</td>\n      <td>25.0</td>\n      <td>6063.0</td>\n      <td>33981.0</td>\n      <td>-14.0</td>\n      <td>12986.968804</td>\n      <td>3776.666667</td>\n      <td>2.777778</td>\n      <td>0.129475</td>\n      <td>12176.0</td>\n      <td>152.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4150.0</td>\n      <td>0.752077</td>\n      <td>25.0</td>\n      <td>162.882705</td>\n      <td>34.0</td>\n      <td>25232.0</td>\n      <td>25.408137</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>1368.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.149294</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>186375.0</td>\n      <td>9.0</td>\n      <td>152.0</td>\n      <td>21352.0</td>\n      <td>5.023881</td>\n      <td>0.111111</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>33990.0</td>\n      <td>3.000000</td>\n      <td>0.632113</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>444306.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.268198</td>\n      <td>47.998774</td>\n      <td>0.0</td>\n      <td>0.186864</td>\n      <td>0.0</td>\n      <td>12200.0</td>\n      <td>299.0</td>\n      <td>0.181256</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>18.444444</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>161.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>10.0</td>\n      <td>0.173696</td>\n      <td>0.0</td>\n      <td>0.915663</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>61.0</td>\n      <td>2.197225</td>\n      <td>0.741062</td>\n      <td>0.999877</td>\n      <td>0.501025</td>\n      <td>0.0</td>\n      <td>0.164303</td>\n      <td>30984.0</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>32993.0</td>\n      <td>-8.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>25.0</td>\n      <td>0.166778</td>\n      <td>25.0</td>\n      <td>31400.949636</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>0.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>33824.0</td>\n      <td>9.0</td>\n      <td>0.500006</td>\n      <td>0.288312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>166.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>313424.0</td>\n      <td>0.0</td>\n      <td>717515.0</td>\n      <td>148.647138</td>\n    </tr>\n    <tr>\n      <th>7879547</th>\n      <td>Female</td>\n      <td>21</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>49514.0</td>\n      <td>152.0</td>\n      <td>84</td>\n      <td>4159176.0</td>\n      <td>0.190024</td>\n      <td>82.0</td>\n      <td>8441.0</td>\n      <td>542950.0</td>\n      <td>0.477880</td>\n      <td>84.0</td>\n      <td>1386392.0</td>\n      <td>1.0</td>\n      <td>83.0</td>\n      <td>85.0</td>\n      <td>0.785224</td>\n      <td>84.0</td>\n      <td>0.007696</td>\n      <td>13.832753</td>\n      <td>0.923739</td>\n      <td>0.194659</td>\n      <td>0.842734</td>\n      <td>49515.0</td>\n      <td>105.0</td>\n      <td>79439.0</td>\n      <td>0.908039</td>\n      <td>49598.0</td>\n      <td>0.922583</td>\n      <td>49514.0</td>\n      <td>49535.0</td>\n      <td>2357.809524</td>\n      <td>0.194317</td>\n      <td>0.205071</td>\n      <td>0.190794</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.867675</td>\n      <td>325.750000</td>\n      <td>0.266412</td>\n      <td>37064.0</td>\n      <td>191241.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>112.0</td>\n      <td>4709.0</td>\n      <td>15600.277416</td>\n      <td>0.937936</td>\n      <td>141443.0</td>\n      <td>5.428571</td>\n      <td>0.080719</td>\n      <td>0.662050</td>\n      <td>1.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>1.0</td>\n      <td>0.913898</td>\n      <td>86.787510</td>\n      <td>49493.0</td>\n      <td>1.0</td>\n      <td>0.043722</td>\n      <td>0.201356</td>\n      <td>84.0</td>\n      <td>7526128.0</td>\n      <td>49514.0</td>\n      <td>1.0</td>\n      <td>0.195420</td>\n      <td>79.342478</td>\n      <td>131.0</td>\n      <td>1.0</td>\n      <td>4.000000</td>\n      <td>83.0</td>\n      <td>1039794.0</td>\n      <td>0.314534</td>\n      <td>85.0</td>\n      <td>49514.0</td>\n      <td>0.466655</td>\n      <td>173.0</td>\n      <td>49666.0</td>\n      <td>63813.0</td>\n      <td>36329.0</td>\n      <td>236.0</td>\n      <td>488138.0</td>\n      <td>2352.0</td>\n      <td>28.0</td>\n      <td>0.464330</td>\n      <td>49542.0</td>\n      <td>84.0</td>\n      <td>3192.0</td>\n      <td>49513.0</td>\n      <td>124.0</td>\n      <td>84.0</td>\n      <td>56.0</td>\n      <td>65868.0</td>\n      <td>0.997138</td>\n      <td>49362.0</td>\n      <td>85.0</td>\n      <td>588.0</td>\n      <td>0.492940</td>\n      <td>589.452381</td>\n      <td>1.0</td>\n      <td>745026.0</td>\n      <td>21.0</td>\n      <td>152580.0</td>\n      <td>49486.0</td>\n      <td>68.0</td>\n      <td>13812.283547</td>\n      <td>1768.357143</td>\n      <td>0.750000</td>\n      <td>0.730673</td>\n      <td>448404.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>-7.0</td>\n      <td>1.0</td>\n      <td>152.0</td>\n      <td>1764.0</td>\n      <td>0.627876</td>\n      <td>21.0</td>\n      <td>163.305323</td>\n      <td>49.0</td>\n      <td>12768.0</td>\n      <td>26.768385</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>4256.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.490040</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>29.0</td>\n      <td>152.0</td>\n      <td>35481.0</td>\n      <td>5.023881</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>49514.0</td>\n      <td>5.291503</td>\n      <td>0.942969</td>\n      <td>1.0</td>\n      <td>166.127907</td>\n      <td>378441.0</td>\n      <td>1.0</td>\n      <td>49514.0</td>\n      <td>27.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>1.0</td>\n      <td>0.497347</td>\n      <td>152.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.038379</td>\n      <td>0.644572</td>\n      <td>15.008618</td>\n      <td>3.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>180.0</td>\n      <td>20.0</td>\n      <td>0.036905</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>29.0</td>\n      <td>0.037390</td>\n      <td>1.0</td>\n      <td>1.809524</td>\n      <td>166.0</td>\n      <td>84.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.538668</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>1.0</td>\n      <td>0.494718</td>\n      <td>30502.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>30962.0</td>\n      <td>-27.0</td>\n      <td>7.238095</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>151.0</td>\n      <td>20.0</td>\n      <td>0.493777</td>\n      <td>22.0</td>\n      <td>30066.032353</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>65868.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.498506</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>12.379879</td>\n      <td>10.0</td>\n      <td>49430.0</td>\n      <td>28.0</td>\n      <td>0.500008</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>84.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>658494.0</td>\n      <td>154.448108</td>\n    </tr>\n    <tr>\n      <th>1554558</th>\n      <td>Female</td>\n      <td>31</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36913.0</td>\n      <td>152.0</td>\n      <td>149</td>\n      <td>5500037.0</td>\n      <td>0.451416</td>\n      <td>25.0</td>\n      <td>4808.0</td>\n      <td>542950.0</td>\n      <td>0.248792</td>\n      <td>149.0</td>\n      <td>553695.0</td>\n      <td>2.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>0.741447</td>\n      <td>NaN</td>\n      <td>0.645197</td>\n      <td>13.084419</td>\n      <td>0.715309</td>\n      <td>0.432102</td>\n      <td>0.260368</td>\n      <td>36913.0</td>\n      <td>180.0</td>\n      <td>34800.0</td>\n      <td>0.670870</td>\n      <td>37062.0</td>\n      <td>0.634359</td>\n      <td>36913.0</td>\n      <td>36944.0</td>\n      <td>1190.741935</td>\n      <td>0.446585</td>\n      <td>0.417740</td>\n      <td>0.430972</td>\n      <td>2.0</td>\n      <td>32.053954</td>\n      <td>0.553907</td>\n      <td>242.848684</td>\n      <td>0.546335</td>\n      <td>11290.0</td>\n      <td>26554.0</td>\n      <td>26.0</td>\n      <td>80.427887</td>\n      <td>164.0</td>\n      <td>855.0</td>\n      <td>13766.828032</td>\n      <td>0.746904</td>\n      <td>27245.0</td>\n      <td>10.133333</td>\n      <td>0.946592</td>\n      <td>0.162050</td>\n      <td>2.0</td>\n      <td>28959.286744</td>\n      <td>31574.5</td>\n      <td>1.0</td>\n      <td>0.690318</td>\n      <td>132.888517</td>\n      <td>36882.0</td>\n      <td>2.0</td>\n      <td>0.522780</td>\n      <td>0.439885</td>\n      <td>149.0</td>\n      <td>5610776.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.431017</td>\n      <td>79.379644</td>\n      <td>121.0</td>\n      <td>2.0</td>\n      <td>4.806452</td>\n      <td>148.0</td>\n      <td>1144303.0</td>\n      <td>0.479282</td>\n      <td>150.0</td>\n      <td>36913.0</td>\n      <td>0.466655</td>\n      <td>183.0</td>\n      <td>37065.0</td>\n      <td>15033.0</td>\n      <td>9979.0</td>\n      <td>301.0</td>\n      <td>346200.0</td>\n      <td>2235.0</td>\n      <td>31.0</td>\n      <td>0.499575</td>\n      <td>36928.0</td>\n      <td>149.0</td>\n      <td>4712.0</td>\n      <td>36913.0</td>\n      <td>137.0</td>\n      <td>149.0</td>\n      <td>134.0</td>\n      <td>21118.0</td>\n      <td>0.998624</td>\n      <td>36761.0</td>\n      <td>81.0</td>\n      <td>465.0</td>\n      <td>0.286846</td>\n      <td>247.738255</td>\n      <td>2.0</td>\n      <td>745026.0</td>\n      <td>31.0</td>\n      <td>29458.0</td>\n      <td>36898.0</td>\n      <td>3.0</td>\n      <td>17397.458965</td>\n      <td>2460.866667</td>\n      <td>2.066667</td>\n      <td>0.254759</td>\n      <td>52250.0</td>\n      <td>152.0</td>\n      <td>2.0</td>\n      <td>16.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>4619.0</td>\n      <td>0.627876</td>\n      <td>31.0</td>\n      <td>166.741256</td>\n      <td>46.0</td>\n      <td>22648.0</td>\n      <td>26.660858</td>\n      <td>1.00</td>\n      <td>2.0</td>\n      <td>2280.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.218216</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>15.0</td>\n      <td>152.0</td>\n      <td>10759.0</td>\n      <td>5.023881</td>\n      <td>0.066667</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>36913.0</td>\n      <td>3.872983</td>\n      <td>0.766029</td>\n      <td>0.0</td>\n      <td>163.824747</td>\n      <td>378441.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>687365.0</td>\n      <td>0.268198</td>\n      <td>43.929969</td>\n      <td>0.0</td>\n      <td>0.313013</td>\n      <td>0.0</td>\n      <td>52322.0</td>\n      <td>299.0</td>\n      <td>0.267417</td>\n      <td>0.644572</td>\n      <td>13.183065</td>\n      <td>9.933333</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>31.0</td>\n      <td>167.0</td>\n      <td>30.0</td>\n      <td>0.433391</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>31.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>16.0</td>\n      <td>0.613586</td>\n      <td>0.0</td>\n      <td>1.020134</td>\n      <td>171.0</td>\n      <td>149.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>2.708050</td>\n      <td>0.490529</td>\n      <td>1.000000</td>\n      <td>0.500698</td>\n      <td>1.0</td>\n      <td>0.250191</td>\n      <td>29007.5</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-14.0</td>\n      <td>4.903226</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>31.0</td>\n      <td>0.254576</td>\n      <td>31.0</td>\n      <td>25857.063169</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>21118.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>164.729211</td>\n      <td>0.499922</td>\n      <td>32.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>2.0</td>\n      <td>37.619003</td>\n      <td>10.0</td>\n      <td>36764.0</td>\n      <td>15.0</td>\n      <td>0.500024</td>\n      <td>0.244792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>149.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>309870.0</td>\n      <td>0.0</td>\n      <td>87525.0</td>\n      <td>135.694052</td>\n    </tr>\n    <tr>\n      <th>5792501</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36274.0</td>\n      <td>152.0</td>\n      <td>256</td>\n      <td>9286144.0</td>\n      <td>0.829111</td>\n      <td>106.0</td>\n      <td>11913.0</td>\n      <td>542950.0</td>\n      <td>0.195258</td>\n      <td>256.0</td>\n      <td>362740.0</td>\n      <td>1.0</td>\n      <td>255.0</td>\n      <td>257.0</td>\n      <td>0.921735</td>\n      <td>256.0</td>\n      <td>0.466462</td>\n      <td>11.027021</td>\n      <td>0.700654</td>\n      <td>0.835286</td>\n      <td>0.672440</td>\n      <td>36275.0</td>\n      <td>281.0</td>\n      <td>12342.0</td>\n      <td>0.655816</td>\n      <td>36530.0</td>\n      <td>0.697934</td>\n      <td>36274.0</td>\n      <td>36299.0</td>\n      <td>1450.960000</td>\n      <td>0.835485</td>\n      <td>0.824923</td>\n      <td>0.836364</td>\n      <td>3.0</td>\n      <td>30.023605</td>\n      <td>0.539638</td>\n      <td>238.644737</td>\n      <td>0.499175</td>\n      <td>64870.0</td>\n      <td>7566.0</td>\n      <td>25.0</td>\n      <td>82.260215</td>\n      <td>266.0</td>\n      <td>1847.0</td>\n      <td>11346.682460</td>\n      <td>0.731384</td>\n      <td>10686.0</td>\n      <td>15.200000</td>\n      <td>0.625551</td>\n      <td>0.662050</td>\n      <td>3.0</td>\n      <td>24892.287308</td>\n      <td>28015.0</td>\n      <td>0.0</td>\n      <td>0.638378</td>\n      <td>139.645371</td>\n      <td>36249.0</td>\n      <td>3.0</td>\n      <td>0.225569</td>\n      <td>0.842750</td>\n      <td>256.0</td>\n      <td>5513648.0</td>\n      <td>36274.0</td>\n      <td>3.0</td>\n      <td>0.840829</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>10.240000</td>\n      <td>255.0</td>\n      <td>906850.0</td>\n      <td>0.655181</td>\n      <td>257.0</td>\n      <td>36274.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>36426.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>408.0</td>\n      <td>290594.0</td>\n      <td>2560.0</td>\n      <td>25.0</td>\n      <td>0.475324</td>\n      <td>36284.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>36273.0</td>\n      <td>142.0</td>\n      <td>256.0</td>\n      <td>246.0</td>\n      <td>82460.0</td>\n      <td>0.999387</td>\n      <td>36122.0</td>\n      <td>80.0</td>\n      <td>250.0</td>\n      <td>0.193205</td>\n      <td>141.695312</td>\n      <td>3.0</td>\n      <td>745026.0</td>\n      <td>25.0</td>\n      <td>11198.0</td>\n      <td>36264.0</td>\n      <td>-104.0</td>\n      <td>12986.968804</td>\n      <td>3627.400000</td>\n      <td>2.500000</td>\n      <td>0.629475</td>\n      <td>16300.0</td>\n      <td>152.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>6400.0</td>\n      <td>0.627876</td>\n      <td>25.0</td>\n      <td>164.072226</td>\n      <td>35.0</td>\n      <td>38912.0</td>\n      <td>25.408137</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>1520.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.193213</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>11.0</td>\n      <td>152.0</td>\n      <td>61108.0</td>\n      <td>5.023881</td>\n      <td>0.100000</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>36274.0</td>\n      <td>3.162278</td>\n      <td>0.719155</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>367578.0</td>\n      <td>3.0</td>\n      <td>36274.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>35.676699</td>\n      <td>0.0</td>\n      <td>0.201518</td>\n      <td>152.0</td>\n      <td>16310.0</td>\n      <td>299.0</td>\n      <td>0.393759</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>25.600000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>162.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>256.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>11.0</td>\n      <td>0.386855</td>\n      <td>1.0</td>\n      <td>0.593750</td>\n      <td>169.0</td>\n      <td>256.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>61.0</td>\n      <td>2.302585</td>\n      <td>0.741062</td>\n      <td>0.999955</td>\n      <td>0.500337</td>\n      <td>1.0</td>\n      <td>0.173814</td>\n      <td>30984.0</td>\n      <td>0.500683</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-9.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>151.0</td>\n      <td>24.0</td>\n      <td>0.175181</td>\n      <td>26.0</td>\n      <td>31400.949636</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>25.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>36018.0</td>\n      <td>10.0</td>\n      <td>0.500006</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>256.0</td>\n      <td>299.0</td>\n      <td>2.0</td>\n      <td>85517.0</td>\n      <td>10.0</td>\n      <td>658494.0</td>\n      <td>148.647138</td>\n    </tr>\n    <tr>\n      <th>7511622</th>\n      <td>Male</td>\n      <td>66</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>1-2 Year</td>\n      <td>Yes</td>\n      <td>44214.0</td>\n      <td>26.0</td>\n      <td>223</td>\n      <td>9859722.0</td>\n      <td>0.708394</td>\n      <td>93.0</td>\n      <td>4884.0</td>\n      <td>317263.0</td>\n      <td>0.485734</td>\n      <td>223.0</td>\n      <td>1237992.0</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>224.0</td>\n      <td>0.609766</td>\n      <td>223.0</td>\n      <td>0.906301</td>\n      <td>13.832753</td>\n      <td>0.790905</td>\n      <td>0.725049</td>\n      <td>0.842734</td>\n      <td>44215.0</td>\n      <td>289.0</td>\n      <td>332339.0</td>\n      <td>0.821479</td>\n      <td>44437.0</td>\n      <td>0.849935</td>\n      <td>44214.0</td>\n      <td>44280.0</td>\n      <td>669.909091</td>\n      <td>0.701396</td>\n      <td>0.717548</td>\n      <td>0.718992</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.223135</td>\n      <td>1700.538462</td>\n      <td>0.303853</td>\n      <td>6696.0</td>\n      <td>258450.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>251.0</td>\n      <td>4950.0</td>\n      <td>15600.277416</td>\n      <td>0.787907</td>\n      <td>141443.0</td>\n      <td>0.928571</td>\n      <td>0.903795</td>\n      <td>0.839407</td>\n      <td>4.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>2.0</td>\n      <td>0.810001</td>\n      <td>86.787510</td>\n      <td>44148.0</td>\n      <td>1.0</td>\n      <td>0.938345</td>\n      <td>0.713352</td>\n      <td>223.0</td>\n      <td>1149564.0</td>\n      <td>44214.0</td>\n      <td>4.0</td>\n      <td>0.714188</td>\n      <td>79.709674</td>\n      <td>-40.0</td>\n      <td>3.0</td>\n      <td>3.378788</td>\n      <td>222.0</td>\n      <td>2918124.0</td>\n      <td>0.314534</td>\n      <td>224.0</td>\n      <td>44214.0</td>\n      <td>0.235340</td>\n      <td>92.0</td>\n      <td>44240.0</td>\n      <td>9586.0</td>\n      <td>6662.0</td>\n      <td>249.0</td>\n      <td>308615.0</td>\n      <td>6244.0</td>\n      <td>66.0</td>\n      <td>0.464330</td>\n      <td>44242.0</td>\n      <td>26.0</td>\n      <td>1716.0</td>\n      <td>44213.0</td>\n      <td>-2.0</td>\n      <td>223.0</td>\n      <td>195.0</td>\n      <td>10791.0</td>\n      <td>0.997138</td>\n      <td>44188.0</td>\n      <td>85.0</td>\n      <td>1848.0</td>\n      <td>0.496518</td>\n      <td>198.269058</td>\n      <td>4.0</td>\n      <td>751966.0</td>\n      <td>66.0</td>\n      <td>297111.0</td>\n      <td>44186.0</td>\n      <td>-197.0</td>\n      <td>19314.044119</td>\n      <td>1579.071429</td>\n      <td>2.357143</td>\n      <td>0.813899</td>\n      <td>448404.0</td>\n      <td>26.0</td>\n      <td>4.0</td>\n      <td>38.0</td>\n      <td>3.0</td>\n      <td>26.0</td>\n      <td>14718.0</td>\n      <td>0.187160</td>\n      <td>26.0</td>\n      <td>163.305323</td>\n      <td>94.0</td>\n      <td>5798.0</td>\n      <td>27.238165</td>\n      <td>0.99</td>\n      <td>3.0</td>\n      <td>728.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.490040</td>\n      <td>250795.0</td>\n      <td>0.099503</td>\n      <td>505403.0</td>\n      <td>29.0</td>\n      <td>28.0</td>\n      <td>4058.0</td>\n      <td>3.258097</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>44214.0</td>\n      <td>5.291503</td>\n      <td>0.709128</td>\n      <td>0.0</td>\n      <td>165.347064</td>\n      <td>444306.0</td>\n      <td>4.0</td>\n      <td>44214.0</td>\n      <td>27.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>0.0</td>\n      <td>0.492609</td>\n      <td>26.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.952428</td>\n      <td>0.162168</td>\n      <td>11.906707</td>\n      <td>7.964286</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>66.0</td>\n      <td>54.0</td>\n      <td>65.0</td>\n      <td>0.944127</td>\n      <td>0.0</td>\n      <td>223.0</td>\n      <td>27.0</td>\n      <td>0.143149</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>0.095901</td>\n      <td>29.0</td>\n      <td>0.932316</td>\n      <td>2.0</td>\n      <td>0.116592</td>\n      <td>166.0</td>\n      <td>223.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.372294</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>2.0</td>\n      <td>0.494718</td>\n      <td>35500.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>32993.0</td>\n      <td>-27.0</td>\n      <td>0.393939</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>25.0</td>\n      <td>65.0</td>\n      <td>0.495553</td>\n      <td>67.0</td>\n      <td>32524.870734</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>27.0</td>\n      <td>479626.0</td>\n      <td>1.0</td>\n      <td>10900.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.483438</td>\n      <td>67.0</td>\n      <td>66.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>53.125551</td>\n      <td>10.0</td>\n      <td>43991.0</td>\n      <td>28.0</td>\n      <td>0.505046</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>223.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>36466.0</td>\n      <td>71.982385</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_x.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:04:55.063245Z","iopub.execute_input":"2024-07-29T09:04:55.063904Z","iopub.status.idle":"2024-07-29T09:04:55.069605Z","shell.execute_reply.started":"2024-07-29T09:04:55.063871Z","shell.execute_reply":"2024-07-29T09:04:55.068729Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(1000000, 251)"},"metadata":{}}]},{"cell_type":"code","source":"train_x.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:05:25.08089Z","iopub.execute_input":"2024-07-29T09:05:25.081245Z","iopub.status.idle":"2024-07-29T09:05:25.085807Z","shell.execute_reply.started":"2024-07-29T09:05:25.081212Z","shell.execute_reply":"2024-07-29T09:05:25.084879Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"train_x","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:05:34.090219Z","iopub.execute_input":"2024-07-29T09:05:34.090622Z","iopub.status.idle":"2024-07-29T09:05:34.412047Z","shell.execute_reply.started":"2024-07-29T09:05:34.090592Z","shell.execute_reply":"2024-07-29T09:05:34.411228Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"        Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n0         Male   25                1          9.0                   0   \n1       Female   21                1         28.0                   1   \n2       Female   31                1         15.0                   0   \n3         Male   25                1         10.0                   1   \n4         Male   66                1         28.0                   1   \n...        ...  ...              ...          ...                 ...   \n999995  Female   57                1         28.0                   0   \n999996    Male   23                1         15.0                   0   \n999997  Female   26                1          8.0                   1   \n999998  Female   20                1         15.0                   1   \n999999  Female   24                1         28.0                   1   \n\n       Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  \\\n0         < 1 Year            Yes         33990.0                 152.0   \n1         < 1 Year             No         49514.0                 152.0   \n2         < 1 Year             No         36913.0                 152.0   \n3         < 1 Year             No         36274.0                 152.0   \n4         1-2 Year            Yes         44214.0                  26.0   \n...            ...            ...             ...                   ...   \n999995    1-2 Year            Yes         50895.0                  26.0   \n999996    < 1 Year            Yes         39443.0                 152.0   \n999997    < 1 Year             No         38892.0                 152.0   \n999998    < 1 Year             No         41203.0                 160.0   \n999999    < 1 Year             No         29091.0                 152.0   \n\n        Vintage  autoFE_f_0  autoFE_f_1  autoFE_f_2  autoFE_f_3  autoFE_f_4  \\\n0           166   5642340.0    0.480421       186.0      5109.0    542950.0   \n1            84   4159176.0    0.190024        82.0      8441.0    542950.0   \n2           149   5500037.0    0.451416        25.0      4808.0    542950.0   \n3           256   9286144.0    0.829111       106.0     11913.0    542950.0   \n4           223   9859722.0    0.708394        93.0      4884.0    317263.0   \n...         ...         ...         ...         ...         ...         ...   \n999995      212  10789740.0    0.683872        41.0      5298.0    317263.0   \n999996       92   3628756.0    0.224518       363.0      7282.0    542950.0   \n999997      218   8478456.0    0.685769        38.0      5395.0    542950.0   \n999998      158   6510074.0    0.475799        13.0     10440.0     83727.0   \n999999      187   5440017.0    0.564127       294.0     13009.0    542950.0   \n\n        autoFE_f_5  autoFE_f_6  autoFE_f_7 autoFE_f_8  autoFE_f_9  \\\n0         0.178632       166.0    305910.0        0.0       166.0   \n1         0.477880        84.0   1386392.0        1.0        83.0   \n2         0.248792       149.0    553695.0        2.0       149.0   \n3         0.195258       256.0    362740.0        1.0       255.0   \n4         0.485734       223.0   1237992.0        3.0       222.0   \n...            ...         ...         ...        ...         ...   \n999995    0.509225       212.0   1425060.0        0.0       212.0   \n999996    0.331764        92.0    591645.0        0.0        92.0   \n999997    0.122214       218.0    311136.0        1.0       217.0   \n999998    0.339533       158.0    618045.0        1.0       157.0   \n999999    0.526811       187.0    814548.0        1.0       186.0   \n\n        autoFE_f_10  autoFE_f_11  autoFE_f_12  autoFE_f_13  autoFE_f_14  \\\n0             166.0     0.653033          NaN     0.380000    14.485116   \n1              85.0     0.785224         84.0     0.007696    13.832753   \n2             149.0     0.741447          NaN     0.645197    13.084419   \n3             257.0     0.921735        256.0     0.466462    11.027021   \n4             224.0     0.609766        223.0     0.906301    13.832753   \n...             ...          ...          ...          ...          ...   \n999995        212.0     0.817174          NaN     0.791244    13.832753   \n999996         92.0     0.845811          NaN     0.265844    13.084419   \n999997        219.0     0.556250        218.0     0.388011    15.261335   \n999998        159.0     0.887246        158.0     0.016207    13.084419   \n999999        188.0     0.167369        187.0     0.061204    13.832753   \n\n        autoFE_f_15  autoFE_f_16  autoFE_f_17  autoFE_f_18  autoFE_f_19  \\\n0          0.529460     0.510935     0.280615      33990.0        191.0   \n1          0.923739     0.194659     0.842734      49515.0        105.0   \n2          0.715309     0.432102     0.260368      36913.0        180.0   \n3          0.700654     0.835286     0.672440      36275.0        281.0   \n4          0.790905     0.725049     0.842734      44215.0        289.0   \n...             ...          ...          ...          ...          ...   \n999995     0.908685     0.681131     0.342734      50895.0        269.0   \n999996     0.708975     0.230392     0.260368      39443.0        115.0   \n999997     0.772004     0.681054     0.735225      38893.0        244.0   \n999998     0.822324     0.458426     0.760368      41204.0        178.0   \n999999     0.404037     0.558326     0.842734      29092.0        211.0   \n\n        autoFE_f_20  autoFE_f_21  autoFE_f_22  autoFE_f_23  autoFE_f_24  \\\n0            6910.0     0.573447      34156.0     0.537776      33990.0   \n1           79439.0     0.908039      49598.0     0.922583      49514.0   \n2           34800.0     0.670870      37062.0     0.634359      36913.0   \n3           12342.0     0.655816      36530.0     0.697934      36274.0   \n4          332339.0     0.821479      44437.0     0.849935      44214.0   \n...             ...          ...          ...          ...          ...   \n999995     332339.0     0.922072      51107.0     0.911381      50895.0   \n999996      34800.0     0.749903      39535.0     0.716186      39443.0   \n999997      64583.0     0.730097      39110.0     0.769719      38892.0   \n999998      34800.0     0.784786      41361.0     0.820123      41203.0   \n999999      79439.0     0.384363      29278.0     0.398541      29091.0   \n\n        autoFE_f_25  autoFE_f_26  autoFE_f_27  autoFE_f_28  autoFE_f_29  \\\n0           34015.0  1359.600000     0.509686     0.500451     0.487574   \n1           49535.0  2357.809524     0.194317     0.205071     0.190794   \n2           36944.0  1190.741935     0.446585     0.417740     0.430972   \n3           36299.0  1450.960000     0.835485     0.824923     0.836364   \n4           44280.0   669.909091     0.701396     0.717548     0.718992   \n...             ...          ...          ...          ...          ...   \n999995      50952.0   892.894737     0.679725     0.673129     0.673843   \n999996      39466.0  1714.913043     0.229877     0.219955     0.215363   \n999997      38918.0  1495.846154     0.680803     0.681213     0.685214   \n999998      41223.0  2060.150000     0.458278     0.448081     0.455761   \n999999      29115.0  1212.125000     0.558039     0.569769     0.560980   \n\n       autoFE_f_30  autoFE_f_31  autoFE_f_32  autoFE_f_33  autoFE_f_34  \\\n0              0.0    34.986475     0.589180   223.618421     0.499175   \n1              1.0    45.709578     0.867675   325.750000     0.266412   \n2              2.0    32.053954     0.553907   242.848684     0.546335   \n3              3.0    30.023605     0.539638   238.644737     0.499175   \n4              1.0    45.709578     0.223135  1700.538462     0.303853   \n...            ...          ...          ...          ...          ...   \n999995         1.0    45.709578     0.223135  1957.500000     0.280891   \n999996         2.0    32.053954     0.553907   259.493421     0.525428   \n999997         6.0    37.423027     0.664285   255.868421     0.493532   \n999998         2.0    32.053954     0.937139   257.518750     0.546542   \n999999         1.0    45.709578     0.867675   191.388158     0.510372   \n\n        autoFE_f_35  autoFE_f_36  autoFE_f_37  autoFE_f_38  autoFE_f_39  \\\n0           17590.0       6078.0         28.0    78.856796        175.0   \n1           37064.0     191241.0         45.0    79.938583        112.0   \n2           11290.0      26554.0         26.0    80.427887        164.0   \n3           64870.0       7566.0         25.0    82.260215        266.0   \n4            6696.0     258450.0         45.0    79.938583        251.0   \n...             ...          ...          ...          ...          ...   \n999995      10586.0     191241.0         45.0    79.938583        240.0   \n999996      26487.0      25768.0         26.0    80.427887        107.0   \n999997      41903.0      58736.0         33.0    80.164582        226.0   \n999998       9361.0      26554.0         26.0    80.427887        173.0   \n999999      81032.0     191241.0         45.0    79.938583        215.0   \n\n        autoFE_f_40   autoFE_f_41  autoFE_f_42  autoFE_f_43  autoFE_f_44  \\\n0             895.0  14079.233812     0.643177       6846.0    16.888889   \n1            4709.0  15600.277416     0.937936     141443.0     5.428571   \n2             855.0  13766.828032     0.746904      27245.0    10.133333   \n3            1847.0  11346.682460     0.731384      10686.0    15.200000   \n4            4950.0  15600.277416     0.787907     141443.0     0.928571   \n...             ...           ...          ...          ...          ...   \n999995       6800.0  15600.277416     0.908179     308248.0     0.928571   \n999996       5516.0  13766.828032     0.821843      27245.0    10.133333   \n999997       4234.0  15938.073188     0.803288      70528.0    19.000000   \n999998       1695.0  13766.828032     0.851652      25077.0    10.666667   \n999999      10887.0  15600.277416     0.409586     141443.0     5.428571   \n\n        autoFE_f_45  autoFE_f_46 autoFE_f_47   autoFE_f_48  autoFE_f_49  \\\n0          0.625551     0.162050         0.0  26902.168033      30648.0   \n1          0.080719     0.662050         1.0  39558.794219      39802.0   \n2          0.946592     0.162050         2.0  28959.286744      31574.5   \n3          0.625551     0.662050         3.0  24892.287308      28015.0   \n4          0.903795     0.839407         4.0  39558.794219      39802.0   \n...             ...          ...         ...           ...          ...   \n999995     0.771002     0.339407         4.0  39558.794219      39802.0   \n999996     0.330809     0.162050        45.0  28959.286744      31574.5   \n999997     0.726146     0.662050        14.0  36023.673284      37502.0   \n999998     0.016075     0.662050         2.0  28959.286744      31574.5   \n999999     0.483864     0.662050         1.0  39558.794219      39802.0   \n\n       autoFE_f_50  autoFE_f_51  autoFE_f_52  autoFE_f_53 autoFE_f_54  \\\n0              0.0     0.554006   125.991393      33965.0         0.0   \n1              1.0     0.913898    86.787510      49493.0         1.0   \n2              1.0     0.690318   132.888517      36882.0         2.0   \n3              0.0     0.638378   139.645371      36249.0         3.0   \n4              2.0     0.810001    86.787510      44148.0         1.0   \n...            ...          ...          ...          ...         ...   \n999995         3.0     0.926544    86.787510      50838.0         4.0   \n999996         0.0     0.735255   132.888517      39420.0         2.0   \n999997         1.0     0.747863   113.786114      38866.0        14.0   \n999998         1.0     0.800013   132.888517      41183.0        33.0   \n999999         1.0     0.397682    86.787510      29067.0         1.0   \n\n        autoFE_f_55  autoFE_f_56  autoFE_f_57  autoFE_f_58  autoFE_f_59  \\\n0          0.225569     0.499973        166.0    5166480.0          NaN   \n1          0.043722     0.201356         84.0    7526128.0      49514.0   \n2          0.522780     0.439885        149.0    5610776.0          NaN   \n3          0.225569     0.842750        256.0    5513648.0      36274.0   \n4          0.938345     0.713352        223.0    1149564.0      44214.0   \n...             ...          ...          ...          ...          ...   \n999995     0.889559     0.668030        212.0    1323270.0          NaN   \n999996     0.120996     0.225462         92.0    5995336.0          NaN   \n999997     0.398904     0.693955        218.0    5911584.0      38892.0   \n999998     0.008663     0.467676        158.0    6592480.0      41203.0   \n999999     0.265695     0.569303        187.0    4421832.0      29091.0   \n\n       autoFE_f_60  autoFE_f_61  autoFE_f_62  autoFE_f_63 autoFE_f_64  \\\n0              0.0     0.505359    79.646454        127.0         0.0   \n1              1.0     0.195420    79.342478        131.0         1.0   \n2              2.0     0.431017    79.379644        121.0         2.0   \n3              3.0     0.840829    79.646454        127.0         0.0   \n4              4.0     0.714188    79.709674        -40.0         3.0   \n...            ...          ...          ...          ...         ...   \n999995         4.0     0.666054    79.898332        -31.0         4.0   \n999996         2.0     0.230227    79.875385        129.0        10.0   \n999997        20.0     0.692340    79.378535        126.0         6.0   \n999998         2.0     0.460549    81.204575        140.0        37.0   \n999999         1.0     0.566832    79.480976        128.0        12.0   \n\n        autoFE_f_65  autoFE_f_66  autoFE_f_67  autoFE_f_68  autoFE_f_69  \\\n0          6.640000        165.0     849750.0     0.438852        167.0   \n1          4.000000         83.0    1039794.0     0.314534         85.0   \n2          4.806452        148.0    1144303.0     0.479282        150.0   \n3         10.240000        255.0     906850.0     0.655181        257.0   \n4          3.378788        222.0    2918124.0     0.314534        224.0   \n...             ...          ...          ...          ...          ...   \n999995     3.719298        211.0    2901015.0     0.314534        213.0   \n999996     4.000000         91.0     907189.0     0.479282         93.0   \n999997     8.384615        217.0    1011192.0     0.529557        219.0   \n999998     7.900000        157.0     824060.0     0.479282        159.0   \n999999     7.791667        186.0     698184.0     0.314534        188.0   \n\n        autoFE_f_70  autoFE_f_71  autoFE_f_72  autoFE_f_73  autoFE_f_74  \\\n0           33990.0     0.466655        177.0      34142.0      81240.0   \n1           49514.0     0.466655        173.0      49666.0      63813.0   \n2           36913.0     0.466655        183.0      37065.0      15033.0   \n3           36274.0     0.466655        177.0      36426.0      81240.0   \n4           44214.0     0.235340         92.0      44240.0       9586.0   \n...             ...          ...          ...          ...          ...   \n999995      50895.0     0.235340         83.0      50921.0      14132.0   \n999996      39443.0     0.466655        175.0      39595.0      96175.0   \n999997      38892.0     0.466655        178.0      39044.0      50923.0   \n999998      41203.0     0.935515        180.0      41363.0      21118.0   \n999999      29091.0     0.466655        176.0      29243.0     104911.0   \n\n        autoFE_f_75  autoFE_f_76  autoFE_f_77  autoFE_f_78  autoFE_f_79  \\\n0           35526.0        318.0     290594.0       1494.0         25.0   \n1           36329.0        236.0     488138.0       2352.0         28.0   \n2            9979.0        301.0     346200.0       2235.0         31.0   \n3           35526.0        408.0     290594.0       2560.0         25.0   \n4            6662.0        249.0     308615.0       6244.0         66.0   \n...             ...          ...          ...          ...          ...   \n999995       4912.0        238.0     336395.0       5936.0         57.0   \n999996      42027.0        244.0     294467.0       1380.0         23.0   \n999997      29191.0        370.0     540165.0       1744.0         26.0   \n999998      11921.0        318.0     117057.0       2370.0         20.0   \n999999      60267.0        339.0     495106.0       5236.0         28.0   \n\n        autoFE_f_80  autoFE_f_81  autoFE_f_82  autoFE_f_83  autoFE_f_84  \\\n0          0.496267      33999.0        152.0       3800.0      33990.0   \n1          0.464330      49542.0         84.0       3192.0      49513.0   \n2          0.499575      36928.0        149.0       4712.0      36913.0   \n3          0.475324      36284.0        152.0       3800.0      36273.0   \n4          0.464330      44242.0         26.0       1716.0      44213.0   \n...             ...          ...          ...          ...          ...   \n999995     0.464330      50923.0         26.0       1482.0      50895.0   \n999996     0.499575      39458.0         92.0       3496.0      39443.0   \n999997     0.499127      38900.0        152.0       3952.0      38891.0   \n999998     0.499575      41218.0        158.0       3200.0      41202.0   \n999999     0.464330      29119.0        152.0       3648.0      29090.0   \n\n        autoFE_f_85  autoFE_f_86  autoFE_f_87  autoFE_f_88  autoFE_f_89  \\\n0             143.0        166.0        157.0      82460.0     0.998033   \n1             124.0         84.0         56.0      65868.0     0.997138   \n2             137.0        149.0        134.0      21118.0     0.998624   \n3             142.0        256.0        246.0      82460.0     0.999387   \n4              -2.0        223.0        195.0      10791.0     0.997138   \n...             ...          ...          ...          ...          ...   \n999995         -2.0        212.0        184.0      15885.0     0.997138   \n999996        137.0         92.0         77.0      97589.0     0.998624   \n999997        144.0        218.0        210.0      52182.0     0.997808   \n999998        145.0        158.0        143.0      22312.0     0.998624   \n999999        124.0        187.0        159.0     106390.0     0.997138   \n\n        autoFE_f_90  autoFE_f_91  autoFE_f_92  autoFE_f_93  autoFE_f_94  \\\n0           33838.0         80.0        225.0     0.147187   204.759036   \n1           49362.0         85.0        588.0     0.492940   589.452381   \n2           36761.0         81.0        465.0     0.286846   247.738255   \n3           36122.0         80.0        250.0     0.193205   141.695312   \n4           44188.0         85.0       1848.0     0.496518   198.269058   \n...             ...          ...          ...          ...          ...   \n999995      50869.0         85.0       1596.0     0.496518   240.070755   \n999996      39291.0         81.0        345.0     0.213765   428.728261   \n999997      38740.0         83.0        208.0     0.127158   178.403670   \n999998      41043.0         81.0        300.0     0.286846   260.778481   \n999999      28939.0         85.0        672.0     0.492940   155.566845   \n\n       autoFE_f_95  autoFE_f_96  autoFE_f_97  autoFE_f_98  autoFE_f_99  \\\n0              0.0     751966.0         25.0       6063.0      33981.0   \n1              1.0     745026.0         21.0     152580.0      49486.0   \n2              2.0     745026.0         31.0      29458.0      36898.0   \n3              3.0     745026.0         25.0      11198.0      36264.0   \n4              4.0     751966.0         66.0     297111.0      44186.0   \n...            ...          ...          ...          ...          ...   \n999995         1.0     751966.0         57.0     297111.0      50867.0   \n999996        49.0     751966.0         23.0      22864.0      39428.0   \n999997        14.0     745026.0         26.0      75072.0      38884.0   \n999998         2.0     745026.0         20.0      29458.0      41188.0   \n999999         1.0     745026.0         24.0     152580.0      29063.0   \n\n        autoFE_f_100  autoFE_f_101  autoFE_f_102  autoFE_f_103  autoFE_f_104  \\\n0              -14.0  12986.968804   3776.666667      2.777778      0.129475   \n1               68.0  13812.283547   1768.357143      0.750000      0.730673   \n2                3.0  17397.458965   2460.866667      2.066667      0.254759   \n3             -104.0  12986.968804   3627.400000      2.500000      0.629475   \n4             -197.0  19314.044119   1579.071429      2.357143      0.813899   \n...              ...           ...           ...           ...           ...   \n999995        -186.0  18669.048339   1817.678571      2.035714      0.341261   \n999996          60.0  12873.909148   2629.533333      1.533333      0.154331   \n999997         -66.0  14016.918475   4861.500000      3.250000      0.616456   \n999998           2.0  15163.198069   2746.866667      1.333333      0.799892   \n999999         -35.0  12837.501221   1038.964286      0.857143      0.641473   \n\n        autoFE_f_105  autoFE_f_106 autoFE_f_107  autoFE_f_108 autoFE_f_109  \\\n0            12176.0         152.0          0.0          16.0          0.0   \n1           448404.0         152.0          1.0          -7.0          1.0   \n2            52250.0         152.0          2.0          16.0          2.0   \n3            16300.0         152.0          3.0          15.0          0.0   \n4           448404.0          26.0          4.0          38.0          3.0   \n...              ...           ...          ...           ...          ...   \n999995      448404.0          26.0          5.0          29.0          4.0   \n999996       52250.0         152.0         50.0           8.0         10.0   \n999997      132891.0         152.0          7.0          18.0          6.0   \n999998       52250.0         160.0         46.0           5.0         35.0   \n999999      448404.0         152.0         13.0          -4.0         12.0   \n\n        autoFE_f_110  autoFE_f_111  autoFE_f_112  autoFE_f_113  autoFE_f_114  \\\n0                NaN        4150.0      0.752077          25.0    162.882705   \n1              152.0        1764.0      0.627876          21.0    163.305323   \n2                NaN        4619.0      0.627876          31.0    166.741256   \n3              152.0        6400.0      0.627876          25.0    164.072226   \n4               26.0       14718.0      0.187160          26.0    163.305323   \n...              ...           ...           ...           ...           ...   \n999995           NaN       12084.0      0.187160          26.0    163.305323   \n999996           NaN        2116.0      0.752077          23.0    166.741256   \n999997         152.0        5668.0      0.627876          26.0    165.422133   \n999998         160.0        3160.0      0.967104          20.0    166.741256   \n999999         152.0        4488.0      0.627876          24.0    163.305323   \n\n        autoFE_f_115  autoFE_f_116  autoFE_f_117  autoFE_f_118 autoFE_f_119  \\\n0               34.0       25232.0     25.408137      1.000000          0.0   \n1               49.0       12768.0     26.768385      1.000000          1.0   \n2               46.0       22648.0     26.660858      1.000000          2.0   \n3               35.0       38912.0     25.408137      1.000000          0.0   \n4               94.0        5798.0     27.238165      0.990000          3.0   \n...              ...           ...           ...           ...          ...   \n999995          85.0        5512.0     26.352306      0.996862         93.0   \n999996          38.0       13984.0     25.517681      1.000000         38.0   \n999997          34.0       33136.0     25.994481      0.999981          6.0   \n999998          35.0       25280.0     26.359745      0.999955         46.0   \n999999          52.0       28424.0     24.750985      0.999991         14.0   \n\n        autoFE_f_120  autoFE_f_121  autoFE_f_122 autoFE_f_123  autoFE_f_124  \\\n0             1368.0         152.0           1.0          0.0      0.149294   \n1             4256.0         152.0           1.0          1.0      0.490040   \n2             2280.0         152.0           1.0          2.0      0.218216   \n3             1520.0         152.0           1.0          3.0      0.193213   \n4              728.0          66.0           1.0          4.0      0.490040   \n...              ...           ...           ...          ...           ...   \n999995         728.0          57.0           1.0          5.0      0.498794   \n999996        2280.0         152.0           1.0         55.0      0.218216   \n999997        1216.0         152.0           1.0        228.0      0.127079   \n999998        2400.0         160.0           1.0        689.0      0.287058   \n999999        4256.0         152.0           1.0        118.0      0.490040   \n\n        autoFE_f_125  autoFE_f_126  autoFE_f_127  autoFE_f_128  autoFE_f_129  \\\n0           212903.0      0.000000      186375.0           9.0         152.0   \n1           444005.0      0.000000      470533.0          29.0         152.0   \n2           212903.0      0.000000      470533.0          15.0         152.0   \n3           444005.0      0.000000      470533.0          11.0         152.0   \n4           250795.0      0.099503      505403.0          29.0          28.0   \n...              ...           ...           ...           ...           ...   \n999995      530041.0      0.055929      505403.0          28.0          28.0   \n999996      212903.0      0.000000      186375.0          15.0         152.0   \n999997      444005.0      0.004378      470533.0           9.0         152.0   \n999998      444005.0      0.006695      470533.0          16.0         160.0   \n999999      444005.0      0.003066      470533.0          29.0         152.0   \n\n        autoFE_f_130  autoFE_f_131  autoFE_f_132  autoFE_f_133  autoFE_f_134  \\\n0            21352.0      5.023881      0.111111           9.0           9.0   \n1            35481.0      5.023881      0.035714          28.0          28.0   \n2            10759.0      5.023881      0.066667          15.0          15.0   \n3            61108.0      5.023881      0.100000          10.0          10.0   \n4             4058.0      3.258097      0.035714          28.0          28.0   \n...              ...           ...           ...           ...           ...   \n999995       10875.0      3.258097      0.035714          28.0          28.0   \n999996       30121.0      5.023881      0.066667          15.0          15.0   \n999997       40030.0      5.023881      0.125000           8.0           8.0   \n999998        8931.0      5.075174      0.066667          15.0          15.0   \n999999       76289.0      5.023881      0.035714          28.0          28.0   \n\n        autoFE_f_135  autoFE_f_136  autoFE_f_137  autoFE_f_138  autoFE_f_139  \\\n0            33990.0      3.000000      0.632113           1.0    167.142991   \n1            49514.0      5.291503      0.942969           1.0    166.127907   \n2            36913.0      3.872983      0.766029           0.0    163.824747   \n3            36274.0      3.162278      0.719155           1.0    167.142991   \n4            44214.0      5.291503      0.709128           0.0    165.347064   \n...              ...           ...           ...           ...           ...   \n999995       50895.0      5.291503      0.882523           0.0    160.584123   \n999996       39443.0      3.872983      0.818914           1.0    163.074804   \n999997       38892.0      2.828427      0.807006           1.0    165.493302   \n999998       41203.0      3.872983      0.890423           0.0    161.894635   \n999999       29091.0      5.291503      0.370257           1.0    165.618351   \n\n        autoFE_f_140 autoFE_f_141  autoFE_f_142  autoFE_f_143  autoFE_f_144  \\\n0           444306.0          0.0           0.0           9.0      809627.0   \n1           378441.0          1.0       49514.0          27.0      687365.0   \n2           378441.0          2.0           0.0          15.0      687365.0   \n3           367578.0          3.0       36274.0           9.0      809627.0   \n4           444306.0          4.0       44214.0          27.0      809627.0   \n...              ...          ...           ...           ...           ...   \n999995      309675.0          5.0           0.0          28.0      687365.0   \n999996      444306.0         52.0           0.0          15.0      809627.0   \n999997      378441.0          7.0       38892.0           7.0      687365.0   \n999998      378441.0         48.0       41203.0          14.0      687365.0   \n999999      378441.0         13.0       29091.0          27.0      687365.0   \n\n        autoFE_f_145  autoFE_f_146  autoFE_f_147  autoFE_f_148  autoFE_f_149  \\\n0           0.268198     47.998774           0.0      0.186864           0.0   \n1           0.768198     54.847088           1.0      0.497347         152.0   \n2           0.268198     43.929969           0.0      0.313013           0.0   \n3           0.768198     35.676699           0.0      0.201518         152.0   \n4           0.768198     54.847088           0.0      0.492609          26.0   \n...              ...           ...           ...           ...           ...   \n999995      0.268198     54.847088           0.0      0.492609           0.0   \n999996      0.268198     43.929969           0.0      0.313013           0.0   \n999997      0.768198     55.230125           0.0      0.132448         152.0   \n999998      0.768198     43.929969           1.0      0.313013         160.0   \n999999      0.768198     54.847088           0.0      0.497347         152.0   \n\n        autoFE_f_150  autoFE_f_151  autoFE_f_152  autoFE_f_153  autoFE_f_154  \\\n0            12200.0         299.0      0.181256      0.729068     13.707443   \n1           449691.0         299.0      0.038379      0.644572     15.008618   \n2            52322.0         299.0      0.267417      0.644572     13.183065   \n3            16310.0         299.0      0.393759      0.729068     13.707443   \n4           449691.0         299.0      0.952428      0.162168     11.906707   \n...              ...           ...           ...           ...           ...   \n999995      449691.0         299.0      0.837216      0.121649     12.302707   \n999996       52322.0         299.0      0.111895      0.729068     14.309252   \n999997      133183.0         299.0      0.466524      0.644572     13.567714   \n999998       52322.0         299.0      0.006426      0.962357     15.621437   \n999999      449691.0         299.0      0.294906      0.644572     14.194695   \n\n        autoFE_f_155 autoFE_f_156  autoFE_f_157  autoFE_f_158  autoFE_f_159  \\\n0          18.444444          0.0           NaN          25.0         161.0   \n1           3.000000          1.0           1.0          21.0         180.0   \n2           9.933333          1.0           NaN          31.0         167.0   \n3          25.600000          1.0           1.0          25.0         162.0   \n4           7.964286          0.0           1.0          66.0          54.0   \n...              ...          ...           ...           ...           ...   \n999995      7.571429          0.0           NaN          57.0          54.0   \n999996      6.133333          0.0           NaN          23.0         167.0   \n999997     27.250000          1.0           1.0          26.0         160.0   \n999998     10.533333          1.0           1.0          20.0         175.0   \n999999      6.678571          1.0           1.0          24.0         180.0   \n\n        autoFE_f_160  autoFE_f_161  autoFE_f_162  autoFE_f_163  autoFE_f_164  \\\n0               24.0      0.280270           0.0         166.0         153.0   \n1               20.0      0.036905           0.0         152.0         153.0   \n2               30.0      0.433391           0.0         152.0         153.0   \n3               24.0      0.280270           0.0         256.0         153.0   \n4               65.0      0.944127           0.0         223.0          27.0   \n...              ...           ...           ...           ...           ...   \n999995          56.0      0.865969           0.0         212.0          27.0   \n999996          22.0      0.149064           0.0         152.0         153.0   \n999997          25.0      0.325241           0.0         218.0         153.0   \n999998          19.0      0.007453           0.0         160.0         161.0   \n999999          23.0      0.217194           0.0         187.0         153.0   \n\n        autoFE_f_165  autoFE_f_166  autoFE_f_167  autoFE_f_168  autoFE_f_169  \\\n0           0.689718          25.0           1.0      0.748409          10.0   \n1           0.689718          21.0           1.0      0.622999          29.0   \n2           0.689718          31.0           1.0      0.748409          16.0   \n3           0.689718          25.0           1.0      0.622999          11.0   \n4           0.143149          66.0           1.0      0.095901          29.0   \n...              ...           ...           ...           ...           ...   \n999995      0.143149          57.0           1.0      0.184740          29.0   \n999996      0.689718          23.0           1.0      0.748409          16.0   \n999997      0.689718          26.0           1.0      0.622999           9.0   \n999998      0.964688          20.0           1.0      0.965784          16.0   \n999999      0.689718          24.0           1.0      0.622999          29.0   \n\n        autoFE_f_170 autoFE_f_171  autoFE_f_172  autoFE_f_173  autoFE_f_174  \\\n0           0.173696          0.0      0.915663         166.0         166.0   \n1           0.037390          1.0      1.809524         166.0          84.0   \n2           0.613586          0.0      1.020134         171.0         149.0   \n3           0.386855          1.0      0.593750         169.0         256.0   \n4           0.932316          2.0      0.116592         166.0         223.0   \n...              ...          ...           ...           ...           ...   \n999995      0.830667          3.0      0.122642         166.0         212.0   \n999996      0.110833          0.0      1.652174         171.0          92.0   \n999997      0.458417          1.0      0.697248         168.0         218.0   \n999998      0.006275          1.0      1.012658         171.0         158.0   \n999999      0.289068          1.0      0.812834         166.0         187.0   \n\n        autoFE_f_175  autoFE_f_176  autoFE_f_177  autoFE_f_178  autoFE_f_179  \\\n0                1.0           1.0          61.0      2.197225      0.741062   \n1                1.0           2.0          66.0      3.332205      0.538668   \n2                1.0           1.0          62.0      2.708050      0.490529   \n3                1.0           2.0          61.0      2.302585      0.741062   \n4                1.0           2.0          66.0      3.332205      0.372294   \n...              ...           ...           ...           ...           ...   \n999995           1.0           1.0          66.0      3.332205      0.317540   \n999996           1.0           1.0          62.0      2.708050      0.691348   \n999997           1.0           2.0          64.0      2.079442      0.767108   \n999998           1.0           2.0          62.0      2.708050      0.400260   \n999999           1.0           2.0          66.0      3.332205      0.717063   \n\n        autoFE_f_180  autoFE_f_181 autoFE_f_182  autoFE_f_183  autoFE_f_184  \\\n0           0.999877      0.501025          0.0      0.164303       30984.0   \n1           1.000000      0.501432          1.0      0.494718       30502.0   \n2           1.000000      0.500698          1.0      0.250191       29007.5   \n3           0.999955      0.500337          1.0      0.173814       30984.0   \n4           1.000000      0.501432          2.0      0.494718       35500.0   \n...              ...           ...          ...           ...           ...   \n999995      1.000000      0.501432          2.0      0.494718       34807.0   \n999996      1.000000      0.500698          0.0      0.250191       30897.0   \n999997      0.999665      0.501100          1.0      0.115850       30288.0   \n999998      1.000000      0.500698          1.0      0.250191       29399.0   \n999999      1.000000      0.501432          1.0      0.494718       31420.0   \n\n        autoFE_f_185  autoFE_f_186  autoFE_f_187  autoFE_f_188  autoFE_f_189  \\\n0           0.501280         152.0       32993.0          -8.0      6.080000   \n1           0.500683         122.0       30962.0         -27.0      7.238095   \n2           0.501280         152.0       30962.0         -14.0      4.903226   \n3           0.500683         152.0       30962.0          -9.0      6.080000   \n4           0.500683         122.0       32993.0         -27.0      0.393939   \n...              ...           ...           ...           ...           ...   \n999995      0.501280         122.0       32993.0         -27.0      0.456140   \n999996      0.501280         152.0       32993.0         -14.0      6.608696   \n999997      0.500683         152.0       30962.0          -7.0      5.846154   \n999998      0.500683         152.0       30962.0         -14.0      8.000000   \n999999      0.500683         122.0       30962.0         -27.0      6.333333   \n\n        autoFE_f_190  autoFE_f_191  autoFE_f_192  autoFE_f_193  autoFE_f_194  \\\n0              163.0           3.0           0.0           1.0           9.0   \n1              163.0           3.0          21.0           1.0          21.0   \n2              163.0           3.0           0.0           1.0          15.0   \n3              163.0           3.0          25.0           1.0          10.0   \n4              163.0           3.0          66.0           1.0          28.0   \n...              ...           ...           ...           ...           ...   \n999995         163.0           3.0           0.0           1.0          28.0   \n999996         163.0           3.0           0.0           1.0          15.0   \n999997         163.0           3.0          26.0           1.0           8.0   \n999998         163.0           3.0          20.0           1.0          15.0   \n999999         163.0           3.0          24.0           1.0          24.0   \n\n        autoFE_f_195  autoFE_f_196  autoFE_f_197  autoFE_f_198  autoFE_f_199  \\\n0                0.0           NaN         152.0          25.0      0.166778   \n1                1.0          28.0         151.0          20.0      0.493777   \n2                0.0           NaN         152.0          31.0      0.254576   \n3                1.0          10.0         151.0          24.0      0.175181   \n4                1.0          28.0          25.0          65.0      0.495553   \n...              ...           ...           ...           ...           ...   \n999995           0.0           NaN          26.0          57.0      0.493777   \n999996           0.0           NaN         152.0          23.0      0.246328   \n999997           1.0           8.0         151.0          25.0      0.114239   \n999998           1.0          15.0         159.0          19.0      0.254576   \n999999           1.0          28.0         151.0          23.0      0.493777   \n\n        autoFE_f_200  autoFE_f_201  autoFE_f_202  autoFE_f_203  autoFE_f_204  \\\n0               25.0  31400.949636           1.0           1.0         164.0   \n1               22.0  30066.032353           0.0           1.0         170.0   \n2               31.0  25857.063169           1.0           1.0         164.0   \n3               26.0  31400.949636           0.0           1.0         170.0   \n4               67.0  32524.870734           0.0           1.0         170.0   \n...              ...           ...           ...           ...           ...   \n999995          57.0  31709.833448           1.0           1.0         164.0   \n999996          23.0  31093.172755           1.0           1.0         164.0   \n999997          27.0  30028.138877           0.0           1.0         170.0   \n999998          21.0  26111.556402           0.0           1.0         170.0   \n999999          25.0  31875.865750           0.0           1.0         170.0   \n\n        autoFE_f_205  autoFE_f_206  autoFE_f_207  autoFE_f_208  autoFE_f_209  \\\n0              152.0      291520.0           1.0       82460.0          20.0   \n1              153.0      365388.0           1.0       65868.0          20.0   \n2              152.0      365388.0           1.0       21118.0          20.0   \n3              153.0      291520.0           1.0       82460.0          20.0   \n4               27.0      479626.0           1.0       10900.0          20.0   \n...              ...           ...           ...           ...           ...   \n999995          26.0      301210.0           1.0       15935.0          20.0   \n999996         152.0      291520.0           1.0       97589.0          20.0   \n999997         153.0      365388.0           1.0       52183.0          20.0   \n999998         161.0      365388.0           1.0       22313.0          20.0   \n999999         153.0      365388.0           1.0      106391.0          20.0   \n\n        autoFE_f_210  autoFE_f_211  autoFE_f_212  autoFE_f_213  autoFE_f_214  \\\n0                1.0           9.0           0.0           2.0           0.0   \n1                1.0          28.0           0.0           2.0           1.0   \n2                1.0          15.0           0.0           2.0           0.0   \n3                1.0          10.0           1.0           2.0           1.0   \n4                1.0          28.0           0.0           2.0           1.0   \n...              ...           ...           ...           ...           ...   \n999995           1.0          28.0           0.0           2.0           0.0   \n999996           1.0          15.0           0.0           2.0           0.0   \n999997           1.0           8.0           1.0           2.0           1.0   \n999998           1.0          15.0           0.0           2.0           1.0   \n999999           1.0          28.0           0.0           2.0           1.0   \n\n        autoFE_f_215  autoFE_f_216  autoFE_f_217  autoFE_f_218  autoFE_f_219  \\\n0         163.172999      0.438054          26.0           NaN         152.0   \n1         164.729211      0.498506          22.0          21.0         152.0   \n2         164.729211      0.499922          32.0           NaN         152.0   \n3         163.172999      0.438054          26.0          25.0         152.0   \n4         163.172999      0.483438          67.0          66.0          26.0   \n...              ...           ...           ...           ...           ...   \n999995    164.729211      0.465534          58.0           NaN          26.0   \n999996    163.172999      0.461939          24.0           NaN         152.0   \n999997    164.729211      0.422678          27.0          26.0         152.0   \n999998    164.729211      0.489962          21.0          20.0         160.0   \n999999    164.729211      0.450428          25.0          24.0         152.0   \n\n        autoFE_f_220  autoFE_f_221  autoFE_f_222  autoFE_f_223  autoFE_f_224  \\\n0              152.0           9.0           1.0         151.0          53.0   \n1              152.0          28.0           1.0         151.0          52.0   \n2              152.0          15.0           1.0         151.0          53.0   \n3              152.0          10.0           1.0         151.0          53.0   \n4               26.0          26.0           1.0          25.0          53.0   \n...              ...           ...           ...           ...           ...   \n999995          30.0          26.0           1.0          25.0          52.0   \n999996         152.0          15.0           1.0         151.0          53.0   \n999997         152.0           8.0           1.0         151.0          53.0   \n999998         160.0          15.0           1.0         159.0          50.0   \n999999         152.0          28.0           1.0         151.0          53.0   \n\n       autoFE_f_225  autoFE_f_226  autoFE_f_227  autoFE_f_228  autoFE_f_229  \\\n0               0.0     20.478651          10.0       33824.0           9.0   \n1               1.0     12.379879          10.0       49430.0          28.0   \n2               2.0     37.619003          10.0       36764.0          15.0   \n3               3.0     20.478651          10.0       36018.0          10.0   \n4               3.0     53.125551          10.0       43991.0          28.0   \n...             ...           ...           ...           ...           ...   \n999995          2.0     53.878391          10.0       50683.0          28.0   \n999996          0.0     28.867072          10.0       39351.0          15.0   \n999997          1.0     21.641112          10.0       38674.0           8.0   \n999998          1.0     14.275307          10.0       41045.0          15.0   \n999999          1.0     17.997360          10.0       28904.0          28.0   \n\n        autoFE_f_230  autoFE_f_231  autoFE_f_232  autoFE_f_233  autoFE_f_234  \\\n0           0.500006      0.288312          10.0           1.0         166.0   \n1           0.500008      0.744792          10.0           1.0          84.0   \n2           0.500024      0.244792          10.0           1.0         149.0   \n3           0.500006      0.788312          10.0           1.0         256.0   \n4           0.505046      0.788312          10.0           1.0         223.0   \n...              ...           ...           ...           ...           ...   \n999995      0.501600      0.244792          10.0           1.0         212.0   \n999996      0.500005      0.288312          10.0           1.0          92.0   \n999997      0.500019      0.744792          10.0           1.0         218.0   \n999998      0.500045      0.744792          10.0           1.0         158.0   \n999999      0.500009      0.744792          10.0           1.0         187.0   \n\n        autoFE_f_235 autoFE_f_236  autoFE_f_237  autoFE_f_238  autoFE_f_239  \\\n0              299.0          0.0      313424.0           0.0      717515.0   \n1              299.0          1.0      540165.0          28.0      658494.0   \n2              299.0          1.0      309870.0           0.0       87525.0   \n3              299.0          2.0       85517.0          10.0      658494.0   \n4              299.0          0.0      540165.0          28.0       36466.0   \n...              ...          ...           ...           ...           ...   \n999995         299.0          3.0      540165.0           0.0      717515.0   \n999996         299.0          0.0      309870.0           0.0      717515.0   \n999997         299.0          1.0      336395.0           8.0      658494.0   \n999998         299.0          1.0      309870.0          15.0      658494.0   \n999999         299.0          1.0      540165.0          28.0      658494.0   \n\n        autoFE_f_240  \n0         148.647138  \n1         154.448108  \n2         135.694052  \n3         148.647138  \n4          71.982385  \n...              ...  \n999995     75.078193  \n999996    145.131931  \n999997    148.248644  \n999998    158.052257  \n999999    149.376930  \n\n[1000000 rows x 251 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Driving_License</th>\n      <th>Region_Code</th>\n      <th>Previously_Insured</th>\n      <th>Vehicle_Age</th>\n      <th>Vehicle_Damage</th>\n      <th>Annual_Premium</th>\n      <th>Policy_Sales_Channel</th>\n      <th>Vintage</th>\n      <th>autoFE_f_0</th>\n      <th>autoFE_f_1</th>\n      <th>autoFE_f_2</th>\n      <th>autoFE_f_3</th>\n      <th>autoFE_f_4</th>\n      <th>autoFE_f_5</th>\n      <th>autoFE_f_6</th>\n      <th>autoFE_f_7</th>\n      <th>autoFE_f_8</th>\n      <th>autoFE_f_9</th>\n      <th>autoFE_f_10</th>\n      <th>autoFE_f_11</th>\n      <th>autoFE_f_12</th>\n      <th>autoFE_f_13</th>\n      <th>autoFE_f_14</th>\n      <th>autoFE_f_15</th>\n      <th>autoFE_f_16</th>\n      <th>autoFE_f_17</th>\n      <th>autoFE_f_18</th>\n      <th>autoFE_f_19</th>\n      <th>autoFE_f_20</th>\n      <th>autoFE_f_21</th>\n      <th>autoFE_f_22</th>\n      <th>autoFE_f_23</th>\n      <th>autoFE_f_24</th>\n      <th>autoFE_f_25</th>\n      <th>autoFE_f_26</th>\n      <th>autoFE_f_27</th>\n      <th>autoFE_f_28</th>\n      <th>autoFE_f_29</th>\n      <th>autoFE_f_30</th>\n      <th>autoFE_f_31</th>\n      <th>autoFE_f_32</th>\n      <th>autoFE_f_33</th>\n      <th>autoFE_f_34</th>\n      <th>autoFE_f_35</th>\n      <th>autoFE_f_36</th>\n      <th>autoFE_f_37</th>\n      <th>autoFE_f_38</th>\n      <th>autoFE_f_39</th>\n      <th>autoFE_f_40</th>\n      <th>autoFE_f_41</th>\n      <th>autoFE_f_42</th>\n      <th>autoFE_f_43</th>\n      <th>autoFE_f_44</th>\n      <th>autoFE_f_45</th>\n      <th>autoFE_f_46</th>\n      <th>autoFE_f_47</th>\n      <th>autoFE_f_48</th>\n      <th>autoFE_f_49</th>\n      <th>autoFE_f_50</th>\n      <th>autoFE_f_51</th>\n      <th>autoFE_f_52</th>\n      <th>autoFE_f_53</th>\n      <th>autoFE_f_54</th>\n      <th>autoFE_f_55</th>\n      <th>autoFE_f_56</th>\n      <th>autoFE_f_57</th>\n      <th>autoFE_f_58</th>\n      <th>autoFE_f_59</th>\n      <th>autoFE_f_60</th>\n      <th>autoFE_f_61</th>\n      <th>autoFE_f_62</th>\n      <th>autoFE_f_63</th>\n      <th>autoFE_f_64</th>\n      <th>autoFE_f_65</th>\n      <th>autoFE_f_66</th>\n      <th>autoFE_f_67</th>\n      <th>autoFE_f_68</th>\n      <th>autoFE_f_69</th>\n      <th>autoFE_f_70</th>\n      <th>autoFE_f_71</th>\n      <th>autoFE_f_72</th>\n      <th>autoFE_f_73</th>\n      <th>autoFE_f_74</th>\n      <th>autoFE_f_75</th>\n      <th>autoFE_f_76</th>\n      <th>autoFE_f_77</th>\n      <th>autoFE_f_78</th>\n      <th>autoFE_f_79</th>\n      <th>autoFE_f_80</th>\n      <th>autoFE_f_81</th>\n      <th>autoFE_f_82</th>\n      <th>autoFE_f_83</th>\n      <th>autoFE_f_84</th>\n      <th>autoFE_f_85</th>\n      <th>autoFE_f_86</th>\n      <th>autoFE_f_87</th>\n      <th>autoFE_f_88</th>\n      <th>autoFE_f_89</th>\n      <th>autoFE_f_90</th>\n      <th>autoFE_f_91</th>\n      <th>autoFE_f_92</th>\n      <th>autoFE_f_93</th>\n      <th>autoFE_f_94</th>\n      <th>autoFE_f_95</th>\n      <th>autoFE_f_96</th>\n      <th>autoFE_f_97</th>\n      <th>autoFE_f_98</th>\n      <th>autoFE_f_99</th>\n      <th>autoFE_f_100</th>\n      <th>autoFE_f_101</th>\n      <th>autoFE_f_102</th>\n      <th>autoFE_f_103</th>\n      <th>autoFE_f_104</th>\n      <th>autoFE_f_105</th>\n      <th>autoFE_f_106</th>\n      <th>autoFE_f_107</th>\n      <th>autoFE_f_108</th>\n      <th>autoFE_f_109</th>\n      <th>autoFE_f_110</th>\n      <th>autoFE_f_111</th>\n      <th>autoFE_f_112</th>\n      <th>autoFE_f_113</th>\n      <th>autoFE_f_114</th>\n      <th>autoFE_f_115</th>\n      <th>autoFE_f_116</th>\n      <th>autoFE_f_117</th>\n      <th>autoFE_f_118</th>\n      <th>autoFE_f_119</th>\n      <th>autoFE_f_120</th>\n      <th>autoFE_f_121</th>\n      <th>autoFE_f_122</th>\n      <th>autoFE_f_123</th>\n      <th>autoFE_f_124</th>\n      <th>autoFE_f_125</th>\n      <th>autoFE_f_126</th>\n      <th>autoFE_f_127</th>\n      <th>autoFE_f_128</th>\n      <th>autoFE_f_129</th>\n      <th>autoFE_f_130</th>\n      <th>autoFE_f_131</th>\n      <th>autoFE_f_132</th>\n      <th>autoFE_f_133</th>\n      <th>autoFE_f_134</th>\n      <th>autoFE_f_135</th>\n      <th>autoFE_f_136</th>\n      <th>autoFE_f_137</th>\n      <th>autoFE_f_138</th>\n      <th>autoFE_f_139</th>\n      <th>autoFE_f_140</th>\n      <th>autoFE_f_141</th>\n      <th>autoFE_f_142</th>\n      <th>autoFE_f_143</th>\n      <th>autoFE_f_144</th>\n      <th>autoFE_f_145</th>\n      <th>autoFE_f_146</th>\n      <th>autoFE_f_147</th>\n      <th>autoFE_f_148</th>\n      <th>autoFE_f_149</th>\n      <th>autoFE_f_150</th>\n      <th>autoFE_f_151</th>\n      <th>autoFE_f_152</th>\n      <th>autoFE_f_153</th>\n      <th>autoFE_f_154</th>\n      <th>autoFE_f_155</th>\n      <th>autoFE_f_156</th>\n      <th>autoFE_f_157</th>\n      <th>autoFE_f_158</th>\n      <th>autoFE_f_159</th>\n      <th>autoFE_f_160</th>\n      <th>autoFE_f_161</th>\n      <th>autoFE_f_162</th>\n      <th>autoFE_f_163</th>\n      <th>autoFE_f_164</th>\n      <th>autoFE_f_165</th>\n      <th>autoFE_f_166</th>\n      <th>autoFE_f_167</th>\n      <th>autoFE_f_168</th>\n      <th>autoFE_f_169</th>\n      <th>autoFE_f_170</th>\n      <th>autoFE_f_171</th>\n      <th>autoFE_f_172</th>\n      <th>autoFE_f_173</th>\n      <th>autoFE_f_174</th>\n      <th>autoFE_f_175</th>\n      <th>autoFE_f_176</th>\n      <th>autoFE_f_177</th>\n      <th>autoFE_f_178</th>\n      <th>autoFE_f_179</th>\n      <th>autoFE_f_180</th>\n      <th>autoFE_f_181</th>\n      <th>autoFE_f_182</th>\n      <th>autoFE_f_183</th>\n      <th>autoFE_f_184</th>\n      <th>autoFE_f_185</th>\n      <th>autoFE_f_186</th>\n      <th>autoFE_f_187</th>\n      <th>autoFE_f_188</th>\n      <th>autoFE_f_189</th>\n      <th>autoFE_f_190</th>\n      <th>autoFE_f_191</th>\n      <th>autoFE_f_192</th>\n      <th>autoFE_f_193</th>\n      <th>autoFE_f_194</th>\n      <th>autoFE_f_195</th>\n      <th>autoFE_f_196</th>\n      <th>autoFE_f_197</th>\n      <th>autoFE_f_198</th>\n      <th>autoFE_f_199</th>\n      <th>autoFE_f_200</th>\n      <th>autoFE_f_201</th>\n      <th>autoFE_f_202</th>\n      <th>autoFE_f_203</th>\n      <th>autoFE_f_204</th>\n      <th>autoFE_f_205</th>\n      <th>autoFE_f_206</th>\n      <th>autoFE_f_207</th>\n      <th>autoFE_f_208</th>\n      <th>autoFE_f_209</th>\n      <th>autoFE_f_210</th>\n      <th>autoFE_f_211</th>\n      <th>autoFE_f_212</th>\n      <th>autoFE_f_213</th>\n      <th>autoFE_f_214</th>\n      <th>autoFE_f_215</th>\n      <th>autoFE_f_216</th>\n      <th>autoFE_f_217</th>\n      <th>autoFE_f_218</th>\n      <th>autoFE_f_219</th>\n      <th>autoFE_f_220</th>\n      <th>autoFE_f_221</th>\n      <th>autoFE_f_222</th>\n      <th>autoFE_f_223</th>\n      <th>autoFE_f_224</th>\n      <th>autoFE_f_225</th>\n      <th>autoFE_f_226</th>\n      <th>autoFE_f_227</th>\n      <th>autoFE_f_228</th>\n      <th>autoFE_f_229</th>\n      <th>autoFE_f_230</th>\n      <th>autoFE_f_231</th>\n      <th>autoFE_f_232</th>\n      <th>autoFE_f_233</th>\n      <th>autoFE_f_234</th>\n      <th>autoFE_f_235</th>\n      <th>autoFE_f_236</th>\n      <th>autoFE_f_237</th>\n      <th>autoFE_f_238</th>\n      <th>autoFE_f_239</th>\n      <th>autoFE_f_240</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>9.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>Yes</td>\n      <td>33990.0</td>\n      <td>152.0</td>\n      <td>166</td>\n      <td>5642340.0</td>\n      <td>0.480421</td>\n      <td>186.0</td>\n      <td>5109.0</td>\n      <td>542950.0</td>\n      <td>0.178632</td>\n      <td>166.0</td>\n      <td>305910.0</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>0.653033</td>\n      <td>NaN</td>\n      <td>0.380000</td>\n      <td>14.485116</td>\n      <td>0.529460</td>\n      <td>0.510935</td>\n      <td>0.280615</td>\n      <td>33990.0</td>\n      <td>191.0</td>\n      <td>6910.0</td>\n      <td>0.573447</td>\n      <td>34156.0</td>\n      <td>0.537776</td>\n      <td>33990.0</td>\n      <td>34015.0</td>\n      <td>1359.600000</td>\n      <td>0.509686</td>\n      <td>0.500451</td>\n      <td>0.487574</td>\n      <td>0.0</td>\n      <td>34.986475</td>\n      <td>0.589180</td>\n      <td>223.618421</td>\n      <td>0.499175</td>\n      <td>17590.0</td>\n      <td>6078.0</td>\n      <td>28.0</td>\n      <td>78.856796</td>\n      <td>175.0</td>\n      <td>895.0</td>\n      <td>14079.233812</td>\n      <td>0.643177</td>\n      <td>6846.0</td>\n      <td>16.888889</td>\n      <td>0.625551</td>\n      <td>0.162050</td>\n      <td>0.0</td>\n      <td>26902.168033</td>\n      <td>30648.0</td>\n      <td>0.0</td>\n      <td>0.554006</td>\n      <td>125.991393</td>\n      <td>33965.0</td>\n      <td>0.0</td>\n      <td>0.225569</td>\n      <td>0.499973</td>\n      <td>166.0</td>\n      <td>5166480.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.505359</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>6.640000</td>\n      <td>165.0</td>\n      <td>849750.0</td>\n      <td>0.438852</td>\n      <td>167.0</td>\n      <td>33990.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>34142.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>318.0</td>\n      <td>290594.0</td>\n      <td>1494.0</td>\n      <td>25.0</td>\n      <td>0.496267</td>\n      <td>33999.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>33990.0</td>\n      <td>143.0</td>\n      <td>166.0</td>\n      <td>157.0</td>\n      <td>82460.0</td>\n      <td>0.998033</td>\n      <td>33838.0</td>\n      <td>80.0</td>\n      <td>225.0</td>\n      <td>0.147187</td>\n      <td>204.759036</td>\n      <td>0.0</td>\n      <td>751966.0</td>\n      <td>25.0</td>\n      <td>6063.0</td>\n      <td>33981.0</td>\n      <td>-14.0</td>\n      <td>12986.968804</td>\n      <td>3776.666667</td>\n      <td>2.777778</td>\n      <td>0.129475</td>\n      <td>12176.0</td>\n      <td>152.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4150.0</td>\n      <td>0.752077</td>\n      <td>25.0</td>\n      <td>162.882705</td>\n      <td>34.0</td>\n      <td>25232.0</td>\n      <td>25.408137</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1368.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.149294</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>186375.0</td>\n      <td>9.0</td>\n      <td>152.0</td>\n      <td>21352.0</td>\n      <td>5.023881</td>\n      <td>0.111111</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>33990.0</td>\n      <td>3.000000</td>\n      <td>0.632113</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>444306.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.268198</td>\n      <td>47.998774</td>\n      <td>0.0</td>\n      <td>0.186864</td>\n      <td>0.0</td>\n      <td>12200.0</td>\n      <td>299.0</td>\n      <td>0.181256</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>18.444444</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>161.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>10.0</td>\n      <td>0.173696</td>\n      <td>0.0</td>\n      <td>0.915663</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>61.0</td>\n      <td>2.197225</td>\n      <td>0.741062</td>\n      <td>0.999877</td>\n      <td>0.501025</td>\n      <td>0.0</td>\n      <td>0.164303</td>\n      <td>30984.0</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>32993.0</td>\n      <td>-8.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>25.0</td>\n      <td>0.166778</td>\n      <td>25.0</td>\n      <td>31400.949636</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>0.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>33824.0</td>\n      <td>9.0</td>\n      <td>0.500006</td>\n      <td>0.288312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>166.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>313424.0</td>\n      <td>0.0</td>\n      <td>717515.0</td>\n      <td>148.647138</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Female</td>\n      <td>21</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>49514.0</td>\n      <td>152.0</td>\n      <td>84</td>\n      <td>4159176.0</td>\n      <td>0.190024</td>\n      <td>82.0</td>\n      <td>8441.0</td>\n      <td>542950.0</td>\n      <td>0.477880</td>\n      <td>84.0</td>\n      <td>1386392.0</td>\n      <td>1.0</td>\n      <td>83.0</td>\n      <td>85.0</td>\n      <td>0.785224</td>\n      <td>84.0</td>\n      <td>0.007696</td>\n      <td>13.832753</td>\n      <td>0.923739</td>\n      <td>0.194659</td>\n      <td>0.842734</td>\n      <td>49515.0</td>\n      <td>105.0</td>\n      <td>79439.0</td>\n      <td>0.908039</td>\n      <td>49598.0</td>\n      <td>0.922583</td>\n      <td>49514.0</td>\n      <td>49535.0</td>\n      <td>2357.809524</td>\n      <td>0.194317</td>\n      <td>0.205071</td>\n      <td>0.190794</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.867675</td>\n      <td>325.750000</td>\n      <td>0.266412</td>\n      <td>37064.0</td>\n      <td>191241.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>112.0</td>\n      <td>4709.0</td>\n      <td>15600.277416</td>\n      <td>0.937936</td>\n      <td>141443.0</td>\n      <td>5.428571</td>\n      <td>0.080719</td>\n      <td>0.662050</td>\n      <td>1.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>1.0</td>\n      <td>0.913898</td>\n      <td>86.787510</td>\n      <td>49493.0</td>\n      <td>1.0</td>\n      <td>0.043722</td>\n      <td>0.201356</td>\n      <td>84.0</td>\n      <td>7526128.0</td>\n      <td>49514.0</td>\n      <td>1.0</td>\n      <td>0.195420</td>\n      <td>79.342478</td>\n      <td>131.0</td>\n      <td>1.0</td>\n      <td>4.000000</td>\n      <td>83.0</td>\n      <td>1039794.0</td>\n      <td>0.314534</td>\n      <td>85.0</td>\n      <td>49514.0</td>\n      <td>0.466655</td>\n      <td>173.0</td>\n      <td>49666.0</td>\n      <td>63813.0</td>\n      <td>36329.0</td>\n      <td>236.0</td>\n      <td>488138.0</td>\n      <td>2352.0</td>\n      <td>28.0</td>\n      <td>0.464330</td>\n      <td>49542.0</td>\n      <td>84.0</td>\n      <td>3192.0</td>\n      <td>49513.0</td>\n      <td>124.0</td>\n      <td>84.0</td>\n      <td>56.0</td>\n      <td>65868.0</td>\n      <td>0.997138</td>\n      <td>49362.0</td>\n      <td>85.0</td>\n      <td>588.0</td>\n      <td>0.492940</td>\n      <td>589.452381</td>\n      <td>1.0</td>\n      <td>745026.0</td>\n      <td>21.0</td>\n      <td>152580.0</td>\n      <td>49486.0</td>\n      <td>68.0</td>\n      <td>13812.283547</td>\n      <td>1768.357143</td>\n      <td>0.750000</td>\n      <td>0.730673</td>\n      <td>448404.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>-7.0</td>\n      <td>1.0</td>\n      <td>152.0</td>\n      <td>1764.0</td>\n      <td>0.627876</td>\n      <td>21.0</td>\n      <td>163.305323</td>\n      <td>49.0</td>\n      <td>12768.0</td>\n      <td>26.768385</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>4256.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.490040</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>29.0</td>\n      <td>152.0</td>\n      <td>35481.0</td>\n      <td>5.023881</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>49514.0</td>\n      <td>5.291503</td>\n      <td>0.942969</td>\n      <td>1.0</td>\n      <td>166.127907</td>\n      <td>378441.0</td>\n      <td>1.0</td>\n      <td>49514.0</td>\n      <td>27.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>1.0</td>\n      <td>0.497347</td>\n      <td>152.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.038379</td>\n      <td>0.644572</td>\n      <td>15.008618</td>\n      <td>3.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>180.0</td>\n      <td>20.0</td>\n      <td>0.036905</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>29.0</td>\n      <td>0.037390</td>\n      <td>1.0</td>\n      <td>1.809524</td>\n      <td>166.0</td>\n      <td>84.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.538668</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>1.0</td>\n      <td>0.494718</td>\n      <td>30502.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>30962.0</td>\n      <td>-27.0</td>\n      <td>7.238095</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>151.0</td>\n      <td>20.0</td>\n      <td>0.493777</td>\n      <td>22.0</td>\n      <td>30066.032353</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>65868.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.498506</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>12.379879</td>\n      <td>10.0</td>\n      <td>49430.0</td>\n      <td>28.0</td>\n      <td>0.500008</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>84.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>658494.0</td>\n      <td>154.448108</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Female</td>\n      <td>31</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36913.0</td>\n      <td>152.0</td>\n      <td>149</td>\n      <td>5500037.0</td>\n      <td>0.451416</td>\n      <td>25.0</td>\n      <td>4808.0</td>\n      <td>542950.0</td>\n      <td>0.248792</td>\n      <td>149.0</td>\n      <td>553695.0</td>\n      <td>2.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>0.741447</td>\n      <td>NaN</td>\n      <td>0.645197</td>\n      <td>13.084419</td>\n      <td>0.715309</td>\n      <td>0.432102</td>\n      <td>0.260368</td>\n      <td>36913.0</td>\n      <td>180.0</td>\n      <td>34800.0</td>\n      <td>0.670870</td>\n      <td>37062.0</td>\n      <td>0.634359</td>\n      <td>36913.0</td>\n      <td>36944.0</td>\n      <td>1190.741935</td>\n      <td>0.446585</td>\n      <td>0.417740</td>\n      <td>0.430972</td>\n      <td>2.0</td>\n      <td>32.053954</td>\n      <td>0.553907</td>\n      <td>242.848684</td>\n      <td>0.546335</td>\n      <td>11290.0</td>\n      <td>26554.0</td>\n      <td>26.0</td>\n      <td>80.427887</td>\n      <td>164.0</td>\n      <td>855.0</td>\n      <td>13766.828032</td>\n      <td>0.746904</td>\n      <td>27245.0</td>\n      <td>10.133333</td>\n      <td>0.946592</td>\n      <td>0.162050</td>\n      <td>2.0</td>\n      <td>28959.286744</td>\n      <td>31574.5</td>\n      <td>1.0</td>\n      <td>0.690318</td>\n      <td>132.888517</td>\n      <td>36882.0</td>\n      <td>2.0</td>\n      <td>0.522780</td>\n      <td>0.439885</td>\n      <td>149.0</td>\n      <td>5610776.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.431017</td>\n      <td>79.379644</td>\n      <td>121.0</td>\n      <td>2.0</td>\n      <td>4.806452</td>\n      <td>148.0</td>\n      <td>1144303.0</td>\n      <td>0.479282</td>\n      <td>150.0</td>\n      <td>36913.0</td>\n      <td>0.466655</td>\n      <td>183.0</td>\n      <td>37065.0</td>\n      <td>15033.0</td>\n      <td>9979.0</td>\n      <td>301.0</td>\n      <td>346200.0</td>\n      <td>2235.0</td>\n      <td>31.0</td>\n      <td>0.499575</td>\n      <td>36928.0</td>\n      <td>149.0</td>\n      <td>4712.0</td>\n      <td>36913.0</td>\n      <td>137.0</td>\n      <td>149.0</td>\n      <td>134.0</td>\n      <td>21118.0</td>\n      <td>0.998624</td>\n      <td>36761.0</td>\n      <td>81.0</td>\n      <td>465.0</td>\n      <td>0.286846</td>\n      <td>247.738255</td>\n      <td>2.0</td>\n      <td>745026.0</td>\n      <td>31.0</td>\n      <td>29458.0</td>\n      <td>36898.0</td>\n      <td>3.0</td>\n      <td>17397.458965</td>\n      <td>2460.866667</td>\n      <td>2.066667</td>\n      <td>0.254759</td>\n      <td>52250.0</td>\n      <td>152.0</td>\n      <td>2.0</td>\n      <td>16.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>4619.0</td>\n      <td>0.627876</td>\n      <td>31.0</td>\n      <td>166.741256</td>\n      <td>46.0</td>\n      <td>22648.0</td>\n      <td>26.660858</td>\n      <td>1.000000</td>\n      <td>2.0</td>\n      <td>2280.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.218216</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>15.0</td>\n      <td>152.0</td>\n      <td>10759.0</td>\n      <td>5.023881</td>\n      <td>0.066667</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>36913.0</td>\n      <td>3.872983</td>\n      <td>0.766029</td>\n      <td>0.0</td>\n      <td>163.824747</td>\n      <td>378441.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>687365.0</td>\n      <td>0.268198</td>\n      <td>43.929969</td>\n      <td>0.0</td>\n      <td>0.313013</td>\n      <td>0.0</td>\n      <td>52322.0</td>\n      <td>299.0</td>\n      <td>0.267417</td>\n      <td>0.644572</td>\n      <td>13.183065</td>\n      <td>9.933333</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>31.0</td>\n      <td>167.0</td>\n      <td>30.0</td>\n      <td>0.433391</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>31.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>16.0</td>\n      <td>0.613586</td>\n      <td>0.0</td>\n      <td>1.020134</td>\n      <td>171.0</td>\n      <td>149.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>2.708050</td>\n      <td>0.490529</td>\n      <td>1.000000</td>\n      <td>0.500698</td>\n      <td>1.0</td>\n      <td>0.250191</td>\n      <td>29007.5</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-14.0</td>\n      <td>4.903226</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>31.0</td>\n      <td>0.254576</td>\n      <td>31.0</td>\n      <td>25857.063169</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>21118.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>164.729211</td>\n      <td>0.499922</td>\n      <td>32.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>2.0</td>\n      <td>37.619003</td>\n      <td>10.0</td>\n      <td>36764.0</td>\n      <td>15.0</td>\n      <td>0.500024</td>\n      <td>0.244792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>149.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>309870.0</td>\n      <td>0.0</td>\n      <td>87525.0</td>\n      <td>135.694052</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Male</td>\n      <td>25</td>\n      <td>1</td>\n      <td>10.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36274.0</td>\n      <td>152.0</td>\n      <td>256</td>\n      <td>9286144.0</td>\n      <td>0.829111</td>\n      <td>106.0</td>\n      <td>11913.0</td>\n      <td>542950.0</td>\n      <td>0.195258</td>\n      <td>256.0</td>\n      <td>362740.0</td>\n      <td>1.0</td>\n      <td>255.0</td>\n      <td>257.0</td>\n      <td>0.921735</td>\n      <td>256.0</td>\n      <td>0.466462</td>\n      <td>11.027021</td>\n      <td>0.700654</td>\n      <td>0.835286</td>\n      <td>0.672440</td>\n      <td>36275.0</td>\n      <td>281.0</td>\n      <td>12342.0</td>\n      <td>0.655816</td>\n      <td>36530.0</td>\n      <td>0.697934</td>\n      <td>36274.0</td>\n      <td>36299.0</td>\n      <td>1450.960000</td>\n      <td>0.835485</td>\n      <td>0.824923</td>\n      <td>0.836364</td>\n      <td>3.0</td>\n      <td>30.023605</td>\n      <td>0.539638</td>\n      <td>238.644737</td>\n      <td>0.499175</td>\n      <td>64870.0</td>\n      <td>7566.0</td>\n      <td>25.0</td>\n      <td>82.260215</td>\n      <td>266.0</td>\n      <td>1847.0</td>\n      <td>11346.682460</td>\n      <td>0.731384</td>\n      <td>10686.0</td>\n      <td>15.200000</td>\n      <td>0.625551</td>\n      <td>0.662050</td>\n      <td>3.0</td>\n      <td>24892.287308</td>\n      <td>28015.0</td>\n      <td>0.0</td>\n      <td>0.638378</td>\n      <td>139.645371</td>\n      <td>36249.0</td>\n      <td>3.0</td>\n      <td>0.225569</td>\n      <td>0.842750</td>\n      <td>256.0</td>\n      <td>5513648.0</td>\n      <td>36274.0</td>\n      <td>3.0</td>\n      <td>0.840829</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>10.240000</td>\n      <td>255.0</td>\n      <td>906850.0</td>\n      <td>0.655181</td>\n      <td>257.0</td>\n      <td>36274.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>36426.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>408.0</td>\n      <td>290594.0</td>\n      <td>2560.0</td>\n      <td>25.0</td>\n      <td>0.475324</td>\n      <td>36284.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>36273.0</td>\n      <td>142.0</td>\n      <td>256.0</td>\n      <td>246.0</td>\n      <td>82460.0</td>\n      <td>0.999387</td>\n      <td>36122.0</td>\n      <td>80.0</td>\n      <td>250.0</td>\n      <td>0.193205</td>\n      <td>141.695312</td>\n      <td>3.0</td>\n      <td>745026.0</td>\n      <td>25.0</td>\n      <td>11198.0</td>\n      <td>36264.0</td>\n      <td>-104.0</td>\n      <td>12986.968804</td>\n      <td>3627.400000</td>\n      <td>2.500000</td>\n      <td>0.629475</td>\n      <td>16300.0</td>\n      <td>152.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>6400.0</td>\n      <td>0.627876</td>\n      <td>25.0</td>\n      <td>164.072226</td>\n      <td>35.0</td>\n      <td>38912.0</td>\n      <td>25.408137</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>1520.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.193213</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>11.0</td>\n      <td>152.0</td>\n      <td>61108.0</td>\n      <td>5.023881</td>\n      <td>0.100000</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>36274.0</td>\n      <td>3.162278</td>\n      <td>0.719155</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>367578.0</td>\n      <td>3.0</td>\n      <td>36274.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>35.676699</td>\n      <td>0.0</td>\n      <td>0.201518</td>\n      <td>152.0</td>\n      <td>16310.0</td>\n      <td>299.0</td>\n      <td>0.393759</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>25.600000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>162.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>256.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>11.0</td>\n      <td>0.386855</td>\n      <td>1.0</td>\n      <td>0.593750</td>\n      <td>169.0</td>\n      <td>256.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>61.0</td>\n      <td>2.302585</td>\n      <td>0.741062</td>\n      <td>0.999955</td>\n      <td>0.500337</td>\n      <td>1.0</td>\n      <td>0.173814</td>\n      <td>30984.0</td>\n      <td>0.500683</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-9.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>151.0</td>\n      <td>24.0</td>\n      <td>0.175181</td>\n      <td>26.0</td>\n      <td>31400.949636</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>25.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>36018.0</td>\n      <td>10.0</td>\n      <td>0.500006</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>256.0</td>\n      <td>299.0</td>\n      <td>2.0</td>\n      <td>85517.0</td>\n      <td>10.0</td>\n      <td>658494.0</td>\n      <td>148.647138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Male</td>\n      <td>66</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>1-2 Year</td>\n      <td>Yes</td>\n      <td>44214.0</td>\n      <td>26.0</td>\n      <td>223</td>\n      <td>9859722.0</td>\n      <td>0.708394</td>\n      <td>93.0</td>\n      <td>4884.0</td>\n      <td>317263.0</td>\n      <td>0.485734</td>\n      <td>223.0</td>\n      <td>1237992.0</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>224.0</td>\n      <td>0.609766</td>\n      <td>223.0</td>\n      <td>0.906301</td>\n      <td>13.832753</td>\n      <td>0.790905</td>\n      <td>0.725049</td>\n      <td>0.842734</td>\n      <td>44215.0</td>\n      <td>289.0</td>\n      <td>332339.0</td>\n      <td>0.821479</td>\n      <td>44437.0</td>\n      <td>0.849935</td>\n      <td>44214.0</td>\n      <td>44280.0</td>\n      <td>669.909091</td>\n      <td>0.701396</td>\n      <td>0.717548</td>\n      <td>0.718992</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.223135</td>\n      <td>1700.538462</td>\n      <td>0.303853</td>\n      <td>6696.0</td>\n      <td>258450.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>251.0</td>\n      <td>4950.0</td>\n      <td>15600.277416</td>\n      <td>0.787907</td>\n      <td>141443.0</td>\n      <td>0.928571</td>\n      <td>0.903795</td>\n      <td>0.839407</td>\n      <td>4.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>2.0</td>\n      <td>0.810001</td>\n      <td>86.787510</td>\n      <td>44148.0</td>\n      <td>1.0</td>\n      <td>0.938345</td>\n      <td>0.713352</td>\n      <td>223.0</td>\n      <td>1149564.0</td>\n      <td>44214.0</td>\n      <td>4.0</td>\n      <td>0.714188</td>\n      <td>79.709674</td>\n      <td>-40.0</td>\n      <td>3.0</td>\n      <td>3.378788</td>\n      <td>222.0</td>\n      <td>2918124.0</td>\n      <td>0.314534</td>\n      <td>224.0</td>\n      <td>44214.0</td>\n      <td>0.235340</td>\n      <td>92.0</td>\n      <td>44240.0</td>\n      <td>9586.0</td>\n      <td>6662.0</td>\n      <td>249.0</td>\n      <td>308615.0</td>\n      <td>6244.0</td>\n      <td>66.0</td>\n      <td>0.464330</td>\n      <td>44242.0</td>\n      <td>26.0</td>\n      <td>1716.0</td>\n      <td>44213.0</td>\n      <td>-2.0</td>\n      <td>223.0</td>\n      <td>195.0</td>\n      <td>10791.0</td>\n      <td>0.997138</td>\n      <td>44188.0</td>\n      <td>85.0</td>\n      <td>1848.0</td>\n      <td>0.496518</td>\n      <td>198.269058</td>\n      <td>4.0</td>\n      <td>751966.0</td>\n      <td>66.0</td>\n      <td>297111.0</td>\n      <td>44186.0</td>\n      <td>-197.0</td>\n      <td>19314.044119</td>\n      <td>1579.071429</td>\n      <td>2.357143</td>\n      <td>0.813899</td>\n      <td>448404.0</td>\n      <td>26.0</td>\n      <td>4.0</td>\n      <td>38.0</td>\n      <td>3.0</td>\n      <td>26.0</td>\n      <td>14718.0</td>\n      <td>0.187160</td>\n      <td>26.0</td>\n      <td>163.305323</td>\n      <td>94.0</td>\n      <td>5798.0</td>\n      <td>27.238165</td>\n      <td>0.990000</td>\n      <td>3.0</td>\n      <td>728.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.490040</td>\n      <td>250795.0</td>\n      <td>0.099503</td>\n      <td>505403.0</td>\n      <td>29.0</td>\n      <td>28.0</td>\n      <td>4058.0</td>\n      <td>3.258097</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>44214.0</td>\n      <td>5.291503</td>\n      <td>0.709128</td>\n      <td>0.0</td>\n      <td>165.347064</td>\n      <td>444306.0</td>\n      <td>4.0</td>\n      <td>44214.0</td>\n      <td>27.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>0.0</td>\n      <td>0.492609</td>\n      <td>26.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.952428</td>\n      <td>0.162168</td>\n      <td>11.906707</td>\n      <td>7.964286</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>66.0</td>\n      <td>54.0</td>\n      <td>65.0</td>\n      <td>0.944127</td>\n      <td>0.0</td>\n      <td>223.0</td>\n      <td>27.0</td>\n      <td>0.143149</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>0.095901</td>\n      <td>29.0</td>\n      <td>0.932316</td>\n      <td>2.0</td>\n      <td>0.116592</td>\n      <td>166.0</td>\n      <td>223.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.372294</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>2.0</td>\n      <td>0.494718</td>\n      <td>35500.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>32993.0</td>\n      <td>-27.0</td>\n      <td>0.393939</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>25.0</td>\n      <td>65.0</td>\n      <td>0.495553</td>\n      <td>67.0</td>\n      <td>32524.870734</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>27.0</td>\n      <td>479626.0</td>\n      <td>1.0</td>\n      <td>10900.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.483438</td>\n      <td>67.0</td>\n      <td>66.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>53.125551</td>\n      <td>10.0</td>\n      <td>43991.0</td>\n      <td>28.0</td>\n      <td>0.505046</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>223.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>36466.0</td>\n      <td>71.982385</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>Female</td>\n      <td>57</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>1-2 Year</td>\n      <td>Yes</td>\n      <td>50895.0</td>\n      <td>26.0</td>\n      <td>212</td>\n      <td>10789740.0</td>\n      <td>0.683872</td>\n      <td>41.0</td>\n      <td>5298.0</td>\n      <td>317263.0</td>\n      <td>0.509225</td>\n      <td>212.0</td>\n      <td>1425060.0</td>\n      <td>0.0</td>\n      <td>212.0</td>\n      <td>212.0</td>\n      <td>0.817174</td>\n      <td>NaN</td>\n      <td>0.791244</td>\n      <td>13.832753</td>\n      <td>0.908685</td>\n      <td>0.681131</td>\n      <td>0.342734</td>\n      <td>50895.0</td>\n      <td>269.0</td>\n      <td>332339.0</td>\n      <td>0.922072</td>\n      <td>51107.0</td>\n      <td>0.911381</td>\n      <td>50895.0</td>\n      <td>50952.0</td>\n      <td>892.894737</td>\n      <td>0.679725</td>\n      <td>0.673129</td>\n      <td>0.673843</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.223135</td>\n      <td>1957.500000</td>\n      <td>0.280891</td>\n      <td>10586.0</td>\n      <td>191241.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>240.0</td>\n      <td>6800.0</td>\n      <td>15600.277416</td>\n      <td>0.908179</td>\n      <td>308248.0</td>\n      <td>0.928571</td>\n      <td>0.771002</td>\n      <td>0.339407</td>\n      <td>4.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>3.0</td>\n      <td>0.926544</td>\n      <td>86.787510</td>\n      <td>50838.0</td>\n      <td>4.0</td>\n      <td>0.889559</td>\n      <td>0.668030</td>\n      <td>212.0</td>\n      <td>1323270.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>0.666054</td>\n      <td>79.898332</td>\n      <td>-31.0</td>\n      <td>4.0</td>\n      <td>3.719298</td>\n      <td>211.0</td>\n      <td>2901015.0</td>\n      <td>0.314534</td>\n      <td>213.0</td>\n      <td>50895.0</td>\n      <td>0.235340</td>\n      <td>83.0</td>\n      <td>50921.0</td>\n      <td>14132.0</td>\n      <td>4912.0</td>\n      <td>238.0</td>\n      <td>336395.0</td>\n      <td>5936.0</td>\n      <td>57.0</td>\n      <td>0.464330</td>\n      <td>50923.0</td>\n      <td>26.0</td>\n      <td>1482.0</td>\n      <td>50895.0</td>\n      <td>-2.0</td>\n      <td>212.0</td>\n      <td>184.0</td>\n      <td>15885.0</td>\n      <td>0.997138</td>\n      <td>50869.0</td>\n      <td>85.0</td>\n      <td>1596.0</td>\n      <td>0.496518</td>\n      <td>240.070755</td>\n      <td>1.0</td>\n      <td>751966.0</td>\n      <td>57.0</td>\n      <td>297111.0</td>\n      <td>50867.0</td>\n      <td>-186.0</td>\n      <td>18669.048339</td>\n      <td>1817.678571</td>\n      <td>2.035714</td>\n      <td>0.341261</td>\n      <td>448404.0</td>\n      <td>26.0</td>\n      <td>5.0</td>\n      <td>29.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>12084.0</td>\n      <td>0.187160</td>\n      <td>26.0</td>\n      <td>163.305323</td>\n      <td>85.0</td>\n      <td>5512.0</td>\n      <td>26.352306</td>\n      <td>0.996862</td>\n      <td>93.0</td>\n      <td>728.0</td>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>0.498794</td>\n      <td>530041.0</td>\n      <td>0.055929</td>\n      <td>505403.0</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>10875.0</td>\n      <td>3.258097</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>50895.0</td>\n      <td>5.291503</td>\n      <td>0.882523</td>\n      <td>0.0</td>\n      <td>160.584123</td>\n      <td>309675.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>28.0</td>\n      <td>687365.0</td>\n      <td>0.268198</td>\n      <td>54.847088</td>\n      <td>0.0</td>\n      <td>0.492609</td>\n      <td>0.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.837216</td>\n      <td>0.121649</td>\n      <td>12.302707</td>\n      <td>7.571429</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>57.0</td>\n      <td>54.0</td>\n      <td>56.0</td>\n      <td>0.865969</td>\n      <td>0.0</td>\n      <td>212.0</td>\n      <td>27.0</td>\n      <td>0.143149</td>\n      <td>57.0</td>\n      <td>1.0</td>\n      <td>0.184740</td>\n      <td>29.0</td>\n      <td>0.830667</td>\n      <td>3.0</td>\n      <td>0.122642</td>\n      <td>166.0</td>\n      <td>212.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.317540</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>2.0</td>\n      <td>0.494718</td>\n      <td>34807.0</td>\n      <td>0.501280</td>\n      <td>122.0</td>\n      <td>32993.0</td>\n      <td>-27.0</td>\n      <td>0.456140</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>26.0</td>\n      <td>57.0</td>\n      <td>0.493777</td>\n      <td>57.0</td>\n      <td>31709.833448</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>26.0</td>\n      <td>301210.0</td>\n      <td>1.0</td>\n      <td>15935.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>164.729211</td>\n      <td>0.465534</td>\n      <td>58.0</td>\n      <td>NaN</td>\n      <td>26.0</td>\n      <td>30.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>52.0</td>\n      <td>2.0</td>\n      <td>53.878391</td>\n      <td>10.0</td>\n      <td>50683.0</td>\n      <td>28.0</td>\n      <td>0.501600</td>\n      <td>0.244792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>212.0</td>\n      <td>299.0</td>\n      <td>3.0</td>\n      <td>540165.0</td>\n      <td>0.0</td>\n      <td>717515.0</td>\n      <td>75.078193</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>Male</td>\n      <td>23</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>&lt; 1 Year</td>\n      <td>Yes</td>\n      <td>39443.0</td>\n      <td>152.0</td>\n      <td>92</td>\n      <td>3628756.0</td>\n      <td>0.224518</td>\n      <td>363.0</td>\n      <td>7282.0</td>\n      <td>542950.0</td>\n      <td>0.331764</td>\n      <td>92.0</td>\n      <td>591645.0</td>\n      <td>0.0</td>\n      <td>92.0</td>\n      <td>92.0</td>\n      <td>0.845811</td>\n      <td>NaN</td>\n      <td>0.265844</td>\n      <td>13.084419</td>\n      <td>0.708975</td>\n      <td>0.230392</td>\n      <td>0.260368</td>\n      <td>39443.0</td>\n      <td>115.0</td>\n      <td>34800.0</td>\n      <td>0.749903</td>\n      <td>39535.0</td>\n      <td>0.716186</td>\n      <td>39443.0</td>\n      <td>39466.0</td>\n      <td>1714.913043</td>\n      <td>0.229877</td>\n      <td>0.219955</td>\n      <td>0.215363</td>\n      <td>2.0</td>\n      <td>32.053954</td>\n      <td>0.553907</td>\n      <td>259.493421</td>\n      <td>0.525428</td>\n      <td>26487.0</td>\n      <td>25768.0</td>\n      <td>26.0</td>\n      <td>80.427887</td>\n      <td>107.0</td>\n      <td>5516.0</td>\n      <td>13766.828032</td>\n      <td>0.821843</td>\n      <td>27245.0</td>\n      <td>10.133333</td>\n      <td>0.330809</td>\n      <td>0.162050</td>\n      <td>45.0</td>\n      <td>28959.286744</td>\n      <td>31574.5</td>\n      <td>0.0</td>\n      <td>0.735255</td>\n      <td>132.888517</td>\n      <td>39420.0</td>\n      <td>2.0</td>\n      <td>0.120996</td>\n      <td>0.225462</td>\n      <td>92.0</td>\n      <td>5995336.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.230227</td>\n      <td>79.875385</td>\n      <td>129.0</td>\n      <td>10.0</td>\n      <td>4.000000</td>\n      <td>91.0</td>\n      <td>907189.0</td>\n      <td>0.479282</td>\n      <td>93.0</td>\n      <td>39443.0</td>\n      <td>0.466655</td>\n      <td>175.0</td>\n      <td>39595.0</td>\n      <td>96175.0</td>\n      <td>42027.0</td>\n      <td>244.0</td>\n      <td>294467.0</td>\n      <td>1380.0</td>\n      <td>23.0</td>\n      <td>0.499575</td>\n      <td>39458.0</td>\n      <td>92.0</td>\n      <td>3496.0</td>\n      <td>39443.0</td>\n      <td>137.0</td>\n      <td>92.0</td>\n      <td>77.0</td>\n      <td>97589.0</td>\n      <td>0.998624</td>\n      <td>39291.0</td>\n      <td>81.0</td>\n      <td>345.0</td>\n      <td>0.213765</td>\n      <td>428.728261</td>\n      <td>49.0</td>\n      <td>751966.0</td>\n      <td>23.0</td>\n      <td>22864.0</td>\n      <td>39428.0</td>\n      <td>60.0</td>\n      <td>12873.909148</td>\n      <td>2629.533333</td>\n      <td>1.533333</td>\n      <td>0.154331</td>\n      <td>52250.0</td>\n      <td>152.0</td>\n      <td>50.0</td>\n      <td>8.0</td>\n      <td>10.0</td>\n      <td>NaN</td>\n      <td>2116.0</td>\n      <td>0.752077</td>\n      <td>23.0</td>\n      <td>166.741256</td>\n      <td>38.0</td>\n      <td>13984.0</td>\n      <td>25.517681</td>\n      <td>1.000000</td>\n      <td>38.0</td>\n      <td>2280.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>55.0</td>\n      <td>0.218216</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>186375.0</td>\n      <td>15.0</td>\n      <td>152.0</td>\n      <td>30121.0</td>\n      <td>5.023881</td>\n      <td>0.066667</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>39443.0</td>\n      <td>3.872983</td>\n      <td>0.818914</td>\n      <td>1.0</td>\n      <td>163.074804</td>\n      <td>444306.0</td>\n      <td>52.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>809627.0</td>\n      <td>0.268198</td>\n      <td>43.929969</td>\n      <td>0.0</td>\n      <td>0.313013</td>\n      <td>0.0</td>\n      <td>52322.0</td>\n      <td>299.0</td>\n      <td>0.111895</td>\n      <td>0.729068</td>\n      <td>14.309252</td>\n      <td>6.133333</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>23.0</td>\n      <td>167.0</td>\n      <td>22.0</td>\n      <td>0.149064</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>23.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>16.0</td>\n      <td>0.110833</td>\n      <td>0.0</td>\n      <td>1.652174</td>\n      <td>171.0</td>\n      <td>92.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>2.708050</td>\n      <td>0.691348</td>\n      <td>1.000000</td>\n      <td>0.500698</td>\n      <td>0.0</td>\n      <td>0.250191</td>\n      <td>30897.0</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>32993.0</td>\n      <td>-14.0</td>\n      <td>6.608696</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>23.0</td>\n      <td>0.246328</td>\n      <td>23.0</td>\n      <td>31093.172755</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>97589.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>163.172999</td>\n      <td>0.461939</td>\n      <td>24.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>0.0</td>\n      <td>28.867072</td>\n      <td>10.0</td>\n      <td>39351.0</td>\n      <td>15.0</td>\n      <td>0.500005</td>\n      <td>0.288312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>92.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>309870.0</td>\n      <td>0.0</td>\n      <td>717515.0</td>\n      <td>145.131931</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>Female</td>\n      <td>26</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>38892.0</td>\n      <td>152.0</td>\n      <td>218</td>\n      <td>8478456.0</td>\n      <td>0.685769</td>\n      <td>38.0</td>\n      <td>5395.0</td>\n      <td>542950.0</td>\n      <td>0.122214</td>\n      <td>218.0</td>\n      <td>311136.0</td>\n      <td>1.0</td>\n      <td>217.0</td>\n      <td>219.0</td>\n      <td>0.556250</td>\n      <td>218.0</td>\n      <td>0.388011</td>\n      <td>15.261335</td>\n      <td>0.772004</td>\n      <td>0.681054</td>\n      <td>0.735225</td>\n      <td>38893.0</td>\n      <td>244.0</td>\n      <td>64583.0</td>\n      <td>0.730097</td>\n      <td>39110.0</td>\n      <td>0.769719</td>\n      <td>38892.0</td>\n      <td>38918.0</td>\n      <td>1495.846154</td>\n      <td>0.680803</td>\n      <td>0.681213</td>\n      <td>0.685214</td>\n      <td>6.0</td>\n      <td>37.423027</td>\n      <td>0.664285</td>\n      <td>255.868421</td>\n      <td>0.493532</td>\n      <td>41903.0</td>\n      <td>58736.0</td>\n      <td>33.0</td>\n      <td>80.164582</td>\n      <td>226.0</td>\n      <td>4234.0</td>\n      <td>15938.073188</td>\n      <td>0.803288</td>\n      <td>70528.0</td>\n      <td>19.000000</td>\n      <td>0.726146</td>\n      <td>0.662050</td>\n      <td>14.0</td>\n      <td>36023.673284</td>\n      <td>37502.0</td>\n      <td>1.0</td>\n      <td>0.747863</td>\n      <td>113.786114</td>\n      <td>38866.0</td>\n      <td>14.0</td>\n      <td>0.398904</td>\n      <td>0.693955</td>\n      <td>218.0</td>\n      <td>5911584.0</td>\n      <td>38892.0</td>\n      <td>20.0</td>\n      <td>0.692340</td>\n      <td>79.378535</td>\n      <td>126.0</td>\n      <td>6.0</td>\n      <td>8.384615</td>\n      <td>217.0</td>\n      <td>1011192.0</td>\n      <td>0.529557</td>\n      <td>219.0</td>\n      <td>38892.0</td>\n      <td>0.466655</td>\n      <td>178.0</td>\n      <td>39044.0</td>\n      <td>50923.0</td>\n      <td>29191.0</td>\n      <td>370.0</td>\n      <td>540165.0</td>\n      <td>1744.0</td>\n      <td>26.0</td>\n      <td>0.499127</td>\n      <td>38900.0</td>\n      <td>152.0</td>\n      <td>3952.0</td>\n      <td>38891.0</td>\n      <td>144.0</td>\n      <td>218.0</td>\n      <td>210.0</td>\n      <td>52182.0</td>\n      <td>0.997808</td>\n      <td>38740.0</td>\n      <td>83.0</td>\n      <td>208.0</td>\n      <td>0.127158</td>\n      <td>178.403670</td>\n      <td>14.0</td>\n      <td>745026.0</td>\n      <td>26.0</td>\n      <td>75072.0</td>\n      <td>38884.0</td>\n      <td>-66.0</td>\n      <td>14016.918475</td>\n      <td>4861.500000</td>\n      <td>3.250000</td>\n      <td>0.616456</td>\n      <td>132891.0</td>\n      <td>152.0</td>\n      <td>7.0</td>\n      <td>18.0</td>\n      <td>6.0</td>\n      <td>152.0</td>\n      <td>5668.0</td>\n      <td>0.627876</td>\n      <td>26.0</td>\n      <td>165.422133</td>\n      <td>34.0</td>\n      <td>33136.0</td>\n      <td>25.994481</td>\n      <td>0.999981</td>\n      <td>6.0</td>\n      <td>1216.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>228.0</td>\n      <td>0.127079</td>\n      <td>444005.0</td>\n      <td>0.004378</td>\n      <td>470533.0</td>\n      <td>9.0</td>\n      <td>152.0</td>\n      <td>40030.0</td>\n      <td>5.023881</td>\n      <td>0.125000</td>\n      <td>8.0</td>\n      <td>8.0</td>\n      <td>38892.0</td>\n      <td>2.828427</td>\n      <td>0.807006</td>\n      <td>1.0</td>\n      <td>165.493302</td>\n      <td>378441.0</td>\n      <td>7.0</td>\n      <td>38892.0</td>\n      <td>7.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>55.230125</td>\n      <td>0.0</td>\n      <td>0.132448</td>\n      <td>152.0</td>\n      <td>133183.0</td>\n      <td>299.0</td>\n      <td>0.466524</td>\n      <td>0.644572</td>\n      <td>13.567714</td>\n      <td>27.250000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>26.0</td>\n      <td>160.0</td>\n      <td>25.0</td>\n      <td>0.325241</td>\n      <td>0.0</td>\n      <td>218.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>9.0</td>\n      <td>0.458417</td>\n      <td>1.0</td>\n      <td>0.697248</td>\n      <td>168.0</td>\n      <td>218.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>64.0</td>\n      <td>2.079442</td>\n      <td>0.767108</td>\n      <td>0.999665</td>\n      <td>0.501100</td>\n      <td>1.0</td>\n      <td>0.115850</td>\n      <td>30288.0</td>\n      <td>0.500683</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-7.0</td>\n      <td>5.846154</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>151.0</td>\n      <td>25.0</td>\n      <td>0.114239</td>\n      <td>27.0</td>\n      <td>30028.138877</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>52183.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.422678</td>\n      <td>27.0</td>\n      <td>26.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>8.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>1.0</td>\n      <td>21.641112</td>\n      <td>10.0</td>\n      <td>38674.0</td>\n      <td>8.0</td>\n      <td>0.500019</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>218.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>336395.0</td>\n      <td>8.0</td>\n      <td>658494.0</td>\n      <td>148.248644</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>Female</td>\n      <td>20</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>41203.0</td>\n      <td>160.0</td>\n      <td>158</td>\n      <td>6510074.0</td>\n      <td>0.475799</td>\n      <td>13.0</td>\n      <td>10440.0</td>\n      <td>83727.0</td>\n      <td>0.339533</td>\n      <td>158.0</td>\n      <td>618045.0</td>\n      <td>1.0</td>\n      <td>157.0</td>\n      <td>159.0</td>\n      <td>0.887246</td>\n      <td>158.0</td>\n      <td>0.016207</td>\n      <td>13.084419</td>\n      <td>0.822324</td>\n      <td>0.458426</td>\n      <td>0.760368</td>\n      <td>41204.0</td>\n      <td>178.0</td>\n      <td>34800.0</td>\n      <td>0.784786</td>\n      <td>41361.0</td>\n      <td>0.820123</td>\n      <td>41203.0</td>\n      <td>41223.0</td>\n      <td>2060.150000</td>\n      <td>0.458278</td>\n      <td>0.448081</td>\n      <td>0.455761</td>\n      <td>2.0</td>\n      <td>32.053954</td>\n      <td>0.937139</td>\n      <td>257.518750</td>\n      <td>0.546542</td>\n      <td>9361.0</td>\n      <td>26554.0</td>\n      <td>26.0</td>\n      <td>80.427887</td>\n      <td>173.0</td>\n      <td>1695.0</td>\n      <td>13766.828032</td>\n      <td>0.851652</td>\n      <td>25077.0</td>\n      <td>10.666667</td>\n      <td>0.016075</td>\n      <td>0.662050</td>\n      <td>2.0</td>\n      <td>28959.286744</td>\n      <td>31574.5</td>\n      <td>1.0</td>\n      <td>0.800013</td>\n      <td>132.888517</td>\n      <td>41183.0</td>\n      <td>33.0</td>\n      <td>0.008663</td>\n      <td>0.467676</td>\n      <td>158.0</td>\n      <td>6592480.0</td>\n      <td>41203.0</td>\n      <td>2.0</td>\n      <td>0.460549</td>\n      <td>81.204575</td>\n      <td>140.0</td>\n      <td>37.0</td>\n      <td>7.900000</td>\n      <td>157.0</td>\n      <td>824060.0</td>\n      <td>0.479282</td>\n      <td>159.0</td>\n      <td>41203.0</td>\n      <td>0.935515</td>\n      <td>180.0</td>\n      <td>41363.0</td>\n      <td>21118.0</td>\n      <td>11921.0</td>\n      <td>318.0</td>\n      <td>117057.0</td>\n      <td>2370.0</td>\n      <td>20.0</td>\n      <td>0.499575</td>\n      <td>41218.0</td>\n      <td>158.0</td>\n      <td>3200.0</td>\n      <td>41202.0</td>\n      <td>145.0</td>\n      <td>158.0</td>\n      <td>143.0</td>\n      <td>22312.0</td>\n      <td>0.998624</td>\n      <td>41043.0</td>\n      <td>81.0</td>\n      <td>300.0</td>\n      <td>0.286846</td>\n      <td>260.778481</td>\n      <td>2.0</td>\n      <td>745026.0</td>\n      <td>20.0</td>\n      <td>29458.0</td>\n      <td>41188.0</td>\n      <td>2.0</td>\n      <td>15163.198069</td>\n      <td>2746.866667</td>\n      <td>1.333333</td>\n      <td>0.799892</td>\n      <td>52250.0</td>\n      <td>160.0</td>\n      <td>46.0</td>\n      <td>5.0</td>\n      <td>35.0</td>\n      <td>160.0</td>\n      <td>3160.0</td>\n      <td>0.967104</td>\n      <td>20.0</td>\n      <td>166.741256</td>\n      <td>35.0</td>\n      <td>25280.0</td>\n      <td>26.359745</td>\n      <td>0.999955</td>\n      <td>46.0</td>\n      <td>2400.0</td>\n      <td>160.0</td>\n      <td>1.0</td>\n      <td>689.0</td>\n      <td>0.287058</td>\n      <td>444005.0</td>\n      <td>0.006695</td>\n      <td>470533.0</td>\n      <td>16.0</td>\n      <td>160.0</td>\n      <td>8931.0</td>\n      <td>5.075174</td>\n      <td>0.066667</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>41203.0</td>\n      <td>3.872983</td>\n      <td>0.890423</td>\n      <td>0.0</td>\n      <td>161.894635</td>\n      <td>378441.0</td>\n      <td>48.0</td>\n      <td>41203.0</td>\n      <td>14.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>43.929969</td>\n      <td>1.0</td>\n      <td>0.313013</td>\n      <td>160.0</td>\n      <td>52322.0</td>\n      <td>299.0</td>\n      <td>0.006426</td>\n      <td>0.962357</td>\n      <td>15.621437</td>\n      <td>10.533333</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>20.0</td>\n      <td>175.0</td>\n      <td>19.0</td>\n      <td>0.007453</td>\n      <td>0.0</td>\n      <td>160.0</td>\n      <td>161.0</td>\n      <td>0.964688</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>0.965784</td>\n      <td>16.0</td>\n      <td>0.006275</td>\n      <td>1.0</td>\n      <td>1.012658</td>\n      <td>171.0</td>\n      <td>158.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>62.0</td>\n      <td>2.708050</td>\n      <td>0.400260</td>\n      <td>1.000000</td>\n      <td>0.500698</td>\n      <td>1.0</td>\n      <td>0.250191</td>\n      <td>29399.0</td>\n      <td>0.500683</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-14.0</td>\n      <td>8.000000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>159.0</td>\n      <td>19.0</td>\n      <td>0.254576</td>\n      <td>21.0</td>\n      <td>26111.556402</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>161.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>22313.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.489962</td>\n      <td>21.0</td>\n      <td>20.0</td>\n      <td>160.0</td>\n      <td>160.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>159.0</td>\n      <td>50.0</td>\n      <td>1.0</td>\n      <td>14.275307</td>\n      <td>10.0</td>\n      <td>41045.0</td>\n      <td>15.0</td>\n      <td>0.500045</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>158.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>309870.0</td>\n      <td>15.0</td>\n      <td>658494.0</td>\n      <td>158.052257</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>Female</td>\n      <td>24</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>29091.0</td>\n      <td>152.0</td>\n      <td>187</td>\n      <td>5440017.0</td>\n      <td>0.564127</td>\n      <td>294.0</td>\n      <td>13009.0</td>\n      <td>542950.0</td>\n      <td>0.526811</td>\n      <td>187.0</td>\n      <td>814548.0</td>\n      <td>1.0</td>\n      <td>186.0</td>\n      <td>188.0</td>\n      <td>0.167369</td>\n      <td>187.0</td>\n      <td>0.061204</td>\n      <td>13.832753</td>\n      <td>0.404037</td>\n      <td>0.558326</td>\n      <td>0.842734</td>\n      <td>29092.0</td>\n      <td>211.0</td>\n      <td>79439.0</td>\n      <td>0.384363</td>\n      <td>29278.0</td>\n      <td>0.398541</td>\n      <td>29091.0</td>\n      <td>29115.0</td>\n      <td>1212.125000</td>\n      <td>0.558039</td>\n      <td>0.569769</td>\n      <td>0.560980</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.867675</td>\n      <td>191.388158</td>\n      <td>0.510372</td>\n      <td>81032.0</td>\n      <td>191241.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>215.0</td>\n      <td>10887.0</td>\n      <td>15600.277416</td>\n      <td>0.409586</td>\n      <td>141443.0</td>\n      <td>5.428571</td>\n      <td>0.483864</td>\n      <td>0.662050</td>\n      <td>1.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>1.0</td>\n      <td>0.397682</td>\n      <td>86.787510</td>\n      <td>29067.0</td>\n      <td>1.0</td>\n      <td>0.265695</td>\n      <td>0.569303</td>\n      <td>187.0</td>\n      <td>4421832.0</td>\n      <td>29091.0</td>\n      <td>1.0</td>\n      <td>0.566832</td>\n      <td>79.480976</td>\n      <td>128.0</td>\n      <td>12.0</td>\n      <td>7.791667</td>\n      <td>186.0</td>\n      <td>698184.0</td>\n      <td>0.314534</td>\n      <td>188.0</td>\n      <td>29091.0</td>\n      <td>0.466655</td>\n      <td>176.0</td>\n      <td>29243.0</td>\n      <td>104911.0</td>\n      <td>60267.0</td>\n      <td>339.0</td>\n      <td>495106.0</td>\n      <td>5236.0</td>\n      <td>28.0</td>\n      <td>0.464330</td>\n      <td>29119.0</td>\n      <td>152.0</td>\n      <td>3648.0</td>\n      <td>29090.0</td>\n      <td>124.0</td>\n      <td>187.0</td>\n      <td>159.0</td>\n      <td>106390.0</td>\n      <td>0.997138</td>\n      <td>28939.0</td>\n      <td>85.0</td>\n      <td>672.0</td>\n      <td>0.492940</td>\n      <td>155.566845</td>\n      <td>1.0</td>\n      <td>745026.0</td>\n      <td>24.0</td>\n      <td>152580.0</td>\n      <td>29063.0</td>\n      <td>-35.0</td>\n      <td>12837.501221</td>\n      <td>1038.964286</td>\n      <td>0.857143</td>\n      <td>0.641473</td>\n      <td>448404.0</td>\n      <td>152.0</td>\n      <td>13.0</td>\n      <td>-4.0</td>\n      <td>12.0</td>\n      <td>152.0</td>\n      <td>4488.0</td>\n      <td>0.627876</td>\n      <td>24.0</td>\n      <td>163.305323</td>\n      <td>52.0</td>\n      <td>28424.0</td>\n      <td>24.750985</td>\n      <td>0.999991</td>\n      <td>14.0</td>\n      <td>4256.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>118.0</td>\n      <td>0.490040</td>\n      <td>444005.0</td>\n      <td>0.003066</td>\n      <td>470533.0</td>\n      <td>29.0</td>\n      <td>152.0</td>\n      <td>76289.0</td>\n      <td>5.023881</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>29091.0</td>\n      <td>5.291503</td>\n      <td>0.370257</td>\n      <td>1.0</td>\n      <td>165.618351</td>\n      <td>378441.0</td>\n      <td>13.0</td>\n      <td>29091.0</td>\n      <td>27.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>0.0</td>\n      <td>0.497347</td>\n      <td>152.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.294906</td>\n      <td>0.644572</td>\n      <td>14.194695</td>\n      <td>6.678571</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>24.0</td>\n      <td>180.0</td>\n      <td>23.0</td>\n      <td>0.217194</td>\n      <td>0.0</td>\n      <td>187.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>24.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>29.0</td>\n      <td>0.289068</td>\n      <td>1.0</td>\n      <td>0.812834</td>\n      <td>166.0</td>\n      <td>187.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.717063</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>1.0</td>\n      <td>0.494718</td>\n      <td>31420.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>30962.0</td>\n      <td>-27.0</td>\n      <td>6.333333</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>24.0</td>\n      <td>1.0</td>\n      <td>24.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>151.0</td>\n      <td>23.0</td>\n      <td>0.493777</td>\n      <td>25.0</td>\n      <td>31875.865750</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>106391.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.450428</td>\n      <td>25.0</td>\n      <td>24.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>1.0</td>\n      <td>17.997360</td>\n      <td>10.0</td>\n      <td>28904.0</td>\n      <td>28.0</td>\n      <td>0.500009</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>187.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>658494.0</td>\n      <td>149.376930</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows  251 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.concat([train_x, train_subset['Response']], axis=1)\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:06:02.435535Z","iopub.execute_input":"2024-07-29T09:06:02.436231Z","iopub.status.idle":"2024-07-29T09:06:04.323174Z","shell.execute_reply.started":"2024-07-29T09:06:02.436189Z","shell.execute_reply":"2024-07-29T09:06:04.322236Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"   Gender   Age  Driving_License  Region_Code  Previously_Insured Vehicle_Age  \\\n0    Male  25.0              1.0          9.0                 0.0    < 1 Year   \n1  Female  21.0              1.0         28.0                 1.0    < 1 Year   \n2  Female  31.0              1.0         15.0                 0.0    < 1 Year   \n3    Male  25.0              1.0         10.0                 1.0    < 1 Year   \n4    Male  66.0              1.0         28.0                 1.0    1-2 Year   \n\n  Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  autoFE_f_0  \\\n0            Yes         33990.0                 152.0    166.0   5642340.0   \n1             No         49514.0                 152.0     84.0   4159176.0   \n2             No         36913.0                 152.0    149.0   5500037.0   \n3             No         36274.0                 152.0    256.0   9286144.0   \n4            Yes         44214.0                  26.0    223.0   9859722.0   \n\n   autoFE_f_1  autoFE_f_2  autoFE_f_3  autoFE_f_4  autoFE_f_5  autoFE_f_6  \\\n0    0.480421       186.0      5109.0    542950.0    0.178632       166.0   \n1    0.190024        82.0      8441.0    542950.0    0.477880        84.0   \n2    0.451416        25.0      4808.0    542950.0    0.248792       149.0   \n3    0.829111       106.0     11913.0    542950.0    0.195258       256.0   \n4    0.708394        93.0      4884.0    317263.0    0.485734       223.0   \n\n   autoFE_f_7 autoFE_f_8  autoFE_f_9  autoFE_f_10  autoFE_f_11  autoFE_f_12  \\\n0    305910.0        0.0       166.0        166.0     0.653033          NaN   \n1   1386392.0        1.0        83.0         85.0     0.785224         84.0   \n2    553695.0        2.0       149.0        149.0     0.741447          NaN   \n3    362740.0        1.0       255.0        257.0     0.921735        256.0   \n4   1237992.0        3.0       222.0        224.0     0.609766        223.0   \n\n   autoFE_f_13  autoFE_f_14  autoFE_f_15  autoFE_f_16  autoFE_f_17  \\\n0     0.380000    14.485116     0.529460     0.510935     0.280615   \n1     0.007696    13.832753     0.923739     0.194659     0.842734   \n2     0.645197    13.084419     0.715309     0.432102     0.260368   \n3     0.466462    11.027021     0.700654     0.835286     0.672440   \n4     0.906301    13.832753     0.790905     0.725049     0.842734   \n\n   autoFE_f_18  autoFE_f_19  autoFE_f_20  autoFE_f_21  autoFE_f_22  \\\n0      33990.0        191.0       6910.0     0.573447      34156.0   \n1      49515.0        105.0      79439.0     0.908039      49598.0   \n2      36913.0        180.0      34800.0     0.670870      37062.0   \n3      36275.0        281.0      12342.0     0.655816      36530.0   \n4      44215.0        289.0     332339.0     0.821479      44437.0   \n\n   autoFE_f_23  autoFE_f_24  autoFE_f_25  autoFE_f_26  autoFE_f_27  \\\n0     0.537776      33990.0      34015.0  1359.600000     0.509686   \n1     0.922583      49514.0      49535.0  2357.809524     0.194317   \n2     0.634359      36913.0      36944.0  1190.741935     0.446585   \n3     0.697934      36274.0      36299.0  1450.960000     0.835485   \n4     0.849935      44214.0      44280.0   669.909091     0.701396   \n\n   autoFE_f_28  autoFE_f_29 autoFE_f_30  autoFE_f_31  autoFE_f_32  \\\n0     0.500451     0.487574         0.0    34.986475     0.589180   \n1     0.205071     0.190794         1.0    45.709578     0.867675   \n2     0.417740     0.430972         2.0    32.053954     0.553907   \n3     0.824923     0.836364         3.0    30.023605     0.539638   \n4     0.717548     0.718992         1.0    45.709578     0.223135   \n\n   autoFE_f_33  autoFE_f_34  autoFE_f_35  autoFE_f_36  autoFE_f_37  \\\n0   223.618421     0.499175      17590.0       6078.0         28.0   \n1   325.750000     0.266412      37064.0     191241.0         45.0   \n2   242.848684     0.546335      11290.0      26554.0         26.0   \n3   238.644737     0.499175      64870.0       7566.0         25.0   \n4  1700.538462     0.303853       6696.0     258450.0         45.0   \n\n   autoFE_f_38  autoFE_f_39  autoFE_f_40   autoFE_f_41  autoFE_f_42  \\\n0    78.856796        175.0        895.0  14079.233812     0.643177   \n1    79.938583        112.0       4709.0  15600.277416     0.937936   \n2    80.427887        164.0        855.0  13766.828032     0.746904   \n3    82.260215        266.0       1847.0  11346.682460     0.731384   \n4    79.938583        251.0       4950.0  15600.277416     0.787907   \n\n   autoFE_f_43  autoFE_f_44  autoFE_f_45  autoFE_f_46 autoFE_f_47  \\\n0       6846.0    16.888889     0.625551     0.162050         0.0   \n1     141443.0     5.428571     0.080719     0.662050         1.0   \n2      27245.0    10.133333     0.946592     0.162050         2.0   \n3      10686.0    15.200000     0.625551     0.662050         3.0   \n4     141443.0     0.928571     0.903795     0.839407         4.0   \n\n    autoFE_f_48  autoFE_f_49 autoFE_f_50  autoFE_f_51  autoFE_f_52  \\\n0  26902.168033      30648.0         0.0     0.554006   125.991393   \n1  39558.794219      39802.0         1.0     0.913898    86.787510   \n2  28959.286744      31574.5         1.0     0.690318   132.888517   \n3  24892.287308      28015.0         0.0     0.638378   139.645371   \n4  39558.794219      39802.0         2.0     0.810001    86.787510   \n\n   autoFE_f_53 autoFE_f_54  autoFE_f_55  autoFE_f_56  autoFE_f_57  \\\n0      33965.0         0.0     0.225569     0.499973        166.0   \n1      49493.0         1.0     0.043722     0.201356         84.0   \n2      36882.0         2.0     0.522780     0.439885        149.0   \n3      36249.0         3.0     0.225569     0.842750        256.0   \n4      44148.0         1.0     0.938345     0.713352        223.0   \n\n   autoFE_f_58  autoFE_f_59 autoFE_f_60  autoFE_f_61  autoFE_f_62  \\\n0    5166480.0          NaN         0.0     0.505359    79.646454   \n1    7526128.0      49514.0         1.0     0.195420    79.342478   \n2    5610776.0          NaN         2.0     0.431017    79.379644   \n3    5513648.0      36274.0         3.0     0.840829    79.646454   \n4    1149564.0      44214.0         4.0     0.714188    79.709674   \n\n   autoFE_f_63 autoFE_f_64  autoFE_f_65  autoFE_f_66  autoFE_f_67  \\\n0        127.0         0.0     6.640000        165.0     849750.0   \n1        131.0         1.0     4.000000         83.0    1039794.0   \n2        121.0         2.0     4.806452        148.0    1144303.0   \n3        127.0         0.0    10.240000        255.0     906850.0   \n4        -40.0         3.0     3.378788        222.0    2918124.0   \n\n   autoFE_f_68  autoFE_f_69  autoFE_f_70  autoFE_f_71  autoFE_f_72  \\\n0     0.438852        167.0      33990.0     0.466655        177.0   \n1     0.314534         85.0      49514.0     0.466655        173.0   \n2     0.479282        150.0      36913.0     0.466655        183.0   \n3     0.655181        257.0      36274.0     0.466655        177.0   \n4     0.314534        224.0      44214.0     0.235340         92.0   \n\n   autoFE_f_73  autoFE_f_74  autoFE_f_75  autoFE_f_76  autoFE_f_77  \\\n0      34142.0      81240.0      35526.0        318.0     290594.0   \n1      49666.0      63813.0      36329.0        236.0     488138.0   \n2      37065.0      15033.0       9979.0        301.0     346200.0   \n3      36426.0      81240.0      35526.0        408.0     290594.0   \n4      44240.0       9586.0       6662.0        249.0     308615.0   \n\n   autoFE_f_78  autoFE_f_79  autoFE_f_80  autoFE_f_81  autoFE_f_82  \\\n0       1494.0         25.0     0.496267      33999.0        152.0   \n1       2352.0         28.0     0.464330      49542.0         84.0   \n2       2235.0         31.0     0.499575      36928.0        149.0   \n3       2560.0         25.0     0.475324      36284.0        152.0   \n4       6244.0         66.0     0.464330      44242.0         26.0   \n\n   autoFE_f_83  autoFE_f_84  autoFE_f_85  autoFE_f_86  autoFE_f_87  \\\n0       3800.0      33990.0        143.0        166.0        157.0   \n1       3192.0      49513.0        124.0         84.0         56.0   \n2       4712.0      36913.0        137.0        149.0        134.0   \n3       3800.0      36273.0        142.0        256.0        246.0   \n4       1716.0      44213.0         -2.0        223.0        195.0   \n\n   autoFE_f_88  autoFE_f_89  autoFE_f_90  autoFE_f_91  autoFE_f_92  \\\n0      82460.0     0.998033      33838.0         80.0        225.0   \n1      65868.0     0.997138      49362.0         85.0        588.0   \n2      21118.0     0.998624      36761.0         81.0        465.0   \n3      82460.0     0.999387      36122.0         80.0        250.0   \n4      10791.0     0.997138      44188.0         85.0       1848.0   \n\n   autoFE_f_93  autoFE_f_94 autoFE_f_95  autoFE_f_96  autoFE_f_97  \\\n0     0.147187   204.759036         0.0     751966.0         25.0   \n1     0.492940   589.452381         1.0     745026.0         21.0   \n2     0.286846   247.738255         2.0     745026.0         31.0   \n3     0.193205   141.695312         3.0     745026.0         25.0   \n4     0.496518   198.269058         4.0     751966.0         66.0   \n\n   autoFE_f_98  autoFE_f_99  autoFE_f_100  autoFE_f_101  autoFE_f_102  \\\n0       6063.0      33981.0         -14.0  12986.968804   3776.666667   \n1     152580.0      49486.0          68.0  13812.283547   1768.357143   \n2      29458.0      36898.0           3.0  17397.458965   2460.866667   \n3      11198.0      36264.0        -104.0  12986.968804   3627.400000   \n4     297111.0      44186.0        -197.0  19314.044119   1579.071429   \n\n   autoFE_f_103  autoFE_f_104  autoFE_f_105  autoFE_f_106 autoFE_f_107  \\\n0      2.777778      0.129475       12176.0         152.0          0.0   \n1      0.750000      0.730673      448404.0         152.0          1.0   \n2      2.066667      0.254759       52250.0         152.0          2.0   \n3      2.500000      0.629475       16300.0         152.0          3.0   \n4      2.357143      0.813899      448404.0          26.0          4.0   \n\n   autoFE_f_108 autoFE_f_109  autoFE_f_110  autoFE_f_111  autoFE_f_112  \\\n0          16.0          0.0           NaN        4150.0      0.752077   \n1          -7.0          1.0         152.0        1764.0      0.627876   \n2          16.0          2.0           NaN        4619.0      0.627876   \n3          15.0          0.0         152.0        6400.0      0.627876   \n4          38.0          3.0          26.0       14718.0      0.187160   \n\n   autoFE_f_113  autoFE_f_114  autoFE_f_115  autoFE_f_116  autoFE_f_117  \\\n0          25.0    162.882705          34.0       25232.0     25.408137   \n1          21.0    163.305323          49.0       12768.0     26.768385   \n2          31.0    166.741256          46.0       22648.0     26.660858   \n3          25.0    164.072226          35.0       38912.0     25.408137   \n4          26.0    163.305323          94.0        5798.0     27.238165   \n\n   autoFE_f_118 autoFE_f_119  autoFE_f_120  autoFE_f_121  autoFE_f_122  \\\n0          1.00          0.0        1368.0         152.0           1.0   \n1          1.00          1.0        4256.0         152.0           1.0   \n2          1.00          2.0        2280.0         152.0           1.0   \n3          1.00          0.0        1520.0         152.0           1.0   \n4          0.99          3.0         728.0          66.0           1.0   \n\n  autoFE_f_123  autoFE_f_124  autoFE_f_125  autoFE_f_126  autoFE_f_127  \\\n0          0.0      0.149294      212903.0      0.000000      186375.0   \n1          1.0      0.490040      444005.0      0.000000      470533.0   \n2          2.0      0.218216      212903.0      0.000000      470533.0   \n3          3.0      0.193213      444005.0      0.000000      470533.0   \n4          4.0      0.490040      250795.0      0.099503      505403.0   \n\n   autoFE_f_128  autoFE_f_129  autoFE_f_130  autoFE_f_131  autoFE_f_132  \\\n0           9.0         152.0       21352.0      5.023881      0.111111   \n1          29.0         152.0       35481.0      5.023881      0.035714   \n2          15.0         152.0       10759.0      5.023881      0.066667   \n3          11.0         152.0       61108.0      5.023881      0.100000   \n4          29.0          28.0        4058.0      3.258097      0.035714   \n\n   autoFE_f_133  autoFE_f_134  autoFE_f_135  autoFE_f_136  autoFE_f_137  \\\n0           9.0           9.0       33990.0      3.000000      0.632113   \n1          28.0          28.0       49514.0      5.291503      0.942969   \n2          15.0          15.0       36913.0      3.872983      0.766029   \n3          10.0          10.0       36274.0      3.162278      0.719155   \n4          28.0          28.0       44214.0      5.291503      0.709128   \n\n   autoFE_f_138  autoFE_f_139  autoFE_f_140 autoFE_f_141  autoFE_f_142  \\\n0           1.0    167.142991      444306.0          0.0           0.0   \n1           1.0    166.127907      378441.0          1.0       49514.0   \n2           0.0    163.824747      378441.0          2.0           0.0   \n3           1.0    167.142991      367578.0          3.0       36274.0   \n4           0.0    165.347064      444306.0          4.0       44214.0   \n\n   autoFE_f_143  autoFE_f_144  autoFE_f_145  autoFE_f_146  autoFE_f_147  \\\n0           9.0      809627.0      0.268198     47.998774           0.0   \n1          27.0      687365.0      0.768198     54.847088           1.0   \n2          15.0      687365.0      0.268198     43.929969           0.0   \n3           9.0      809627.0      0.768198     35.676699           0.0   \n4          27.0      809627.0      0.768198     54.847088           0.0   \n\n   autoFE_f_148  autoFE_f_149  autoFE_f_150  autoFE_f_151  autoFE_f_152  \\\n0      0.186864           0.0       12200.0         299.0      0.181256   \n1      0.497347         152.0      449691.0         299.0      0.038379   \n2      0.313013           0.0       52322.0         299.0      0.267417   \n3      0.201518         152.0       16310.0         299.0      0.393759   \n4      0.492609          26.0      449691.0         299.0      0.952428   \n\n   autoFE_f_153  autoFE_f_154  autoFE_f_155 autoFE_f_156  autoFE_f_157  \\\n0      0.729068     13.707443     18.444444          0.0           NaN   \n1      0.644572     15.008618      3.000000          1.0           1.0   \n2      0.644572     13.183065      9.933333          1.0           NaN   \n3      0.729068     13.707443     25.600000          1.0           1.0   \n4      0.162168     11.906707      7.964286          0.0           1.0   \n\n   autoFE_f_158  autoFE_f_159  autoFE_f_160  autoFE_f_161  autoFE_f_162  \\\n0          25.0         161.0          24.0      0.280270           0.0   \n1          21.0         180.0          20.0      0.036905           0.0   \n2          31.0         167.0          30.0      0.433391           0.0   \n3          25.0         162.0          24.0      0.280270           0.0   \n4          66.0          54.0          65.0      0.944127           0.0   \n\n   autoFE_f_163  autoFE_f_164  autoFE_f_165  autoFE_f_166  autoFE_f_167  \\\n0         166.0         153.0      0.689718          25.0           1.0   \n1         152.0         153.0      0.689718          21.0           1.0   \n2         152.0         153.0      0.689718          31.0           1.0   \n3         256.0         153.0      0.689718          25.0           1.0   \n4         223.0          27.0      0.143149          66.0           1.0   \n\n   autoFE_f_168  autoFE_f_169  autoFE_f_170 autoFE_f_171  autoFE_f_172  \\\n0      0.748409          10.0      0.173696          0.0      0.915663   \n1      0.622999          29.0      0.037390          1.0      1.809524   \n2      0.748409          16.0      0.613586          0.0      1.020134   \n3      0.622999          11.0      0.386855          1.0      0.593750   \n4      0.095901          29.0      0.932316          2.0      0.116592   \n\n   autoFE_f_173  autoFE_f_174  autoFE_f_175  autoFE_f_176  autoFE_f_177  \\\n0         166.0         166.0           1.0           1.0          61.0   \n1         166.0          84.0           1.0           2.0          66.0   \n2         171.0         149.0           1.0           1.0          62.0   \n3         169.0         256.0           1.0           2.0          61.0   \n4         166.0         223.0           1.0           2.0          66.0   \n\n   autoFE_f_178  autoFE_f_179  autoFE_f_180  autoFE_f_181 autoFE_f_182  \\\n0      2.197225      0.741062      0.999877      0.501025          0.0   \n1      3.332205      0.538668      1.000000      0.501432          1.0   \n2      2.708050      0.490529      1.000000      0.500698          1.0   \n3      2.302585      0.741062      0.999955      0.500337          1.0   \n4      3.332205      0.372294      1.000000      0.501432          2.0   \n\n   autoFE_f_183  autoFE_f_184  autoFE_f_185  autoFE_f_186  autoFE_f_187  \\\n0      0.164303       30984.0      0.501280         152.0       32993.0   \n1      0.494718       30502.0      0.500683         122.0       30962.0   \n2      0.250191       29007.5      0.501280         152.0       30962.0   \n3      0.173814       30984.0      0.500683         152.0       30962.0   \n4      0.494718       35500.0      0.500683         122.0       32993.0   \n\n   autoFE_f_188  autoFE_f_189  autoFE_f_190  autoFE_f_191  autoFE_f_192  \\\n0          -8.0      6.080000         163.0           3.0           0.0   \n1         -27.0      7.238095         163.0           3.0          21.0   \n2         -14.0      4.903226         163.0           3.0           0.0   \n3          -9.0      6.080000         163.0           3.0          25.0   \n4         -27.0      0.393939         163.0           3.0          66.0   \n\n   autoFE_f_193  autoFE_f_194  autoFE_f_195  autoFE_f_196  autoFE_f_197  \\\n0           1.0           9.0           0.0           NaN         152.0   \n1           1.0          21.0           1.0          28.0         151.0   \n2           1.0          15.0           0.0           NaN         152.0   \n3           1.0          10.0           1.0          10.0         151.0   \n4           1.0          28.0           1.0          28.0          25.0   \n\n   autoFE_f_198  autoFE_f_199  autoFE_f_200  autoFE_f_201  autoFE_f_202  \\\n0          25.0      0.166778          25.0  31400.949636           1.0   \n1          20.0      0.493777          22.0  30066.032353           0.0   \n2          31.0      0.254576          31.0  25857.063169           1.0   \n3          24.0      0.175181          26.0  31400.949636           0.0   \n4          65.0      0.495553          67.0  32524.870734           0.0   \n\n   autoFE_f_203  autoFE_f_204  autoFE_f_205  autoFE_f_206  autoFE_f_207  \\\n0           1.0         164.0         152.0      291520.0           1.0   \n1           1.0         170.0         153.0      365388.0           1.0   \n2           1.0         164.0         152.0      365388.0           1.0   \n3           1.0         170.0         153.0      291520.0           1.0   \n4           1.0         170.0          27.0      479626.0           1.0   \n\n   autoFE_f_208  autoFE_f_209  autoFE_f_210  autoFE_f_211  autoFE_f_212  \\\n0       82460.0          20.0           1.0           9.0           0.0   \n1       65868.0          20.0           1.0          28.0           0.0   \n2       21118.0          20.0           1.0          15.0           0.0   \n3       82460.0          20.0           1.0          10.0           1.0   \n4       10900.0          20.0           1.0          28.0           0.0   \n\n   autoFE_f_213  autoFE_f_214  autoFE_f_215  autoFE_f_216  autoFE_f_217  \\\n0           2.0           0.0    163.172999      0.438054          26.0   \n1           2.0           1.0    164.729211      0.498506          22.0   \n2           2.0           0.0    164.729211      0.499922          32.0   \n3           2.0           1.0    163.172999      0.438054          26.0   \n4           2.0           1.0    163.172999      0.483438          67.0   \n\n   autoFE_f_218  autoFE_f_219  autoFE_f_220  autoFE_f_221  autoFE_f_222  \\\n0           NaN         152.0         152.0           9.0           1.0   \n1          21.0         152.0         152.0          28.0           1.0   \n2           NaN         152.0         152.0          15.0           1.0   \n3          25.0         152.0         152.0          10.0           1.0   \n4          66.0          26.0          26.0          26.0           1.0   \n\n   autoFE_f_223  autoFE_f_224 autoFE_f_225  autoFE_f_226  autoFE_f_227  \\\n0         151.0          53.0          0.0     20.478651          10.0   \n1         151.0          52.0          1.0     12.379879          10.0   \n2         151.0          53.0          2.0     37.619003          10.0   \n3         151.0          53.0          3.0     20.478651          10.0   \n4          25.0          53.0          3.0     53.125551          10.0   \n\n   autoFE_f_228  autoFE_f_229  autoFE_f_230  autoFE_f_231  autoFE_f_232  \\\n0       33824.0           9.0      0.500006      0.288312          10.0   \n1       49430.0          28.0      0.500008      0.744792          10.0   \n2       36764.0          15.0      0.500024      0.244792          10.0   \n3       36018.0          10.0      0.500006      0.788312          10.0   \n4       43991.0          28.0      0.505046      0.788312          10.0   \n\n   autoFE_f_233  autoFE_f_234  autoFE_f_235 autoFE_f_236  autoFE_f_237  \\\n0           1.0         166.0         299.0          0.0      313424.0   \n1           1.0          84.0         299.0          1.0      540165.0   \n2           1.0         149.0         299.0          1.0      309870.0   \n3           1.0         256.0         299.0          2.0       85517.0   \n4           1.0         223.0         299.0          0.0      540165.0   \n\n   autoFE_f_238  autoFE_f_239  autoFE_f_240  Response  \n0           0.0      717515.0    148.647138       NaN  \n1          28.0      658494.0    154.448108       1.0  \n2           0.0       87525.0    135.694052       NaN  \n3          10.0      658494.0    148.647138       NaN  \n4          28.0       36466.0     71.982385       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Driving_License</th>\n      <th>Region_Code</th>\n      <th>Previously_Insured</th>\n      <th>Vehicle_Age</th>\n      <th>Vehicle_Damage</th>\n      <th>Annual_Premium</th>\n      <th>Policy_Sales_Channel</th>\n      <th>Vintage</th>\n      <th>autoFE_f_0</th>\n      <th>autoFE_f_1</th>\n      <th>autoFE_f_2</th>\n      <th>autoFE_f_3</th>\n      <th>autoFE_f_4</th>\n      <th>autoFE_f_5</th>\n      <th>autoFE_f_6</th>\n      <th>autoFE_f_7</th>\n      <th>autoFE_f_8</th>\n      <th>autoFE_f_9</th>\n      <th>autoFE_f_10</th>\n      <th>autoFE_f_11</th>\n      <th>autoFE_f_12</th>\n      <th>autoFE_f_13</th>\n      <th>autoFE_f_14</th>\n      <th>autoFE_f_15</th>\n      <th>autoFE_f_16</th>\n      <th>autoFE_f_17</th>\n      <th>autoFE_f_18</th>\n      <th>autoFE_f_19</th>\n      <th>autoFE_f_20</th>\n      <th>autoFE_f_21</th>\n      <th>autoFE_f_22</th>\n      <th>autoFE_f_23</th>\n      <th>autoFE_f_24</th>\n      <th>autoFE_f_25</th>\n      <th>autoFE_f_26</th>\n      <th>autoFE_f_27</th>\n      <th>autoFE_f_28</th>\n      <th>autoFE_f_29</th>\n      <th>autoFE_f_30</th>\n      <th>autoFE_f_31</th>\n      <th>autoFE_f_32</th>\n      <th>autoFE_f_33</th>\n      <th>autoFE_f_34</th>\n      <th>autoFE_f_35</th>\n      <th>autoFE_f_36</th>\n      <th>autoFE_f_37</th>\n      <th>autoFE_f_38</th>\n      <th>autoFE_f_39</th>\n      <th>autoFE_f_40</th>\n      <th>autoFE_f_41</th>\n      <th>autoFE_f_42</th>\n      <th>autoFE_f_43</th>\n      <th>autoFE_f_44</th>\n      <th>autoFE_f_45</th>\n      <th>autoFE_f_46</th>\n      <th>autoFE_f_47</th>\n      <th>autoFE_f_48</th>\n      <th>autoFE_f_49</th>\n      <th>autoFE_f_50</th>\n      <th>autoFE_f_51</th>\n      <th>autoFE_f_52</th>\n      <th>autoFE_f_53</th>\n      <th>autoFE_f_54</th>\n      <th>autoFE_f_55</th>\n      <th>autoFE_f_56</th>\n      <th>autoFE_f_57</th>\n      <th>autoFE_f_58</th>\n      <th>autoFE_f_59</th>\n      <th>autoFE_f_60</th>\n      <th>autoFE_f_61</th>\n      <th>autoFE_f_62</th>\n      <th>autoFE_f_63</th>\n      <th>autoFE_f_64</th>\n      <th>autoFE_f_65</th>\n      <th>autoFE_f_66</th>\n      <th>autoFE_f_67</th>\n      <th>autoFE_f_68</th>\n      <th>autoFE_f_69</th>\n      <th>autoFE_f_70</th>\n      <th>autoFE_f_71</th>\n      <th>autoFE_f_72</th>\n      <th>autoFE_f_73</th>\n      <th>autoFE_f_74</th>\n      <th>autoFE_f_75</th>\n      <th>autoFE_f_76</th>\n      <th>autoFE_f_77</th>\n      <th>autoFE_f_78</th>\n      <th>autoFE_f_79</th>\n      <th>autoFE_f_80</th>\n      <th>autoFE_f_81</th>\n      <th>autoFE_f_82</th>\n      <th>autoFE_f_83</th>\n      <th>autoFE_f_84</th>\n      <th>autoFE_f_85</th>\n      <th>autoFE_f_86</th>\n      <th>autoFE_f_87</th>\n      <th>autoFE_f_88</th>\n      <th>autoFE_f_89</th>\n      <th>autoFE_f_90</th>\n      <th>autoFE_f_91</th>\n      <th>autoFE_f_92</th>\n      <th>autoFE_f_93</th>\n      <th>autoFE_f_94</th>\n      <th>autoFE_f_95</th>\n      <th>autoFE_f_96</th>\n      <th>autoFE_f_97</th>\n      <th>autoFE_f_98</th>\n      <th>autoFE_f_99</th>\n      <th>autoFE_f_100</th>\n      <th>autoFE_f_101</th>\n      <th>autoFE_f_102</th>\n      <th>autoFE_f_103</th>\n      <th>autoFE_f_104</th>\n      <th>autoFE_f_105</th>\n      <th>autoFE_f_106</th>\n      <th>autoFE_f_107</th>\n      <th>autoFE_f_108</th>\n      <th>autoFE_f_109</th>\n      <th>autoFE_f_110</th>\n      <th>autoFE_f_111</th>\n      <th>autoFE_f_112</th>\n      <th>autoFE_f_113</th>\n      <th>autoFE_f_114</th>\n      <th>autoFE_f_115</th>\n      <th>autoFE_f_116</th>\n      <th>autoFE_f_117</th>\n      <th>autoFE_f_118</th>\n      <th>autoFE_f_119</th>\n      <th>autoFE_f_120</th>\n      <th>autoFE_f_121</th>\n      <th>autoFE_f_122</th>\n      <th>autoFE_f_123</th>\n      <th>autoFE_f_124</th>\n      <th>autoFE_f_125</th>\n      <th>autoFE_f_126</th>\n      <th>autoFE_f_127</th>\n      <th>autoFE_f_128</th>\n      <th>autoFE_f_129</th>\n      <th>autoFE_f_130</th>\n      <th>autoFE_f_131</th>\n      <th>autoFE_f_132</th>\n      <th>autoFE_f_133</th>\n      <th>autoFE_f_134</th>\n      <th>autoFE_f_135</th>\n      <th>autoFE_f_136</th>\n      <th>autoFE_f_137</th>\n      <th>autoFE_f_138</th>\n      <th>autoFE_f_139</th>\n      <th>autoFE_f_140</th>\n      <th>autoFE_f_141</th>\n      <th>autoFE_f_142</th>\n      <th>autoFE_f_143</th>\n      <th>autoFE_f_144</th>\n      <th>autoFE_f_145</th>\n      <th>autoFE_f_146</th>\n      <th>autoFE_f_147</th>\n      <th>autoFE_f_148</th>\n      <th>autoFE_f_149</th>\n      <th>autoFE_f_150</th>\n      <th>autoFE_f_151</th>\n      <th>autoFE_f_152</th>\n      <th>autoFE_f_153</th>\n      <th>autoFE_f_154</th>\n      <th>autoFE_f_155</th>\n      <th>autoFE_f_156</th>\n      <th>autoFE_f_157</th>\n      <th>autoFE_f_158</th>\n      <th>autoFE_f_159</th>\n      <th>autoFE_f_160</th>\n      <th>autoFE_f_161</th>\n      <th>autoFE_f_162</th>\n      <th>autoFE_f_163</th>\n      <th>autoFE_f_164</th>\n      <th>autoFE_f_165</th>\n      <th>autoFE_f_166</th>\n      <th>autoFE_f_167</th>\n      <th>autoFE_f_168</th>\n      <th>autoFE_f_169</th>\n      <th>autoFE_f_170</th>\n      <th>autoFE_f_171</th>\n      <th>autoFE_f_172</th>\n      <th>autoFE_f_173</th>\n      <th>autoFE_f_174</th>\n      <th>autoFE_f_175</th>\n      <th>autoFE_f_176</th>\n      <th>autoFE_f_177</th>\n      <th>autoFE_f_178</th>\n      <th>autoFE_f_179</th>\n      <th>autoFE_f_180</th>\n      <th>autoFE_f_181</th>\n      <th>autoFE_f_182</th>\n      <th>autoFE_f_183</th>\n      <th>autoFE_f_184</th>\n      <th>autoFE_f_185</th>\n      <th>autoFE_f_186</th>\n      <th>autoFE_f_187</th>\n      <th>autoFE_f_188</th>\n      <th>autoFE_f_189</th>\n      <th>autoFE_f_190</th>\n      <th>autoFE_f_191</th>\n      <th>autoFE_f_192</th>\n      <th>autoFE_f_193</th>\n      <th>autoFE_f_194</th>\n      <th>autoFE_f_195</th>\n      <th>autoFE_f_196</th>\n      <th>autoFE_f_197</th>\n      <th>autoFE_f_198</th>\n      <th>autoFE_f_199</th>\n      <th>autoFE_f_200</th>\n      <th>autoFE_f_201</th>\n      <th>autoFE_f_202</th>\n      <th>autoFE_f_203</th>\n      <th>autoFE_f_204</th>\n      <th>autoFE_f_205</th>\n      <th>autoFE_f_206</th>\n      <th>autoFE_f_207</th>\n      <th>autoFE_f_208</th>\n      <th>autoFE_f_209</th>\n      <th>autoFE_f_210</th>\n      <th>autoFE_f_211</th>\n      <th>autoFE_f_212</th>\n      <th>autoFE_f_213</th>\n      <th>autoFE_f_214</th>\n      <th>autoFE_f_215</th>\n      <th>autoFE_f_216</th>\n      <th>autoFE_f_217</th>\n      <th>autoFE_f_218</th>\n      <th>autoFE_f_219</th>\n      <th>autoFE_f_220</th>\n      <th>autoFE_f_221</th>\n      <th>autoFE_f_222</th>\n      <th>autoFE_f_223</th>\n      <th>autoFE_f_224</th>\n      <th>autoFE_f_225</th>\n      <th>autoFE_f_226</th>\n      <th>autoFE_f_227</th>\n      <th>autoFE_f_228</th>\n      <th>autoFE_f_229</th>\n      <th>autoFE_f_230</th>\n      <th>autoFE_f_231</th>\n      <th>autoFE_f_232</th>\n      <th>autoFE_f_233</th>\n      <th>autoFE_f_234</th>\n      <th>autoFE_f_235</th>\n      <th>autoFE_f_236</th>\n      <th>autoFE_f_237</th>\n      <th>autoFE_f_238</th>\n      <th>autoFE_f_239</th>\n      <th>autoFE_f_240</th>\n      <th>Response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Male</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>&lt; 1 Year</td>\n      <td>Yes</td>\n      <td>33990.0</td>\n      <td>152.0</td>\n      <td>166.0</td>\n      <td>5642340.0</td>\n      <td>0.480421</td>\n      <td>186.0</td>\n      <td>5109.0</td>\n      <td>542950.0</td>\n      <td>0.178632</td>\n      <td>166.0</td>\n      <td>305910.0</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>0.653033</td>\n      <td>NaN</td>\n      <td>0.380000</td>\n      <td>14.485116</td>\n      <td>0.529460</td>\n      <td>0.510935</td>\n      <td>0.280615</td>\n      <td>33990.0</td>\n      <td>191.0</td>\n      <td>6910.0</td>\n      <td>0.573447</td>\n      <td>34156.0</td>\n      <td>0.537776</td>\n      <td>33990.0</td>\n      <td>34015.0</td>\n      <td>1359.600000</td>\n      <td>0.509686</td>\n      <td>0.500451</td>\n      <td>0.487574</td>\n      <td>0.0</td>\n      <td>34.986475</td>\n      <td>0.589180</td>\n      <td>223.618421</td>\n      <td>0.499175</td>\n      <td>17590.0</td>\n      <td>6078.0</td>\n      <td>28.0</td>\n      <td>78.856796</td>\n      <td>175.0</td>\n      <td>895.0</td>\n      <td>14079.233812</td>\n      <td>0.643177</td>\n      <td>6846.0</td>\n      <td>16.888889</td>\n      <td>0.625551</td>\n      <td>0.162050</td>\n      <td>0.0</td>\n      <td>26902.168033</td>\n      <td>30648.0</td>\n      <td>0.0</td>\n      <td>0.554006</td>\n      <td>125.991393</td>\n      <td>33965.0</td>\n      <td>0.0</td>\n      <td>0.225569</td>\n      <td>0.499973</td>\n      <td>166.0</td>\n      <td>5166480.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.505359</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>6.640000</td>\n      <td>165.0</td>\n      <td>849750.0</td>\n      <td>0.438852</td>\n      <td>167.0</td>\n      <td>33990.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>34142.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>318.0</td>\n      <td>290594.0</td>\n      <td>1494.0</td>\n      <td>25.0</td>\n      <td>0.496267</td>\n      <td>33999.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>33990.0</td>\n      <td>143.0</td>\n      <td>166.0</td>\n      <td>157.0</td>\n      <td>82460.0</td>\n      <td>0.998033</td>\n      <td>33838.0</td>\n      <td>80.0</td>\n      <td>225.0</td>\n      <td>0.147187</td>\n      <td>204.759036</td>\n      <td>0.0</td>\n      <td>751966.0</td>\n      <td>25.0</td>\n      <td>6063.0</td>\n      <td>33981.0</td>\n      <td>-14.0</td>\n      <td>12986.968804</td>\n      <td>3776.666667</td>\n      <td>2.777778</td>\n      <td>0.129475</td>\n      <td>12176.0</td>\n      <td>152.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>4150.0</td>\n      <td>0.752077</td>\n      <td>25.0</td>\n      <td>162.882705</td>\n      <td>34.0</td>\n      <td>25232.0</td>\n      <td>25.408137</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>1368.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.149294</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>186375.0</td>\n      <td>9.0</td>\n      <td>152.0</td>\n      <td>21352.0</td>\n      <td>5.023881</td>\n      <td>0.111111</td>\n      <td>9.0</td>\n      <td>9.0</td>\n      <td>33990.0</td>\n      <td>3.000000</td>\n      <td>0.632113</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>444306.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.268198</td>\n      <td>47.998774</td>\n      <td>0.0</td>\n      <td>0.186864</td>\n      <td>0.0</td>\n      <td>12200.0</td>\n      <td>299.0</td>\n      <td>0.181256</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>18.444444</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>25.0</td>\n      <td>161.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>166.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>10.0</td>\n      <td>0.173696</td>\n      <td>0.0</td>\n      <td>0.915663</td>\n      <td>166.0</td>\n      <td>166.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>61.0</td>\n      <td>2.197225</td>\n      <td>0.741062</td>\n      <td>0.999877</td>\n      <td>0.501025</td>\n      <td>0.0</td>\n      <td>0.164303</td>\n      <td>30984.0</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>32993.0</td>\n      <td>-8.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>25.0</td>\n      <td>0.166778</td>\n      <td>25.0</td>\n      <td>31400.949636</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>0.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>33824.0</td>\n      <td>9.0</td>\n      <td>0.500006</td>\n      <td>0.288312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>166.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>313424.0</td>\n      <td>0.0</td>\n      <td>717515.0</td>\n      <td>148.647138</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Female</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>49514.0</td>\n      <td>152.0</td>\n      <td>84.0</td>\n      <td>4159176.0</td>\n      <td>0.190024</td>\n      <td>82.0</td>\n      <td>8441.0</td>\n      <td>542950.0</td>\n      <td>0.477880</td>\n      <td>84.0</td>\n      <td>1386392.0</td>\n      <td>1.0</td>\n      <td>83.0</td>\n      <td>85.0</td>\n      <td>0.785224</td>\n      <td>84.0</td>\n      <td>0.007696</td>\n      <td>13.832753</td>\n      <td>0.923739</td>\n      <td>0.194659</td>\n      <td>0.842734</td>\n      <td>49515.0</td>\n      <td>105.0</td>\n      <td>79439.0</td>\n      <td>0.908039</td>\n      <td>49598.0</td>\n      <td>0.922583</td>\n      <td>49514.0</td>\n      <td>49535.0</td>\n      <td>2357.809524</td>\n      <td>0.194317</td>\n      <td>0.205071</td>\n      <td>0.190794</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.867675</td>\n      <td>325.750000</td>\n      <td>0.266412</td>\n      <td>37064.0</td>\n      <td>191241.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>112.0</td>\n      <td>4709.0</td>\n      <td>15600.277416</td>\n      <td>0.937936</td>\n      <td>141443.0</td>\n      <td>5.428571</td>\n      <td>0.080719</td>\n      <td>0.662050</td>\n      <td>1.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>1.0</td>\n      <td>0.913898</td>\n      <td>86.787510</td>\n      <td>49493.0</td>\n      <td>1.0</td>\n      <td>0.043722</td>\n      <td>0.201356</td>\n      <td>84.0</td>\n      <td>7526128.0</td>\n      <td>49514.0</td>\n      <td>1.0</td>\n      <td>0.195420</td>\n      <td>79.342478</td>\n      <td>131.0</td>\n      <td>1.0</td>\n      <td>4.000000</td>\n      <td>83.0</td>\n      <td>1039794.0</td>\n      <td>0.314534</td>\n      <td>85.0</td>\n      <td>49514.0</td>\n      <td>0.466655</td>\n      <td>173.0</td>\n      <td>49666.0</td>\n      <td>63813.0</td>\n      <td>36329.0</td>\n      <td>236.0</td>\n      <td>488138.0</td>\n      <td>2352.0</td>\n      <td>28.0</td>\n      <td>0.464330</td>\n      <td>49542.0</td>\n      <td>84.0</td>\n      <td>3192.0</td>\n      <td>49513.0</td>\n      <td>124.0</td>\n      <td>84.0</td>\n      <td>56.0</td>\n      <td>65868.0</td>\n      <td>0.997138</td>\n      <td>49362.0</td>\n      <td>85.0</td>\n      <td>588.0</td>\n      <td>0.492940</td>\n      <td>589.452381</td>\n      <td>1.0</td>\n      <td>745026.0</td>\n      <td>21.0</td>\n      <td>152580.0</td>\n      <td>49486.0</td>\n      <td>68.0</td>\n      <td>13812.283547</td>\n      <td>1768.357143</td>\n      <td>0.750000</td>\n      <td>0.730673</td>\n      <td>448404.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>-7.0</td>\n      <td>1.0</td>\n      <td>152.0</td>\n      <td>1764.0</td>\n      <td>0.627876</td>\n      <td>21.0</td>\n      <td>163.305323</td>\n      <td>49.0</td>\n      <td>12768.0</td>\n      <td>26.768385</td>\n      <td>1.00</td>\n      <td>1.0</td>\n      <td>4256.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.490040</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>29.0</td>\n      <td>152.0</td>\n      <td>35481.0</td>\n      <td>5.023881</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>49514.0</td>\n      <td>5.291503</td>\n      <td>0.942969</td>\n      <td>1.0</td>\n      <td>166.127907</td>\n      <td>378441.0</td>\n      <td>1.0</td>\n      <td>49514.0</td>\n      <td>27.0</td>\n      <td>687365.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>1.0</td>\n      <td>0.497347</td>\n      <td>152.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.038379</td>\n      <td>0.644572</td>\n      <td>15.008618</td>\n      <td>3.000000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>180.0</td>\n      <td>20.0</td>\n      <td>0.036905</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>29.0</td>\n      <td>0.037390</td>\n      <td>1.0</td>\n      <td>1.809524</td>\n      <td>166.0</td>\n      <td>84.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.538668</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>1.0</td>\n      <td>0.494718</td>\n      <td>30502.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>30962.0</td>\n      <td>-27.0</td>\n      <td>7.238095</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>151.0</td>\n      <td>20.0</td>\n      <td>0.493777</td>\n      <td>22.0</td>\n      <td>30066.032353</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>65868.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>164.729211</td>\n      <td>0.498506</td>\n      <td>22.0</td>\n      <td>21.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>52.0</td>\n      <td>1.0</td>\n      <td>12.379879</td>\n      <td>10.0</td>\n      <td>49430.0</td>\n      <td>28.0</td>\n      <td>0.500008</td>\n      <td>0.744792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>84.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>658494.0</td>\n      <td>154.448108</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Female</td>\n      <td>31.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36913.0</td>\n      <td>152.0</td>\n      <td>149.0</td>\n      <td>5500037.0</td>\n      <td>0.451416</td>\n      <td>25.0</td>\n      <td>4808.0</td>\n      <td>542950.0</td>\n      <td>0.248792</td>\n      <td>149.0</td>\n      <td>553695.0</td>\n      <td>2.0</td>\n      <td>149.0</td>\n      <td>149.0</td>\n      <td>0.741447</td>\n      <td>NaN</td>\n      <td>0.645197</td>\n      <td>13.084419</td>\n      <td>0.715309</td>\n      <td>0.432102</td>\n      <td>0.260368</td>\n      <td>36913.0</td>\n      <td>180.0</td>\n      <td>34800.0</td>\n      <td>0.670870</td>\n      <td>37062.0</td>\n      <td>0.634359</td>\n      <td>36913.0</td>\n      <td>36944.0</td>\n      <td>1190.741935</td>\n      <td>0.446585</td>\n      <td>0.417740</td>\n      <td>0.430972</td>\n      <td>2.0</td>\n      <td>32.053954</td>\n      <td>0.553907</td>\n      <td>242.848684</td>\n      <td>0.546335</td>\n      <td>11290.0</td>\n      <td>26554.0</td>\n      <td>26.0</td>\n      <td>80.427887</td>\n      <td>164.0</td>\n      <td>855.0</td>\n      <td>13766.828032</td>\n      <td>0.746904</td>\n      <td>27245.0</td>\n      <td>10.133333</td>\n      <td>0.946592</td>\n      <td>0.162050</td>\n      <td>2.0</td>\n      <td>28959.286744</td>\n      <td>31574.5</td>\n      <td>1.0</td>\n      <td>0.690318</td>\n      <td>132.888517</td>\n      <td>36882.0</td>\n      <td>2.0</td>\n      <td>0.522780</td>\n      <td>0.439885</td>\n      <td>149.0</td>\n      <td>5610776.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.431017</td>\n      <td>79.379644</td>\n      <td>121.0</td>\n      <td>2.0</td>\n      <td>4.806452</td>\n      <td>148.0</td>\n      <td>1144303.0</td>\n      <td>0.479282</td>\n      <td>150.0</td>\n      <td>36913.0</td>\n      <td>0.466655</td>\n      <td>183.0</td>\n      <td>37065.0</td>\n      <td>15033.0</td>\n      <td>9979.0</td>\n      <td>301.0</td>\n      <td>346200.0</td>\n      <td>2235.0</td>\n      <td>31.0</td>\n      <td>0.499575</td>\n      <td>36928.0</td>\n      <td>149.0</td>\n      <td>4712.0</td>\n      <td>36913.0</td>\n      <td>137.0</td>\n      <td>149.0</td>\n      <td>134.0</td>\n      <td>21118.0</td>\n      <td>0.998624</td>\n      <td>36761.0</td>\n      <td>81.0</td>\n      <td>465.0</td>\n      <td>0.286846</td>\n      <td>247.738255</td>\n      <td>2.0</td>\n      <td>745026.0</td>\n      <td>31.0</td>\n      <td>29458.0</td>\n      <td>36898.0</td>\n      <td>3.0</td>\n      <td>17397.458965</td>\n      <td>2460.866667</td>\n      <td>2.066667</td>\n      <td>0.254759</td>\n      <td>52250.0</td>\n      <td>152.0</td>\n      <td>2.0</td>\n      <td>16.0</td>\n      <td>2.0</td>\n      <td>NaN</td>\n      <td>4619.0</td>\n      <td>0.627876</td>\n      <td>31.0</td>\n      <td>166.741256</td>\n      <td>46.0</td>\n      <td>22648.0</td>\n      <td>26.660858</td>\n      <td>1.00</td>\n      <td>2.0</td>\n      <td>2280.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.218216</td>\n      <td>212903.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>15.0</td>\n      <td>152.0</td>\n      <td>10759.0</td>\n      <td>5.023881</td>\n      <td>0.066667</td>\n      <td>15.0</td>\n      <td>15.0</td>\n      <td>36913.0</td>\n      <td>3.872983</td>\n      <td>0.766029</td>\n      <td>0.0</td>\n      <td>163.824747</td>\n      <td>378441.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>15.0</td>\n      <td>687365.0</td>\n      <td>0.268198</td>\n      <td>43.929969</td>\n      <td>0.0</td>\n      <td>0.313013</td>\n      <td>0.0</td>\n      <td>52322.0</td>\n      <td>299.0</td>\n      <td>0.267417</td>\n      <td>0.644572</td>\n      <td>13.183065</td>\n      <td>9.933333</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>31.0</td>\n      <td>167.0</td>\n      <td>30.0</td>\n      <td>0.433391</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>31.0</td>\n      <td>1.0</td>\n      <td>0.748409</td>\n      <td>16.0</td>\n      <td>0.613586</td>\n      <td>0.0</td>\n      <td>1.020134</td>\n      <td>171.0</td>\n      <td>149.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>62.0</td>\n      <td>2.708050</td>\n      <td>0.490529</td>\n      <td>1.000000</td>\n      <td>0.500698</td>\n      <td>1.0</td>\n      <td>0.250191</td>\n      <td>29007.5</td>\n      <td>0.501280</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-14.0</td>\n      <td>4.903226</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>31.0</td>\n      <td>0.254576</td>\n      <td>31.0</td>\n      <td>25857.063169</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>164.0</td>\n      <td>152.0</td>\n      <td>365388.0</td>\n      <td>1.0</td>\n      <td>21118.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>164.729211</td>\n      <td>0.499922</td>\n      <td>32.0</td>\n      <td>NaN</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>2.0</td>\n      <td>37.619003</td>\n      <td>10.0</td>\n      <td>36764.0</td>\n      <td>15.0</td>\n      <td>0.500024</td>\n      <td>0.244792</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>149.0</td>\n      <td>299.0</td>\n      <td>1.0</td>\n      <td>309870.0</td>\n      <td>0.0</td>\n      <td>87525.0</td>\n      <td>135.694052</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Male</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>&lt; 1 Year</td>\n      <td>No</td>\n      <td>36274.0</td>\n      <td>152.0</td>\n      <td>256.0</td>\n      <td>9286144.0</td>\n      <td>0.829111</td>\n      <td>106.0</td>\n      <td>11913.0</td>\n      <td>542950.0</td>\n      <td>0.195258</td>\n      <td>256.0</td>\n      <td>362740.0</td>\n      <td>1.0</td>\n      <td>255.0</td>\n      <td>257.0</td>\n      <td>0.921735</td>\n      <td>256.0</td>\n      <td>0.466462</td>\n      <td>11.027021</td>\n      <td>0.700654</td>\n      <td>0.835286</td>\n      <td>0.672440</td>\n      <td>36275.0</td>\n      <td>281.0</td>\n      <td>12342.0</td>\n      <td>0.655816</td>\n      <td>36530.0</td>\n      <td>0.697934</td>\n      <td>36274.0</td>\n      <td>36299.0</td>\n      <td>1450.960000</td>\n      <td>0.835485</td>\n      <td>0.824923</td>\n      <td>0.836364</td>\n      <td>3.0</td>\n      <td>30.023605</td>\n      <td>0.539638</td>\n      <td>238.644737</td>\n      <td>0.499175</td>\n      <td>64870.0</td>\n      <td>7566.0</td>\n      <td>25.0</td>\n      <td>82.260215</td>\n      <td>266.0</td>\n      <td>1847.0</td>\n      <td>11346.682460</td>\n      <td>0.731384</td>\n      <td>10686.0</td>\n      <td>15.200000</td>\n      <td>0.625551</td>\n      <td>0.662050</td>\n      <td>3.0</td>\n      <td>24892.287308</td>\n      <td>28015.0</td>\n      <td>0.0</td>\n      <td>0.638378</td>\n      <td>139.645371</td>\n      <td>36249.0</td>\n      <td>3.0</td>\n      <td>0.225569</td>\n      <td>0.842750</td>\n      <td>256.0</td>\n      <td>5513648.0</td>\n      <td>36274.0</td>\n      <td>3.0</td>\n      <td>0.840829</td>\n      <td>79.646454</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>10.240000</td>\n      <td>255.0</td>\n      <td>906850.0</td>\n      <td>0.655181</td>\n      <td>257.0</td>\n      <td>36274.0</td>\n      <td>0.466655</td>\n      <td>177.0</td>\n      <td>36426.0</td>\n      <td>81240.0</td>\n      <td>35526.0</td>\n      <td>408.0</td>\n      <td>290594.0</td>\n      <td>2560.0</td>\n      <td>25.0</td>\n      <td>0.475324</td>\n      <td>36284.0</td>\n      <td>152.0</td>\n      <td>3800.0</td>\n      <td>36273.0</td>\n      <td>142.0</td>\n      <td>256.0</td>\n      <td>246.0</td>\n      <td>82460.0</td>\n      <td>0.999387</td>\n      <td>36122.0</td>\n      <td>80.0</td>\n      <td>250.0</td>\n      <td>0.193205</td>\n      <td>141.695312</td>\n      <td>3.0</td>\n      <td>745026.0</td>\n      <td>25.0</td>\n      <td>11198.0</td>\n      <td>36264.0</td>\n      <td>-104.0</td>\n      <td>12986.968804</td>\n      <td>3627.400000</td>\n      <td>2.500000</td>\n      <td>0.629475</td>\n      <td>16300.0</td>\n      <td>152.0</td>\n      <td>3.0</td>\n      <td>15.0</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>6400.0</td>\n      <td>0.627876</td>\n      <td>25.0</td>\n      <td>164.072226</td>\n      <td>35.0</td>\n      <td>38912.0</td>\n      <td>25.408137</td>\n      <td>1.00</td>\n      <td>0.0</td>\n      <td>1520.0</td>\n      <td>152.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.193213</td>\n      <td>444005.0</td>\n      <td>0.000000</td>\n      <td>470533.0</td>\n      <td>11.0</td>\n      <td>152.0</td>\n      <td>61108.0</td>\n      <td>5.023881</td>\n      <td>0.100000</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>36274.0</td>\n      <td>3.162278</td>\n      <td>0.719155</td>\n      <td>1.0</td>\n      <td>167.142991</td>\n      <td>367578.0</td>\n      <td>3.0</td>\n      <td>36274.0</td>\n      <td>9.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>35.676699</td>\n      <td>0.0</td>\n      <td>0.201518</td>\n      <td>152.0</td>\n      <td>16310.0</td>\n      <td>299.0</td>\n      <td>0.393759</td>\n      <td>0.729068</td>\n      <td>13.707443</td>\n      <td>25.600000</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>162.0</td>\n      <td>24.0</td>\n      <td>0.280270</td>\n      <td>0.0</td>\n      <td>256.0</td>\n      <td>153.0</td>\n      <td>0.689718</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>0.622999</td>\n      <td>11.0</td>\n      <td>0.386855</td>\n      <td>1.0</td>\n      <td>0.593750</td>\n      <td>169.0</td>\n      <td>256.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>61.0</td>\n      <td>2.302585</td>\n      <td>0.741062</td>\n      <td>0.999955</td>\n      <td>0.500337</td>\n      <td>1.0</td>\n      <td>0.173814</td>\n      <td>30984.0</td>\n      <td>0.500683</td>\n      <td>152.0</td>\n      <td>30962.0</td>\n      <td>-9.0</td>\n      <td>6.080000</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>25.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>151.0</td>\n      <td>24.0</td>\n      <td>0.175181</td>\n      <td>26.0</td>\n      <td>31400.949636</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>153.0</td>\n      <td>291520.0</td>\n      <td>1.0</td>\n      <td>82460.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.438054</td>\n      <td>26.0</td>\n      <td>25.0</td>\n      <td>152.0</td>\n      <td>152.0</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>151.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>20.478651</td>\n      <td>10.0</td>\n      <td>36018.0</td>\n      <td>10.0</td>\n      <td>0.500006</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>256.0</td>\n      <td>299.0</td>\n      <td>2.0</td>\n      <td>85517.0</td>\n      <td>10.0</td>\n      <td>658494.0</td>\n      <td>148.647138</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Male</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>1-2 Year</td>\n      <td>Yes</td>\n      <td>44214.0</td>\n      <td>26.0</td>\n      <td>223.0</td>\n      <td>9859722.0</td>\n      <td>0.708394</td>\n      <td>93.0</td>\n      <td>4884.0</td>\n      <td>317263.0</td>\n      <td>0.485734</td>\n      <td>223.0</td>\n      <td>1237992.0</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>224.0</td>\n      <td>0.609766</td>\n      <td>223.0</td>\n      <td>0.906301</td>\n      <td>13.832753</td>\n      <td>0.790905</td>\n      <td>0.725049</td>\n      <td>0.842734</td>\n      <td>44215.0</td>\n      <td>289.0</td>\n      <td>332339.0</td>\n      <td>0.821479</td>\n      <td>44437.0</td>\n      <td>0.849935</td>\n      <td>44214.0</td>\n      <td>44280.0</td>\n      <td>669.909091</td>\n      <td>0.701396</td>\n      <td>0.717548</td>\n      <td>0.718992</td>\n      <td>1.0</td>\n      <td>45.709578</td>\n      <td>0.223135</td>\n      <td>1700.538462</td>\n      <td>0.303853</td>\n      <td>6696.0</td>\n      <td>258450.0</td>\n      <td>45.0</td>\n      <td>79.938583</td>\n      <td>251.0</td>\n      <td>4950.0</td>\n      <td>15600.277416</td>\n      <td>0.787907</td>\n      <td>141443.0</td>\n      <td>0.928571</td>\n      <td>0.903795</td>\n      <td>0.839407</td>\n      <td>4.0</td>\n      <td>39558.794219</td>\n      <td>39802.0</td>\n      <td>2.0</td>\n      <td>0.810001</td>\n      <td>86.787510</td>\n      <td>44148.0</td>\n      <td>1.0</td>\n      <td>0.938345</td>\n      <td>0.713352</td>\n      <td>223.0</td>\n      <td>1149564.0</td>\n      <td>44214.0</td>\n      <td>4.0</td>\n      <td>0.714188</td>\n      <td>79.709674</td>\n      <td>-40.0</td>\n      <td>3.0</td>\n      <td>3.378788</td>\n      <td>222.0</td>\n      <td>2918124.0</td>\n      <td>0.314534</td>\n      <td>224.0</td>\n      <td>44214.0</td>\n      <td>0.235340</td>\n      <td>92.0</td>\n      <td>44240.0</td>\n      <td>9586.0</td>\n      <td>6662.0</td>\n      <td>249.0</td>\n      <td>308615.0</td>\n      <td>6244.0</td>\n      <td>66.0</td>\n      <td>0.464330</td>\n      <td>44242.0</td>\n      <td>26.0</td>\n      <td>1716.0</td>\n      <td>44213.0</td>\n      <td>-2.0</td>\n      <td>223.0</td>\n      <td>195.0</td>\n      <td>10791.0</td>\n      <td>0.997138</td>\n      <td>44188.0</td>\n      <td>85.0</td>\n      <td>1848.0</td>\n      <td>0.496518</td>\n      <td>198.269058</td>\n      <td>4.0</td>\n      <td>751966.0</td>\n      <td>66.0</td>\n      <td>297111.0</td>\n      <td>44186.0</td>\n      <td>-197.0</td>\n      <td>19314.044119</td>\n      <td>1579.071429</td>\n      <td>2.357143</td>\n      <td>0.813899</td>\n      <td>448404.0</td>\n      <td>26.0</td>\n      <td>4.0</td>\n      <td>38.0</td>\n      <td>3.0</td>\n      <td>26.0</td>\n      <td>14718.0</td>\n      <td>0.187160</td>\n      <td>26.0</td>\n      <td>163.305323</td>\n      <td>94.0</td>\n      <td>5798.0</td>\n      <td>27.238165</td>\n      <td>0.99</td>\n      <td>3.0</td>\n      <td>728.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>0.490040</td>\n      <td>250795.0</td>\n      <td>0.099503</td>\n      <td>505403.0</td>\n      <td>29.0</td>\n      <td>28.0</td>\n      <td>4058.0</td>\n      <td>3.258097</td>\n      <td>0.035714</td>\n      <td>28.0</td>\n      <td>28.0</td>\n      <td>44214.0</td>\n      <td>5.291503</td>\n      <td>0.709128</td>\n      <td>0.0</td>\n      <td>165.347064</td>\n      <td>444306.0</td>\n      <td>4.0</td>\n      <td>44214.0</td>\n      <td>27.0</td>\n      <td>809627.0</td>\n      <td>0.768198</td>\n      <td>54.847088</td>\n      <td>0.0</td>\n      <td>0.492609</td>\n      <td>26.0</td>\n      <td>449691.0</td>\n      <td>299.0</td>\n      <td>0.952428</td>\n      <td>0.162168</td>\n      <td>11.906707</td>\n      <td>7.964286</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>66.0</td>\n      <td>54.0</td>\n      <td>65.0</td>\n      <td>0.944127</td>\n      <td>0.0</td>\n      <td>223.0</td>\n      <td>27.0</td>\n      <td>0.143149</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>0.095901</td>\n      <td>29.0</td>\n      <td>0.932316</td>\n      <td>2.0</td>\n      <td>0.116592</td>\n      <td>166.0</td>\n      <td>223.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>66.0</td>\n      <td>3.332205</td>\n      <td>0.372294</td>\n      <td>1.000000</td>\n      <td>0.501432</td>\n      <td>2.0</td>\n      <td>0.494718</td>\n      <td>35500.0</td>\n      <td>0.500683</td>\n      <td>122.0</td>\n      <td>32993.0</td>\n      <td>-27.0</td>\n      <td>0.393939</td>\n      <td>163.0</td>\n      <td>3.0</td>\n      <td>66.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>25.0</td>\n      <td>65.0</td>\n      <td>0.495553</td>\n      <td>67.0</td>\n      <td>32524.870734</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>170.0</td>\n      <td>27.0</td>\n      <td>479626.0</td>\n      <td>1.0</td>\n      <td>10900.0</td>\n      <td>20.0</td>\n      <td>1.0</td>\n      <td>28.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>163.172999</td>\n      <td>0.483438</td>\n      <td>67.0</td>\n      <td>66.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>26.0</td>\n      <td>1.0</td>\n      <td>25.0</td>\n      <td>53.0</td>\n      <td>3.0</td>\n      <td>53.125551</td>\n      <td>10.0</td>\n      <td>43991.0</td>\n      <td>28.0</td>\n      <td>0.505046</td>\n      <td>0.788312</td>\n      <td>10.0</td>\n      <td>1.0</td>\n      <td>223.0</td>\n      <td>299.0</td>\n      <td>0.0</td>\n      <td>540165.0</td>\n      <td>28.0</td>\n      <td>36466.0</td>\n      <td>71.982385</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:11:17.634111Z","iopub.execute_input":"2024-07-29T09:11:17.635088Z","iopub.status.idle":"2024-07-29T09:11:18.020735Z","shell.execute_reply.started":"2024-07-29T09:11:17.635043Z","shell.execute_reply":"2024-07-29T09:11:18.019336Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"],"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error"}]},{"cell_type":"code","source":"cont_names,cat_names = cont_cat_split(train, dep_var='Response')\nsplits = RandomSplitter(valid_pct=0.2)(range_of(train))\nto = TabularPandas(train, procs=[Categorify, FillMissing,Normalize],\n                  cat_names = cat_names,\n                  cont_names = cont_names,\n                  y_names='Response',\n                  y_block=CategoryBlock(),\n                  splits=splits)\ndls = to.dataloaders(bs=64)\ntest_dl = dls.test_dl(test_subset)\nX_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T09:10:07.327045Z","iopub.execute_input":"2024-07-29T09:10:07.327534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_names='Response'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ofe.fit(data=X_train, label=y_train, n_jobs=n_jobs) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, test_x = transform(X_train, X_test, features, n_jobs=n_jobs)","metadata":{},"execution_count":null,"outputs":[]}]}